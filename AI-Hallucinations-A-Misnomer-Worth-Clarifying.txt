AI Hallucinations: A Misnomer Worth Clarifying

Abstract—As large language models continue to advance in
Artificial Intelligence (AI), text generation systems have been
shown to suffer from a problematic phenomenon termed often
as “hallucination.” However, with AI’s increasing presence across
various domains including medicine, concerns have arisen regarding the use of the term itself. In this study, we conducted a
systematic review to identify papers defining “AI hallucination”
across fourteen databases. We present and analyze definitions
obtained across all databases, categorize them based on their
applications, and extract key points within each category. Our
results highlight a lack of consistency in how the term is used,
but also help identify several alternative terms in the literature.
We discuss implications of these and call for a more unified effort
to bring consistency to an important contemporary AI issue that
can affect multiple domains significantly
1
.
Index Terms—AI, Hallucination, Generative AI

I. INTRODUCTION
One of the early uses of the term "hallucination" in the field
of Artificial Intelligence (AI) was in computer vision, in 2000
[1], where it was associated with constructive implication
s
such as super-resolution [1], image inpainting [2], and image
synthesis [3]. Interestingly, in this context hallucination was
regarded as a valuable asset in computer vision rather than
an issue to be circumvented. For instance, an image with low
resolution might have been rendered more useful with carefu
l
hallucination [1] that generated additional pixels specifically
for this purpose.
Despite this (more positive) beginning, recent research ha
s
started to employ the term "hallucination" to describe a
specific type of error in image captioning [4] and adversaria
l
attack in object detection [5]. In this context, "hallucination"
refers to instances where non-existent objects are erroneously
detected or incorrectly localized at their anticipated positions.
This latter (more negative) interpretation of "hallucination" in
computer vision mirrors its analogous usage in language mod
-
els. For instance, in 2017, researchers highlighted challenges
in language models, such as "the output of the Neural Machine
Translation (NMT) system is often quite fluent but entirely
unrelated to the input" [6], or "language models presume
likelihood, but the generated content is ultimately incorrect
and unsupported by any information" [7], which is interpreted
as a form of hallucination in AI.

To date, a precise and universally accepted definition of
"hallucination" remains absent in the discussions related to this
in the increasingly broader field of AI [8]. Diverse definitions,
or implied interpretations, persist; sometimes even contradictory, as previously highlighted within the field of computer
vision where multiple, disparate interpretations coexist under
the same term.
Beyond the AI context, and specifically in the medical
domain, the term "hallucination" is a psychological concep
t
denoting a specific form of sensory experience [9]. Ji et al.
[10], from the computer science perspective (in ACM Computing Surveys), rationalized the use of the term "hallucination"
as "an unreal perception that feels real" by drawing from
Blom’s definition — "a percept, experienced by a waking
individual, in the absence of an appropriate stimulus from the
extracorporeal world." On the other hand, Østergaard et al.
[11], from the medical perspective (in Schizophrenia Bulletin,
one of the leading journals in the discipline), raised critical
concerns regarding even the adoption of the "hallucination
"
terminology in AI for two primary reasons: 1) The "hallucination" metaphor in AI from this perspective is a misnomer,
as AI lacks sensory perceptions, and errors arise from data
and prompts rather than the absence of stimuli, and 2) this
metaphor is highly stigmatizing, as it associates negative issues
in AI with a specific issue in mental illness, particularly
schizophrenia, thereby possibly undermining many efforts to
reduce stigma in psychiatry and mental health.
Given AI’s increasing presence across various domains,
including the medical field, concerns have arisen regarding
the multifaceted, possibly inappropriate and potentially even
harmful use of the term "hallucination" [11], [12]. To address
this issue effectively, two potential paths of work offer some
promise: 1) The establishment of a consistent and universally applicable terminologies that can be uniformly adopted
across all AI-impacted domains will help, particularly if such
terminologies lead to the use of more specific and nuanced
terms that actually describe the issues they highlight (as w
e
will show later, such vocabulary does exist, but needs more
consistent use) and 2) The formulation of a robust and formal
definition of "AI hallucination" within the context of AI.
These measures are essential to promote clarity and coherence
in discussions and research related to "hallucination" in AI,
and to mitigate potential confusion and ambiguity in crossdisciplinary applications.
Motivated by these issues, in this paper, we conduct a
systematic review of the use of "AI hallucination" across 14 databases with a focus on identifying various definitions that
have been used in the literature so far (our review covers more
fields than just healthcare and computer science, including
ethical and legal settings, and domains as diverse as physics,
sports, etc. in order to explore any broader issues). Recently,
two papers ( [10], [13]) explored the concept of hallucination in Natural Language Generation- (NLG-) specific tasks
(e.g., text translation, text summarization, knowledge graph,
etc.). Our work builds on these studies to also consider the
application of NLG in diverse domains. The pervasive nature
of AI extends beyond these specific tasks, affecting numerous
domains and applications. Consequently, our broader review
done here reveals the extensive utilization of Large Language
Models (LLMs) across almost a much broader space of
domains to date, and provides a comprehensive understanding
of how the term has been leveraged across various fields.
Generally we see that research attempting to define "AI
hallucination" does so based on their individual understanding
and the challenges encountered within their respective fields.
The findings from our systematic and broad review underscore
the challenge that the term "AI hallucination" lacks a precise,
universally accepted definition, resulting in the observation
of various characteristics associated with this term across
different applications. We present a summary of these different
interpretations and provide some guidance going forward.
II. METHODOLOGY
Our systematic review covered an extensive database search
across various domains, including computer science and
health, with a focus on the following databases: PubMed,
MEDLINE, Scopus, PubMed Central, Web of Science,
BioMed Central, Embase, PLOS, CINAHL, ACM, IEEEXplore, ScienceDirect, Google Scholar, and arXiv (no relevant
documents were found in MedlinePlus, Cochrane Library, and
UpToDate databases, so those were excluded).
Our search methodology was tailored to adapt to the volume
of results as well as the relevance of the papers to our
research objectives. We manually reviewed every paper that
made it through this process in order to identify possible definitions/usage of the term "hallucination" in AI. Given this goal
we had to adapt the search in some cases to identify papers
most closely relevant to this objective as noted further below.
Also, given differences in how search queries are interpreted
across the different databases we had to iteratively modify the
search term within each database as well in many cases. For
clarity, we present all the specific details of this below (in
order to be transparent about how we created the subset of
papers from which to examine the definitions). However, the
summary of these is provided in Table I, including details of
the study period for each database.
In the PubMed Database, we initiated an advanced
search employing the keywords "Artificial Intelligence" AND
"Hallucination" within the "All field" category, yielding
103 papers within the last 10 years. However, the query
"AI+hallucination" yielded only 3 papers. Conversely, within
the Scopus database, searches for "AI hallucination" or "AI+hallucination" resulted in a total of 1445 records across all
fields over the same 10-year period. To manage this extensive
dataset, we refined our search criteria to focus on the Title,
Abstract, or Introduction, which reduced the results to 483
relevant records. A detailed review of each abstract led us to
download papers that appeared pertinent to AI hallucination.
This approach significantly differed from searching within
abstracts alone, which produced only 49 records and missed
some relevant documents.
In PubMed Central (PMC), the query "AI+hallucination"
yielded just 1 paper. Consequently, we conducted an advanced
search using the keywords "Artificial Intelligence" AND "Hallucination" within the "Text Word" category, uncovering 371
records from the past decade. PMC does not provide abstracts,
necessitating the manual examination of each paper to assess
its relevance to AI hallucination.
In both the MEDLINE and Web of Science databases,
we employed the term "AI hallucination" within the "All
field" category, yielding 157 and 139 records, respectively,
spanning the last 10 years. In the case of MEDLINE, each
record underwent a thorough review, and records containing
definitions for AI hallucination were downloaded. Conversely,
Web of Science offered abstracts for the records, enabling us
to screen them individually and select those relevant to AI
hallucination.
Within the BioMed Central (BMC) database, a search for
"AI hallucination" led to the retrieval of 76 papers published
within the last 10 years. Subsequently, we accessed each
paper individually and downloaded those containing pertinent
definitions of AI hallucination. In the Embase database, our
search for "AI hallucination" produced 80 records published
within the last 10 years. These records underwent meticulous
abstract review, and those relevant to AI hallucination were
selectively downloaded for further analysis.
In the PLOS database, our initial search with "AI hallucination" resulted in a substantial 1064 records across all
fields for the past 10 years. Given this large dataset, we
refined our search to focus on the "Body" section, yielding
885 records. We proceeded to review the abstracts of each
paper and downloaded those demonstrating relevance to AI
hallucination. Within the CINAHL database, we conducted
an advanced search utilizing the terms "AI" or "Artificial
Intelligence" combined with "Hallucination" within the "All
field" category, yielding 34 records published in the past 10
years.
In the ACM and IEEEXplore databases, we performed
advanced searches using the terms "AI" AND "hallucination"
within the "Full Text" category, resulting in 264 and 257
records, respectively, spanning the past 10 years. Each record
underwent individual review, and those containing definitions
related to AI hallucination, particularly within the field of
LLMs, were downloaded.
In the ScienceDirect database, searches for "AI hallucination" or ("AI" AND "hallucination") yielded an identical
number of records, specifically 769 English records, spanning
the last decade. Each record underwent meticulous examination, and we selectively downloaded papers containing defined
concepts of AI hallucination within the LLMs domain.
Searching for "AI hallucination" on Google Scholar yielded
17,000 records from the last 10 years, rendering a comprehensive review unfeasible. To address this challenge, we
employed Google Scholar’s advanced search feature, identifying records containing the exact phrases "AI hallucination"
and "hallucination in AI," which reduced the results to 89
records from the last decade. Subsequently, we conducted
a meticulous screening of these records to identify those
providing definitions for AI hallucination.
Similarly, within the arXiv database, we conducted an advanced search using the keywords "AI" AND "hallucination"
within the "All field" category, resulting in the retrieval of
40 relevant papers. As with our prior search, we meticulously
examined each paper and downloaded those containing definitions for AI hallucination.
The eligibility criteria encompassed any type of published
scientific research or preprints, such as articles, reviews,
communications, editorials, and opinions, that contained the
following search terms: "AI hallucination," "AI" AND "hallucination," "Hallucination in AI," or ("AI" OR "Artificial Intelligence") AND "hallucination" in any part of the document,
including the title, abstract, and full text. As explained for each
database, we employed the most appropriate search terms.
Initially, our search yielded 3753 records, in total, matching these criteria. However, we refined our search to focus
exclusively on records that offered a definition of "AI hallucination" within the context of LLMs. It is essential to clarify
that we excluded other types of hallucination, such as face
hallucination, auditory voice/verbal hallucination, etc., as they
were not the primary focus of this review. Our search involved
thorough examination of entire documents, and we collected
any documents that indicated the presence of a definition for AI hallucination.
Our exclusion criterion was limited to non-English records.
The precise database search strategy encompassed all available
documents from January 1st, 2013, to October 1st, 2023. In
total, we identified 333 records that provide a definition either
independently or by inference from a referenced paper. The
summary of the methodology is provided in Table I, including
details of the study period for each database. While our review
of this work is one of the broadest to date, we acknowledge
limitations that are implicit in the methodology above - particularly ones where we had to reduce the retrieved number
of papers to focus on potentially more relevant ones due to
the manual nature of our review (i.e. where we individually
reviewed each paper to identify how the term was used and
extract the relevant definition in the proper context). Therefore,
while the definitions presented here are certainly those that
were used it is possible we may have missed a few other
definitions that may have newer connotations not identified
in our work here. All the 333 definitions are provided in the
Appendix.

                                                                                 TABLE I
                                                                             M ETHOD SUMMARY

     Source/                           Search Category                                           Query Terms                                  Num. of    Study
     Database                                                                                                                                 Papers    End Date*
     PubMed                                All Field                              "Artificial Intelligence" AND "Hallucination"                 103     09/27/2023
     MEDLINE                               All Field                                           "AI hallucination"                               157     09/28/2023
     Scopus                     Title, Abstract, or Introduction                    "AI+hallucination" OR "AI hallucination"                    483     09/27/2023
     PubMed Central                       Text Word                               "Artificial Intelligence" AND "Hallucination"                 371     09/28/2023
     Web of Science                        All Field                                           "AI hallucination"                               139     09/28/2023
     BioMed Central                        All Field                                           "AI hallucination"                               76      09/29/2023
     Embase                                All Field                                           "AI hallucination"                               80      09/29/2023
     PLOS                                    Body                                              "AI hallucination"                               885     09/29/2023
     CINAHL                                All Field                        "Hallucination" AND ("AI" OR "Artificial Intelligence")             34      09/29/2023
     ACM                                   Full Text                                       "AI" AND "hallucination"                             264     09/30/2023
     IEEEXplore                            Full Text                                       "AI" AND "hallucination"                             257     09/30/2023
     ScienceDirect                         All Field                           "AI hallucination" OR ("AI" AND "hallucination")                 769     09/30/2023
     Google Scholar                        All Field                              "AI hallucination" AND "hallucination in AI"                  89      10/01/2023
     arXiv                                 All Field                                       "AI" AND "hallucination"                             40      10/01/2023
 *
     The start date is the same for all databases: 01/01/2013 (Date format: mm/dd/yyyy).

III. RESULT
We reviewed all retrieved papers and documented the
definitions provided in each. One main takeaway was that
a formal and consistent definition of hallucination simply
does not currently exist. There is also little agreement on
the specific characteristics of AI hallucination. Depending on
the application, we observe varying characteristics, sometimes
even contradictory ones.
For instance, in the context of text translation, Koehn and
Knowles [6] described hallucination as "fluent but irrelevant,"
or Miao et al. [26] characterized it as "fluent but inadequate,"
while Lee et al. [27] attributed "abnormal and unrelated" characteristics to it, thus illustrating different attributes within the same context. In the text summarization context, hallucination
refers to generated content that is inconsistent with the source
document [28], [29], with some studies categorizing it into
subtypes: "Intrinsic hallucination" and "Extrinsic hallucination" [30], [31], raising concerns, particularly regarding the
latter.
Before the launch of ChatGPT on November 30, 2022,
we hardly observed definitions for AI hallucination in fields
other than computer science. However, with the advent of
ChatGPT, researchers have recognized the urgent need for
Large Language Models (LLMs) in various fields, including
medicine. Therefore, over time, we have observed that the
definition has changed and seems to have become a problem
more relevant to ChatGPT, albeit with different characteristics
under the same term across various applications.
In recent times, for reasons discussed earlier in this paper
as well as broader concern about giving AI "human" characteristics inadvertently by using this term, researchers have
made efforts to replace the term ’hallucination,’ deeming it
unsuitable and advocating for its renaming or for alternatives.
We have compiled many of the suggested terms found in
the literature in Table II, along with their definitions in the
respective papers. This is a start in the right direction perhaps -
in the search for specific definitions and specific characteristics
that we want to model - but does illustrates the lack of
consistency in the literature that we pointed out in this paper.
Based on the alternate terms we found, some "old" problems
appear to re-surface: the terms confabulation and delusional
for instance have connections to mental health conditions
as well. However, fabrication, stochastic parroting and hasty
generalization together suggest three viable alternatives. Fact
fabrication captures many of the cases previously attributed
to ’hallucination’ without the negative connotations, while stochastic parroting appears to be an appropriate descriptive
term for the reasons behind fact fabrication in Generative AI.
While we need clarity in terms of distinguishing between
stated facts (in the training data) and inferences, the reference
to hasty generalization does start to capture such a distinction.
Finally, since our focus here was on reviewing AI hallucinations across various applications, we grouped all the final
papers examined by category, extracted definitions related to
AI hallucination, and used ChatGPT 3.5 [32] to extract key
points. The applications included chatbots, dialogue settings,
generative AI, academia, health, legal and ethical settings,
science, technology, text translation, question and answering,
text summarization, and others. As shown in Table III, the
extracted summaries share similar characteristics, but highlight
different extents of inaccuracy, ranging from "deviating from
established knowledge", "factual incorrectness", "fictional" to
"nonsensical" – offering further considerations for a robust
taxonomy that will be needed to bring out such nuances.

                                                                          TABLE II
                                                                   A LTERNATIVE TERMS USED

 Alternative Terms               Definitions                                                                                                                   References
                                 AI generated responses that sound plausible but are, in fact, incorrect.                                                      [14]
 Confabulation                   Definition was not provided.                                                                                                  [15]
 Delusion                        AI generated responses that are false.                                                                                        [16]
                                 The repetition of training data or its patterns, rather than actual understanding or reasoning.                               [17]
 Stochastic Parroting            LLM model generates confident, specific, and fluent answers that are factually completely wrong.                              [18]
                                 Definition was not provided.                                                                                                  [19]
 Factual Errors                  Inaccuracies in information or statements that are not in accordance with reality or the truth, often unintentional but       [20]
                                 resulting in incorrect or misleading information.
 Fact Fabrication                The occurrence where inaccurate information is invented, not represented in the training dataset, and is presented lucidly.   [21]
                                 The phenomenon where, as a generative AI, ChatGPT generates outputs based on statistical prediction of the text without       [22]
 Fabrication                     human-like reasoning, potentially resulting in plausible-sounding but inaccurate responses.
                                 The phenomenon in ChatGPT output where the text is cogent but not necessarily true.                                           [23]
                                 Definition was not provided.                                                                                                  [24]
 Falsification and Fabrication   Definition was not provided.                                                                                                  [12]
 Mistakes, Blunders, False-      Answers that are fabricated when data are insufficient for an accurate response.                                              [25]
 hoods
 Hasty Generalizations, False    AI models making inferences that do not follow from the premises; also “hasty generalizations,” i.e., the fallacy of          [11]
 Analogy, False Dilemma          making (too) strong claims based on (too) limited data.

IV. DISCUSSION
“Hallucinate” secured its position as the word of 2023 (
[38], [39]) and Dictionary.com noted a 46% surge in searches
for the term over the past year. The popular press has also
keyed in on this (Table IV highlights topics some recent
articles discuss and the meaning of "AI hallucination" they
convey). These articles primarily feature interviews with CEOs
of big tech companies, who discuss future efforts to prevent
"hallucination" in chatbots’ outputs. Indeed, preventing such
occurrences continues to be a key research goal, but few
solutions have emerged so far. In the meanwhile, the Generative AI continues to expand its applications into multiple
domains, making the need for good solutions vital. As a
precursor to even developing solutions, this paper calls for more systematic, consistent and semantically nuanced terms
that can replace "hallucinations" for the reasons noted here.
As one step toward such a call, we presented a short summary
from one of the broadest manual literature reviews on this topic
to date. Our findings illustrate the current lack of consistency
and consensus on this issue, but also bring to light some recent options that are good alternatives. More work is needed to
develop a systematic taxonomy that can be widely adopted as
we discuss these issues in the context of AI applications.

                                                                 TABLE III
     K EY POINTS OF "H ALLUCINATION " DEFINITIONS WITHIN EACH APPLICATION . T HE CHARACTERISTICS OF DEFINITIONS ARE PRESENTED IN B OLD ,
                                       ALTHOUGH THEY MAY BE SIMILAR ACROSS DIFFERENT APPLICATIONS .


    Application               Number          LLM Generated Key Points of Definitions
                                of
                              Papers
    Chatbot                      34           The definitions collectively highlight the central theme of AI-generated content deviating from factual correctness, at times even
                                              leading to entirely fictional or erroneous information. In essence, AI hallucination underscores the ongoing challenge of maintaining
                                              accuracy and reliability in AI-generated content within the context of chatbot applications.
    Dialogue Set-                8            The definitions collectively underscore the challenge of ensuring accuracy and reliability in dialogue systems, given the potential pitfalls
    ting                                      associated with generating content that is unsupported, nonsensical, or factually incorrect. These issues are particularly pertinent
                                              when deploying large pre-trained language models in dialogue applications, as they struggle with maintaining fidelity to the source
                                              material while generating coherent and accurate responses.
    Generative AI                50           The definitions collectively emphasize the complexity of ensuring factual accuracy and reliability in AI-generated content within
                                              generative AI applications, highlighting the potential pitfalls of deviating from adherence to factual correctness.
    Academia                     88           A common thread among these definitions is the generation of text or content by AI models that lacks fidelity to factual accuracy,
                                              reality, or the intended context.
    Health                       82           The key idea common to all the definitions is that "AI hallucination" occurs when AI systems generate information that deviates from
                                              factual accuracy, context, or established knowledge. In essence, AI hallucination manifests as the production of text that, though
                                              potentially plausible, deviates from established facts or knowledge in health applications.
    Legal         and            16           The definitions collectively emphasize the multifaceted challenges posed by AI hallucination in the legal and ethical context. They
    Ethical                                   highlight issues of accuracy, confidence, relevance, context, and potential misinformation, underscoring the critical importance of
    Setting                                   addressing these challenges to ensure the responsible and ethical use of AI systems.
    Science                      10           Across the definitions, the central theme is that AI hallucination involves the generation of text or information that deviates from factual
                                              accuracy, coherence, or faithfulness to the input or source content, with potential consequences for scientific accuracy and integrity.
    Technology                   8            The definitions reflect the multifaceted nature of AI hallucination in technology applications, encompassing accuracy, unpredictability,
                                              credibility, and the balance between reasonableness and correctness.
    Text Transla-                4            The definitions collectively emphasize the central theme of "AI hallucination" in text translation, which revolves around challenges
    tion                                      related to maintaining fidelity, coherence, and relevance in the generated translations to ensure accurate and meaningful output.
    Question and                 7            "AI hallucination" in question and answer applications raises concerns related to the accuracy, truthfulness, and potential spread of
    Answering                                 misinformation in AI-generated answers, emphasizing the need for improving the reliability of these systems.
    Text Summa-                  19           The definitions highlight the multifaceted challenges posed by "AI hallucination" in text summarization, encompassing issues related to
    rization                                  fidelity, coherence, factual accuracy, and the preservation of the original meaning in generated summaries.
              *
    Others                       7            These diverse applications collectively emphasize the challenge of maintaining accuracy, coherence, and trustworthiness in AI-
                                              generated content, highlighting the need for tailored approaches to address domain-specific concerns.
*
    Including: Investment portfolio, Journalism, Reinforcement Learning, Retail, Sport, and Survey Setting.

  TABLE IV
                                                             S OME POPULAR PRESS ARTICLES ON AI HALLUCINATION

                  What the Press Article Discussed. . .                                            The Real Meaning the Press Article Conveys about "AI                    Source
                                                                                                   Hallucination"
    1             CNBC provided some examples where ChatGPT generated outputs that                 When an AI model “hallucinates,” it generates fabricated                CNBC [33]
                  sounded correct but weren’t actually true, such as a legal brief written         information in response to a user’s prompt, but presents it
                  by ChatGPT to a Manhattan federal judge                                          as if it’s factual and correct
    2             The New York Times asked ChatGPT, Google’s Brad, and Microsoft’s                 Chatbots provide inaccurate answers to questions; although              The New York
                  Bing: When did The New York Times first report on "artificial intelli-           false, the responses appear plausible as they blur and conflate         Times [34]
                  gence"?                                                                          people, events, and ideas
    3             The New York Times traced the evolution of the term "hallucination"              -                                                                       The New York
                  throughout the newspaper’s history                                                                                                                       Times [35]
    4             CNN addressed the major issue of "AI hallucination" and narrated on              AI-powered tools like ChatGPT impress with their ability to             CNN [36]
                  the responses of OpenAI’s and Google’s CEOs to the question: Can                 provide human-like responses, but a growing concern is their
                  hallucination be prevented?                                                      tendency to just make things up
    5             Forbes narrated the history of artificial neural networks, which started         "AI hallucination" refers to unrealistic ideas about achieving          Forbes [37]
                  around eight decades ago, when researchers sought to replicate the               "artificial general intelligence" (AGI), while understanding of
                  functioning of the brain                                                         how our brains work is limited

TABLE V: AI hallucination definitions

Num.      Author(s)            Year   Application                              Definition                             Citation
1      Koehn       and         2017   Text Trans-       "AI hallucination" occurs when the output of the              1249
       Knowles [6]                    lation            Neural Machine Translation (NMT) system is often
                                                        quite fluent but entirely unrelated to the input.
2      Wiseman et al.          2017   Natural           "AI hallucination" occurs when a language model               556
       [7]                            Language          presumes likelihood, but the generated content is ulti-
                                      Generation        mately incorrect and unsupported by any information.
3      Lee et al. [40]         2018   Text Trans-       "AI hallucination" occurs when Neural Machine                 83
                                      lation            Translation (NMT) systems produce highly patholog-
                                                        ical translations that are completely untethered from
                                                        the source material.
4      Nie et al. [41]         2019   Large             "AI hallucination" refers to the problem where the            63
                                      Language          generated texts often contain information that is ir-
                                      Model             relevant to or contradicted by the input.
5      Tian et al. [42]        2019   Large             "AI hallucination" occurs when a language model               66
                                      Language          generates text that is fluent but unfaithful to the source.
                                      Model
6      Dušek et al. [43]       2019   Natural           "AI hallucination" occurs when a language model adds          89
                                      Language          information that is not grounded in the input.
                                      Generation
7      Ferreira   et     al.   2019   Natural           "AI hallucination" refers to a language model describ-        141
       [44]                           Language          ing non-linguistic representations that are not present
                                      Generation        in the input.
8      Martindale et al.       2019   Text Trans-       "AI hallucination" occurs when the machine transla-           29
       [45]                           lation            tion output contains more information than the refer-
                                                        ence text.
9      Dušek and Kas-          2020   Natural           "AI hallucination" occurs when expressions in the             46
       ner [46]                       Language          output do not correspond to input facts.
                                      Generation
10     Kang        and         2020   Natural           "AI hallucination" occurs when neural language mod-           72
       Hashimoto [47]                 Language          els often produce fluent text that is unfaithful to the
                                      Generation        source.
11     Parikh et al. [48]      2020   Natural           "AI hallucination" occurs when a language model               242
                                      Language          generates text that is fluent but not faithful to the
                                      Generation        source.
12     Maynez     et     al.   2020   Text Sum-         "AI hallucination" refers to content that is unfaithful       572
       [30]                           marization        to the input document.
                                                        "Intrinsic hallucinations" are consequences of synthe-
                                                        sizing content using the information present in the
                                                        input document.
                                                        "Extrinsic hallucinations" are model generations that
                                                        ignore the source material altogether. Hallucinate con-
                                                        tent that is unfaithful to the input document.
13     Durmus     et     al.   2020   Question          "AI hallucination" refers to the occurrence when AI           269
       [49]                           Answering         generates content inconsistent with the source docu-
                                                        ment, i.e., unfaithful.
14     Zhao et al. [28]        2020   Text Sum-         "AI hallucination" occurs when language models gen-           73
                                      marization        erate material that is not supported by the original text.
15     Dong et al. [29]        2020   Text Sum-         "AI hallucination" occurs when a language model               89
                                      marization        generates content that is factually inconsistent with the
                                                        source documents.
16     Filippova [8]           2020   Large             "AI hallucination" refers to the generated content            46
                                      Language          which is either unfaithful to the input or nonsensical.
                                      Model
17     Zhou et al. [50]        2020   Hallucination     "AI hallucination" occurs when the model generates            87
                                      Detection         additional content not supported by the input.
18     Elsahar    et     al.   2020   Hallucination     "AI hallucination" occurs when the fluency of natural         33
       [51]                           Mitigation        language generation models can be highly misleading,
                                                        as it often distracts from the wrong facts stated in the
                                                        generated text.
                                                                                                        Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

19     Tang et al. [52]        2021     Text Sum-       "AI hallucination" refers to content that is not sup-       32
                                        marization      ported by the source documents.
20     Cao et al. [53]         2021     Text Sum-       "AI hallucination" refers to content that is not directly   37
                                        marization      inferable from the source text.
21     Miao et al. [26]        2021     Text Trans-     "AI hallucination" refers to generating fluent but in-      21
                                        lation          adequate translations to the source sentences.
22     Chen et al. [54]        2021     Text Sum-       "Intrinsic" and "Extrinsic hallucinations", involving       62
                                        marization      the fabrication of untruthful text spans containing
                                                        information that may either be present or absent from
                                                        the source.
23     Wang et al. [55]        2021     Text Sum-       "AI hallucination" refers to where the model generates      14
                                        marization      fictional content.
24     Xiao and Wang           2021     Large           "AI hallucination" occurs where models generate de-         58
       [56]                             Language        scription tokens that are not supported by the source
                                        Model           inputs.
25     Dziri et al. [57]       2021     Knowledge       "AI hallucination" occurs when dialogue models, de-         51
                                        Graph           spite maintaining plausible general linguistic capabil-
                                                        ities, are still unable to fully discern facts and may
                                                        instead hallucinate factually invalid information.
26     Liu et al. [58]         2021     Hallucination   "AI hallucination" occurs when language models ex-          29
                                        Detection       hibit a propensity to hallucinate non-existent or incor-
                                                        rect content that is unacceptable in most user-oriented
                                                        applications.
27     Huang et al. [59]       2021     Text Sum-       "Intrinsic hallucination" is a fact that is contradicted    51
                                        marization      to the source document.
                                                        "Extrinsic hallucination" is the fact that is neutral to
                                                        the source document (i.e., the content that is neither
                                                        supported nor contradicted by the source document).
28     Lin [60]                2021     Question        "AI hallucination" refers to the tendency of LLMs to        250
                                        Answering       generate false statements.
29     Shuster    et     al.   2021     Large           "AI hallucination" refers to the occurrence where lan-      198
       [61]                             Language        guage models generate plausible-looking statements
                                        Model           that are factually incorrect.
30     Sekulić   et     al.   2021     Natural         "AI hallucination" occurs where generated responses         26
       [62]                             Language        do not correspond to the real-world.
                                        Generation
31     Ghosh et al. [63]       2021     Reinforcement   "AI hallucination" occurs when the generated text           5
                                        Learning        asserts information not present in the source.
32     Perez-Beltrachini       2021     Text Sum-       "AI hallucination" refers to neural summarization           18
       and Lapata [64]                  marization      models’ propensity to generate text that does not
                                                        preserve the meaning of the input.
33     Lyu et al. [31]         2022     Text Sum-       "AI hallucination" signifies the presence of distorting     0
                                        marization      or fabricating facts within generated summaries, re-
                                                        sulting in inconsistencies between a summary and the
                                                        corresponding original document.
                                                        "Extrinsic hallucination" entails adding information
                                                        not directly inferable from the input information.
                                                        "Intrinsic hallucination" involves manipulating the in-
                                                        formation present in the input document.
34     Ali et al. [65]         2022     Health          "AI hallucination" refers to scenarios in which a           29
                                                        Language Model (LLM) asserts inaccurate facts or
                                                        contextual data that it falsely believes to be correct
                                                        in its response.
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

35     Ando et al. [66]        2022     Health          "AI hallucination" occurs when abstractive summa-           2
                                                        rization may sometimes unintentionally generate un-
                                                        faithful descriptions.
                                                        "Intrinsic hallucination" is a phenomenon in which the
                                                        concept or term itself is in the source documents; its
                                                        synthesis misrepresents the information in the source,
                                                        and the meaning becomes inconsistent.
                                                        "Extrinsic hallucination" is content that is neither
                                                        supported nor contradicted by the source and is caused
                                                        by source documents with poor information.
36     Ando et al. [67]        2022     Health          "AI hallucination" refers to the phenomenon where the       2
                                                        abstractive approach frequently produces fake content
                                                        that does not align with the reference summary.
37     Rebuffel et al.         2022     Natural         "AI hallucination" refers to model outputs that are         43
       [68]                             Language        often subject to over-generation, where misaligned
                                        Generation      fragments from training instances, known as diver-
                                                        gences, can induce similarly misaligned outputs during
                                                        inference.
38     Wan and Bansal          2022     Text Sum-       "AI hallucination" refers to a scenario where a sum-        30
       [69]                             marization      mary contains facts or entities not present in the
                                                        original document.
39     Corbelle et al.         2022     Natural         "AI hallucination" refers to instances where neural         2
       [70]                             Language        models generate texts that are incoherent or unrelated
                                        Generation      to the input of a D2T system.
40     Lee et al. [71]         2022     Large           "AI hallucination" occurs when a model is making            30
                                        Language        factual errors, generating a named-entity that does not
                                        Model           appear in the ground-truth knowledge source.
41     Cabezudo et al.         2022     Natural         "AI hallucination" refers to text generated by pre-         0
       [72]                             Language        trained models that is irrelevant to or contradicted with
                                        Generation      the input.
42     Van et al. [73]         2022     Text Sum-       "AI hallucination" refers to the occurrence of adding       7
                                        marization      information to the output that was not present in the
                                                        original text.
43     Dziri et al. [74]       2022     Dialogue        "AI hallucination" occurs when large pre-trained lan-       51
                                                        guage models generate factually incorrect statements.
44     Dziri et al. [75]       2022     Dialogue        "AI hallucination" refers to the phenomenon where di-       24
                                                        alogue systems often produce unsupported utterances.
45     Koto et al. [76]        2022     Text Sum-       "AI hallucination" occurs where information is gener-       21
                                        marization      ated that does not exist in the source document, also
                                                        called "factual inconsistencies."
46     Goodman et al.          2022     Email Writ-     "AI hallucination" refers to factually incorrect or non-    10
       [77]                             ing             existent content generated by the LLM.
47     Gehrmann et al.         2022     Natural         "AI hallucination" refers to a situation where a model      62
       [78]                             Language        is not faithful as it adds information not present in the
                                        Generation      source document.
                                                        "Intrinsic hallucinations" misrepresent facts in the
                                                        input.
                                                        "Extrinsic hallucinations" ignore the input altogether.
48     Erdem et al. [79]       2022     Natural         "AI hallucination" refers to the generation of descrip-     20
                                        Language        tions or facts that are not fully supported by the input.
                                        Generation
49     Tun et al. [80]         2022     Dialogue        "AI hallucination" refers to large-scale pre-trained        0
                                                        language models generating text that is nonsensical
                                                        and struggling to remain true to the source content.
50     Gurrapu    et     al.   2022     Claim Veri-     "AI hallucination" refers to the phenomenon where           1
       [81]                             fication        natural language generation models introduce unin-
                                                        tended and irrelevant text during the generation pro-
                                                        cess.
51     Yang et al. [82]        2022     Text Sum-       "AI hallucination" occurs when models generate sum-          2
                                        marization      maries factually inconsistent with their original docu-
                                                        ment.
                                                                                                       Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                            Definition                            Citation

52     Ji et al. [10]         2023     Natural         "AI hallucination" refers to the generated content that      482
                                       Language        is nonsensical or unfaithful to the provided source
                                       Generation      content.
                                                       "Intrinsic hallucinations" encompasses the generated
                                                       output that contradicts the source content.
                                                       "Extrinsic hallucination" involves the generated output
                                                       that cannot be verified from the source content (i.e.,
                                                       output can neither be supported nor contradicted by
                                                       the source).
53     Narayanan et al.       2023     Natural         "AI hallucination" refers to situations in which AI          2
       [83]                            Language        models provide responses with confidence that appear
                                       Generation      faithful but are nonsensical when evaluated against
                                                       common knowledge.
54     Leiser et al. [84]     2023     Large           "Artificial hallucinations" are the distortion of per-       0
                                       Language        ception in Large Language Models (LLMs) as they
                                       Model           present incorrect information, disguising it as factual
                                                       responses.
55     Wang et al. [85]       2023     ChatGPT         "AI hallucination" denotes the tendency to generate          1
                                                       inaccurate outputs unfaithful to the training data.
56     Ma et al. [86]         2023     Software        "AI hallucination" refers to the phenomenon that can         8
                                       Engineering     result in the fabrication of elements that do not actually
                                                       exist.
57     Li [17]                2023     Large           "AI hallucination" occurs when LLMs generate text            11
                                       Language        based on their internal logic or patterns, rather than
                                       Model           the true context, leading to confidently but unjustified
                                                       and unverified deceptive responses.
58     Vaghefi    et    al.   2023     NLP     and     "AI hallucination" refers to mistakes in the generated       5
       [87]                            Climate         text that are semantically incorrect or unsupported by
                                       Change          the input text.
59     Daull et al. [88]      2023     Question        "AI hallucination" refers to confident generated re-         2
                                       Answering       sponses that contain false information not supported
                                                       by the model’s training data.
                                                       "Extrinsic hallucination" involves a model introducing
                                                       information not present in the source data.
                                                       "Intrinsic hallucination" occurs when the model dis-
                                                       torts information from the source data into a factually
                                                       incorrect representation.
60     Zhang et al. [89]      2023     Large           "AI hallucination" commonly refers to the phe-               3
                                       Language        nomenon where LLMs occasionally generate outputs
                                       Model           that, although appearing plausible, deviate from the
                                                       user input, previously generated context, or factual
                                                       knowledge.
61     Romanko et al.         2023     Investment      "AI hallucination" refers to instances where the AI,         3
       [90]                            Portfolio       although generating text based on its training, does so
                                                       without a solid conceptual framework behind it.
62     Henderson et al.       2023     AI Speech       "AI hallucination" refers to the act of generating text      2
       [91]                                            that includes factual claims that are untrue, and in
                                                       some cases, these claims may not have been present
                                                       in its training data.
63     Ni et al. [92]         2023     NLP     and     "AI hallucination" refers to the generation of answers       0
                                       Climate         in which some or all of the covered information is not
                                       Change          adequately supported by the report, particularly when
                                                       there is extrapolation or partial support involved, and
                                                       it may also relate to instances where the model fails to
                                                       honestly report its references while generating content
                                                       that lacks fidelity to the source.
64     Mahmood et al.         2023     Radiology       "AI hallucination" involves the generation of false          1
       [93]                            Report          findings in the produced reports by LLMs.
65     Goyal et al. [94]      2023     Question        "Hallucinations" signify the production of convincing         0
                                       Answering       yet incorrect text outputs by LLMs, with the potential
                                                       to distort scientific facts and disseminate misinforma-
                                                       tion.
                                                                                                       Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

66     Zhang [95]              2023     Robotic         "AI hallucination" refers to unpredictable outputs gen-     0
                                                        erated by LLMs.
67     Li et al. [96]          2023     Large           "AI hallucination" is the phenomenon where responses        1
                                        Language        are confidently generated but are incorrect.
                                        Model
68     Li et al. [97]          2023     Large           "AI hallucination" occurs when AI systems generate          1
                                        Language        outputs that are not aligned with the input context and
                                        Model           encounter challenges in efficiently capturing complex
                                                        dependencies.
69     Curran et al. [98]      2023     Legal           "AI hallucination" refers to tendency for generating        1
                                        Setting         false information.
70     Feldman et al.          2023     Large           "AI hallucination" in the context of LLMs refers to the     2
       [99]                             Language        phenomenon where these language models generate
                                        Model           non-factual statements, which could have a detrimental
                                                        effect on the trustworthiness of their outputs.
71     Mukherjee et al.        2023     Generative      "Hallucinations," characterized by AI responses con-        0
       [100]                            AI              taining random inaccuracies or falsehoods, emerge
                                                        when models prioritize novelty over usefulness.
72     Salvagno et al.         2023     Large           "AI hallucination" refers to the creation of seemingly      4
       [101]                            Language        accurate bibliographic references with recognized au-
                                        Model           thors and coherent titles, even though these references
                                                        are entirely fictitious and have no real existence.
73     Beutel    et      al.   2023     Large           "Hallucinations" are defined as the production of con-      20
       [102]                            Language        tent that does not accurately represent the provided
                                        Model           source and may appear nonsensical due to errors in
                                                        the encoding and decoding processes between text and
                                                        representations.
74     Azamfirei et al.        2023     Text Sum-       "AI hallucination" refers to receiving a response from      32
       [103]                            marization      the model when it lacks an appropriate answer, which
                                                        appears to be the most likely summary of the study,
                                                        despite its potential inaccuracy.
75     Meyer     et      al.   2023     Academia        "AI hallucination" can be defined as the capability of      8
       [104]                                            LLM-based chatbots to convey false information as
                                                        though it were true.
76     Hernigou      and       2023     Health          "AI hallucination" is a confident response that does        2
       Scarlat [105]                                    not seem in concordance with its training data.
77     Patil [106]             2023     Legal           "AI hallucination" is defined as a confident response       0
                                        Setting         by an AI system that lacks justification in its training
                                                        data. These responses can appear factual but are not
                                                        true, often simply being answers "made up" by the AI.
78     Alexander et al.        2023     Academia        "AI hallucination" refers to its tendency to make up        1
       [107]                                            facts and references that do not exist.
79     Lyell [108]             2023     Academia        "AI hallucination" describes the propensity of the          0
                                                        system to convincingly fabricate information.
80     Grassini [109]          2023     Academia        "AI hallucination" involves the generation of incorrect     14
                                                        or even fabricated information.
81     Brodeur    et     al.   2023     Legal           "AI hallucination" refers to the situation where an AI      0
       [110]                            Setting         system, due to its inability to correctly interpret data,
                                                        generates inaccurate or unusual outputs.
82     Lee and         Choi    2023     Health          "AI hallucination" occurs when AI generates informa-        0
       [111]                                            tion about things that are untrue, presenting them as
                                                        if they are true, which can diminish the reliability of
                                                        the generated information.
83     Chatfield [112]         2023     Chatbot         "AI hallucination" represents a response that may or        0
                                                        may not be plausible but is not rooted in reality.
84     McGowan et al.          2023     References      "AI hallucination" refers to the occurrence of mis-          4
       [113]                                            takes in the generated text that, while semantically
                                                        or syntactically plausible, are ultimately incorrect or
                                                        nonsensical upon closer examination.
                                                                                                       Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                            Definition                           Citation

85     Wu and      Dang       2023     Academia        "AI hallucination" refers to a scenario in which a          6
       [114]                                           machine generates seemingly realistic outputs without
                                                       any real-world input.
86     Wang et al. [115]      2023     Academia        "AI hallucination" refers to the generation of content      2
                                                       that is nonsensical or untrue in relation to certain
                                                       sources.
87     Hou and Ji [116]       2023     Health          "AI hallucination" refers to the situation where GPT        3
                                                       models provide confident answers that contradict the
                                                       truth.
88     Au et al. [117]        2023     Health          "AI hallucination" refers to the phenomenon in which        15
                                                       LLMs produce output that is nonsensical or unfaithful
                                                       to the provided input or "prompt," and this problem
                                                       is further amplified when the prompt contains insuf-
                                                       ficient or masked information, despite the high confi-
                                                       dence displayed in the generated output, as observed
                                                       in ChatGPT.
89     Hua et al. [118]       2023     Health/         "AI hallucination" is AI-generated outputs that deviate     1
                                       Academia        from its training data. These outputs may appear
                                                       syntactically or semantically plausible, but in reality,
                                                       they are incorrect or nonsensical.
90     Brameier et al.        2023     Health/         "AI hallucination" is the production of confident re-       1
       [119]                           Academia        sponses by an NLP tool that are nonsensical or that
                                                       seem realistic but are not based on any real-world data.
91     Lee et al. [120]       2023     Health          "AI hallucination" refers to the occurrence where GPT-      260
                                                       4 produces false responses, which are often presented
                                                       in a convincing manner, potentially leading the in-
                                                       quirer to believe their accuracy.
92     Long et al. [121]      2023     Health/         "AI hallucination" refers to the phenomenon where           0
                                       Academia        Language Models (LMs), including ChatGPT, produce
                                                       outputs characterized by blatant factual errors, signifi-
                                                       cant omissions, and erroneous information generation.
93     Zhang [122]            2023     Large           "AI hallucination" refers to the generation of responses    1
                                       Language        by ChatGPT that frequently contain incorrect informa-
                                       Model           tion, and it can even extend to the generation of fake
                                                       scientific abstracts and research papers.
94     Puchert   et   al.     2023     Health          "AI hallucination" refers to a common phenomenon            1
       [123]                                           where the model includes incorrect or false infor-
                                                       mation in its responses, despite providing eloquent
                                                       answers.
95     Wang et al. [124]      2023     Health          "AI hallucination" refers to misinformation, unreason-      0
                                                       able or illogical in common-sense knowledge
96     Garg et al. [125]      2023     Health          "AI hallucination" refers to the production of content      2
                                                       that sounds authoritative but can be inaccurate, incom-
                                                       plete, or biased in nature.
97     Han et al. [126]       2023     Health          "AI hallucination" occurs when AI confidently gener-        0
                                                       ates an impressive-sounding response that may not be
                                                       justified by its training data or may even be factually
                                                       incorrect.
98     Dolan and Freer        2023     Academia        "AI hallucination" refers to the phenomenon where           0
       [127]                                           AI systems may create or invent text and citations that
                                                       sound realistic but are not based on actual information,
                                                       and in certain instances, they may even fabricate
                                                       nonexistent works.
99     Scott-Branch      et   2023     Academia        "AI hallucination" refers to mistakes in the generated      0
       al. [128]                                       text that are semantically or syntactically plausible but
                                                       are in fact incorrect or nonsensical.
100    Larsen-Ledet           2023     Academia        "AI hallucination" refers to the generation of output,      0
       [129]                                           often text, containing falsities, which undermine the
                                                       intended factual nature of the service, intended to be
                                                       reliable and trustworthy.
                                                                                                     Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                           Citation

101    Kim et al. [130]      2023     Health/         "AI hallucination" refers to the phenomenon in which        9
                                      Academia        there is no true internal understanding of the language,
                                                      and LLMs generate text that sounds plausible based
                                                      on the training data they have been exposed to.
102    Tay et al. [131]      2023     Health          "AI hallucination" refers to the situation where GPT        1
                                                      confidently generates responses that lack any justifica-
                                                      tion from training data or correspondence to real-world
                                                      input.
103    Cai et al. [132]      2023     Health          "AI hallucination" refers to the issue of accuracy and      9
                                                      reliability in LLMs, particularly concerning the high
                                                      frequency of false information.
104    Jahic et al. [133]    2023     Academia        "AI hallucination" refers to the phenomenon where           2
                                                      an AI system generates unexpected or meaningless
                                                      outputs that appear coherent and plausible to human
                                                      observers.
105    Randell     and       2023     Academia        "AI hallucination" refers to the plausible yet erroneous    0
       Coghlan [134]                                  text generated by artificial intelligence systems, re-
                                                      sembling compression artifacts, which can only be
                                                      discerned through comparison with original sources
                                                      like the web or our existing knowledge.
106    Brender [135]         2023     Academia        "AI hallucination" refers to the unfaithful or nonsen-      1
                                                      sical text occasionally generated by large language
                                                      models.
107    Pedersen [136]        2023     Academia        "AI hallucination" refers to mistakes in the generated      0
                                                      text that are semantically or syntactically plausible but
                                                      are in fact incorrect or nonsensical.
108    Munoz      et   al.   2023     Academia        “AI hallucination” which results when the system            1
       [137]                                          provides a response that is not factual.
109    Hatem      et   al.   2023     Health          "AI hallucination" is inaccurate and stigmatizing to        0
       [138]                                          both AI systems and individuals who experience hallu-
                                                      cinations. Because of this, they suggest the alternative
                                                      term "AI misinformation" as they feel this is an
                                                      appropriate term to describe the phenomenon at hand
                                                      without attributing lifelike characteristics to AI.
110    Athaluri   et   al.   2023     Health/         "AI hallucination" is a phenomenon where AI gener-          16
       [139]                          Academia        ates a convincing but completely made-up answer.
111    Gorichanaz            2023     Academia        "AI hallucination" refers to generating misinformation,     1
       [140]                                          including making false statements, citing sources that
                                                      do not exist and creating code that doesn’t work.
112    Cox et al. [141]      2023     Health          "AI hallucination" occurs when Language Models              0
                                                      (LLMs) misunderstand medical vocabulary or provide
                                                      advice that does not align with established medical
                                                      guidelines.
113    Yadava [142]          2023     Health/         "AI hallucination" is interestingly labeled as a phe-       11
                                      Academia        nomenon where responses from ChatGPT are some-
                                                      times ambiguous, nonsensical, and undesirable.
114    Dai et al. [143]      2023     Academia        "AI hallucination" is a phenomenon in which it some-        11
                                                      times produces confident but irrelevant or inaccurate
                                                      responses.
115    Lingard [144]         2023     Academia        "AI hallucination" refers to the phenomenon where           1
                                                      AI generates content that confidently presents as
                                                      legitimate-sounding material but is not real.
116    Woodland [145]        2023     Health/         "AI hallucination" refers to a situation where the          0
                                      Academia        generated text is not derived from a factual source
                                                      but instead arises from statistical predictions of words
                                                      that are likely to follow the given input.
117    Kashangura            2023     Health          "AI hallucination" is the generation or production of a     0
       [146]                                          factually invalid statement, characterized by confident
                                                      output from AI that is not correct or not real.
118    Walker     et   al.   2023     Health          “AI hallucinations” known as wrong or out of context      6
       [147]                                          answers generated by ChatGPT AI.
                                                                                                  Continued on next page
                                   TABLE V – continued from previous page
Num.      Author(s)         Year     Application                           Definition                           Citation

119    Tenge   Hansen       2023     Academia        “AI hallucination” is defined as a phenomenon where        0
       and Røsand Valø                               AI generates a convincing but completely made-up
       [148]                                         answer, often incorporating fake references for added
                                                     persuasiveness.
                                                     “Intrinsic hallucination” refers to the LLM generation
                                                     that contradicts the source/input.
                                                     “Extrinsic hallucination” refers to the LLM genera-
                                                     tions that cannot be verified from the source/input
                                                     content, i.e., output that can neither be supported nor
                                                     contradicted by the source.
120    Tupper    et   al.   2023     Academia        "AI hallucination" refers to a phenomenon wherein          0
       [149]                                         incorrect or nonsensical responses are generated in
                                                     response to prompts.
121    Borkowski et al.     2023     Health          "AI Hallucination" refers to the phenomenon often          0
       [150]                                         observed in language models like ChatGPT, charac-
                                                     terized by the generation of inaccurate information,
                                                     which result from the model’s inability to differentiate
                                                     between real and fake information sources.
122    Subramanya and       2023     Legal           "AI hallucination" occurs when AI generates content        0
       Furlong [151]                 Setting         that appears accurate but includes false references.
123    Tenzer [152]         2023     Legal           "AI hallucination" occurs when a chatbot produces          0
                                     Setting         a confident but inaccurate response to a question,
                                                     contributing to the erratic and unreliable behavior of
                                                     A.I. large language models (L.L.M.s), which may in-
                                                     clude providing false information and acting strangely
                                                     toward users.
124    Alarie and Mc-       2023     Legal           "AI hallucination" occurs when AI generates untrue         0
       Creight [153]                 Setting         information that is not backed up by real-world data.
125    Han et al. [154]     2023     Academia        "AI hallucination" refers to the situation where the       0
                                                     AI, while not presenting fictitious information like
                                                     a hallucination, draws from sources other than the
                                                     requested one and fails to indicate this, potentially
                                                     leading to incorrect conclusions when users rely on the
                                                     AI-generated text for understanding a specific paper.
126    Picht [155]          2023     Legal           "AI Hallucination" refers to the phenomenon in which       0
                                     Setting         ChatGPT generates responses that sound confident and
                                                     compelling, despite its inability to genuinely answer
                                                     the posed question.
127    Treleaven et al.     2023     Legal           "AI hallucination" is defined as a confident response      0
       [156]                         Setting         that is biased, too specialized, or even entirely incor-
                                                     rect, with the model fabricating a seemingly plausible
                                                     but factually inaccurate answer.
128    Ariyaratne [157]     2023     Legal           "AI Hallucination" refers to the term used to describe     0
                                     Setting         the phenomena of generative algorithms making up
                                                     facts.
129    Wu et al. [158]      2023     Large           "AI hallucination" occurs when the replies generated       40
                                     Language        by ChatGPT frequently contain factual errors.
                                     Model
130    Wang et al. [159]    2023     Large           "AI hallucination" occurs when ChatGPT produces re-        7
                                     Language        sponses that, despite sounding plausible, are ultimately
                                     Model           incorrect or nonsensical.
131    Amaro     et   al.   2023     Large           "AI hallucination" occurs when ChatGPT generates           0
       [160]                         Language        outputs that invent facts and concepts, as it lacks
                                     Model           objective training to assess the factual correctness of
                                                     its responses.
132    Skrodelis et al.     2023     Natural         "AI hallucination" occurs when NLGs frequently pro-        0
       [161]                         Language        duce text that is either nonsensical or not true to the
                                     Generation      original input.
133    Gupta et al. [162]   2023     Cybersecurity   "AI hallucination" refers to a phenomenon in which       7
                                                     the AI model generates inaccurate or outright false
                                                     information.
                                                                                                Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                            Citation

134    Bahrini    et   al.   2023     Chatbot         "AI hallucination" refers to ChatGPT’s limitations and       28
       [163]                                          challenges, which encompass bias and the occasional
                                                      generation of nonsensical output
135    Hamid [164]           2023     Chatbot         "AI hallucination" refers to the ’hallucinations,’ or        1
                                                      incoherent responses, observed in Language Models
                                                      (LLMs), which are essentially errors in the model’s
                                                      output as it is designed to prioritize coherence rather
                                                      than truth.
136    De Silva et al.       2023     Chatbot         "AI hallucination" occurs when ChatGPT exhibits              3
       [165]                                          anomalous behavior, producing factual inaccuracies,
                                                      logical fallacies, bias, and plagiarism in its responses,
                                                      a phenomenon popularly referred to as ’AI hallucina-
                                                      tions’ or ’stochastic parroting.’
137    Atallah [166]         2023     Health          "AI hallucination" occurs when modern LLMs pro-              1
                                                      duce fluent and grammatically correct textual outputs
                                                      that are categorically false, and in some instances,
                                                      fictitious.
138    Byrne [167]           2023     Large           "AI hallucination" refers to outputs that reflect misin-     0
                                      Language        terpretations and falsehoods.
                                      Model
139    Thirunavukarasu       2023     Health          "AI hallucination" refers to the occurrence where            30
       et al. [21]                                    inaccurate information is invented, not represented in
                                                      the training dataset, and is presented lucidly, with an
                                                      alternative term like ’fact fabrication’ being preferred.
140    Ting et al. [22]      2023     Health          AI Hallucination refers to the phenomenon where, as          5
                                                      a generative AI, ChatGPT generates outputs based on
                                                      statistical prediction of the text without human-like
                                                      reasoning, potentially resulting in plausible-sounding
                                                      but inaccurate responses, also known as "fabrication."
141    Beam et al. [168]     2023     Health          "AI hallucination" occurs when a model generates in-         0
                                                      formation that isn’t present in the input data, leading to
                                                      outputs that seem plausible but are factually incorrect
                                                      or nonsensical.
142    Madden     et   al.   2023     Health          "AI hallucination" occurs when ChatGPT generates             2
       [16]                                           false information, also called "delusions."
143    Komorowski            2023     Health          "AI hallucination" refers to the phenomenon where            4
       [169]                                          ChatGPT has the ability to confidently produce an-
                                                      swers that appear believable but may be incorrect or
                                                      nonsensical.
144    Sallam [170]          2023     Health/         "AI hallucination" refers to concerns arising from           334
                                      Academia        possible bias in ChatGPT’s training datasets, limiting
                                                      its capabilities and potentially causing factual inaccu-
                                                      racies that, surprisingly, seem scientifically plausible.
145    Rothschild [171]      2023     Health/         "AI hallucination" refers to false responses provided        0
                                      Academia        by ChatGPT3.
146    Im [172]              2023     Health          "AI hallucination" occurs when ChatGPT generates             0
                                                      responses that are unfaithful to the provided training
                                                      data.
147    Hulman     et   al.   2023     Health          "AI hallucination" refers to the occurrence of making        5
       [173]                                          up completely false information supported by fictitious
                                                      citations.
148    Cheung     et   al.   2023     Health          "AI hallucination" occurs as a phenomenon where              0
       [174]                                          the AI generates responses that are nonsensical or
                                                      unfaithful to the provided source input.
149    Moskatel    and       2023     Academia        "AI hallucination" is an inaccurate response by an AI        0
       Zhang [175]                                    that is not justified by its training data.
150    Javid et al. [176]    2023     Health/         "AI hallucination" refers to the phenomenon where the       0
                                      Academia        model produces text that is either factually incorrect
                                                      or nonsensical.
                                                                                                    Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

151    Buholayka et al.        2023     Health/         "AI hallucination" occurs when AI-generated content         4
       [177]                            Academia        is nonsensical or unfaithful to the provided source
                                                        content.
152    Alkaissi and Mc-        2023     Health/         "AI hallucination" occurs when ChatGPT provides             230
       Farlane [178]                    Academia        confident responses that appear faithful but are non-
                                                        sensical when evaluated against common knowledge.
153    Eysenbach [179]         2023     Health/         "AI hallucination" is a confident response by an ar-        128
                                        Academia        tificial intelligence system that does not seem to be
                                                        justified by its training data.
154    Østergaard and          2023     Health          "Non sequitur", Latin for “it does not follow,” is a term   0
       Nielbo [11]                                      commonly used in philosophy and rhetoric to describe
                                                        inferences not following from the premises.
                                                        They acknowledged that “non sequitur” does not cover
                                                        all false responses generated by AI models. Indeed, AI
                                                        models can also make “hasty generalizations,” i.e., the
                                                        fallacy of making (too) strong claims based on (too)
                                                        limited data.
                                                        They realized that many other terms from philosophy
                                                        and rhetoric describing logical fallacies of different
                                                        types (e.g., “false analogy,” “appeal to authority” and
                                                        “false dilemma”) could also be used to label false
                                                        responses from AI models.
155    Thorne [180]            2023     Academia        "AI hallucination" refers to occurrences where Chat-        0
                                                        GPT produces statements that are difficult to verify,
                                                        appear plausible, but do not withstand scrutiny, often
                                                        being arbitrary in nature.
156    Huang     et      al.   2023     Health/         "AI hallucination" refers to responses generated by         5
       [181]                            Academia        LLMs in a convincing appearance but are actually
                                                        incorrect statements.
157    Wecel     et      al.   2023     Large           "AI hallucination" refers to the occurrence where AI        0
       [182]                            Language        produces texts that are not consistent with reality and
                                        Model           contain confused facts.
158    Ge and Lai [18]         2023     Health/         "AI hallucination" occurs where the LLM model gen-          21
                                        Academia        erates confident, specific, and fluent answers that are
                                                        factually completely wrong. Also called "stochastic
                                                        parroting."
159    Bhatti [183]            2023     Health/         "AI hallucination" occurs when the information gen-         0
                                        Academia        erated by ChatGPT may not always be correct.
160    Bryant [25]             2023     Academia        "AI hallucination" refers to answers that are fabricated    2
                                                        when data are insufficient for an accurate response.
161    Mahyoob et al.          2023     Academia        "AI hallucination" occurs when it generates ideas with      0
       [184]                                            human-like fluency and persuasiveness but without
                                                        truth or factual accuracy.
162    Lee [185]               2023     Mathematical    "AI hallucination" occurs when GPT models generate          14
                                        Setting         outputs that are contextually implausible or inconsis-
                                                        tent with the real world.
163    Piñeiro-Martín et       2023     Ethical Set-    "AI hallucination" occurs when the LLM generates            0
       al. [186]                        ting            text that goes beyond the scope of the provided input
                                                        or fabricates information that is factually incorrect.
164    Alhaidry et al.         2023     Health          "AI hallucination" occurs when ChatGPT can produce          10
       [187]                                            answers that appear reliable but are incorrect.
165    Khurana     and         2023     Health/         "AI hallucination" occurs when AI generates sentences       1
       Vaddi [188]                      Academia        with the intent to convince the reader, which can be
                                                        misleading, particularly to inexpert readers.
166    Li et al. [189]         2023     Health          "AI hallucination" occurs when LLMs occasionally         11
                                                        generate fallacious and harmful assertions beyond
                                                        their knowledge expertise.
                                                                                                   Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                            Citation

167    Walczak       and     2023     Academia        "AI hallucination" occurs as a primary limitation asso-      0
       Cellary [190]                                  ciated with LLMs, manifesting as the tendency to gen-
                                                      erate errors, including mathematical, computational,
                                                      and conceptual inaccuracies, without any prior indica-
                                                      tion, often characterized by their deceptive plausibility,
                                                      alignment with truthful information, and conveyed in
                                                      a persuasive and self-assured manner, making their
                                                      detection challenging without careful scrutiny and
                                                      rigorous fact verification.
168    Marinaccio et al.     2023     Legal           "AI hallucination" occurs when the output is delivered       0
       [191]                          Setting         in an authoritative and convincing manner, potentially
                                                      leading uninformed users to blindly accept it as truth,
                                                      despite its inaccuracies or falsehoods.
169    Gebrael    et   al.   2023     Health          "AI hallucination" occurs when AI models, particu-           3
       [192]                                          larly in cases like ChatGPT, generate outputs that ap-
                                                      pear plausible but are factually incorrect or unrelated
                                                      to the input context.
170    Tiwari     et   al.   2023     Health          "AI hallucination" occurs as false knowledge that            5
       [193]                                          appears convincing from a scientific perspective.
171    Bhattacharyya         2023     Health/         "AI hallucination" occurs as a phenomenon where              10
       [194]                          Academia        nonsensical or inaccurate content is generated.
172    Sriwastwa et al.      2023     Academia        "AI hallucination" refers to the phenomenon in Chat-         0
       [23]                                           GPT output where the text is cogent but not necessar-
                                                      ily true, often presenting as a complete fabrication.
173    Dossantos et al.      2023     Health          "AI hallucination" occurs when ChatGPT produces in-          1
       [195]                                          accurate results, particularly in specialized topics, due
                                                      to a lack of depth and inaccurate details retrieved from
                                                      the LLM’s database, with no assurance that ChatGPT’s
                                                      suggestions adhere to evidence-based guidelines or
                                                      best practices.
174    Loos et al. [196]     2023     Academia        "AI hallucination" occurs as a phenomenon charac-            0
                                                      terized by the generation of incorrect or outdated
                                                      information, accompanied by the failure to provide
                                                      reliable sources to evaluate the generated content.
175    Koga [197]            2023     Academia        "AI hallucination" occurs when LLMs generate seem-           0
                                                      ingly credible but fabricated information, particularly
                                                      concerning when they create fictitious citations.
176    Birenbaum [198]       2023     Academia        "AI hallucination" occurs when the model fabricates          3
                                                      information and provides untraceable references to
                                                      support its claims.
177    Cascella    et al.    2023     Health          "AI hallucination" refers to the ability of ChatGPT          182
       [199]                                          to produce answers that sound believable but may be
                                                      incorrect or nonsensical.
178    Aronson [200]         2023     Health/         "AI hallucination" refers to all the pieces of misinfor-     0
                                      Academia        mation generated.
179    Kumar      et   al.   2023     Academia        "AI hallucination" refers to instances where an AI           0
       [201]                                          chatbot generates fictional, erroneous, or unsubstan-
                                                      tiated information in response to queries.
180    Gravel et al. [24]    2023     Academia        ChatGPT fabricated a convincing response that con-           19
                                                      tained several factual errors.
181    Ferres     et   al.   2023     Health          "AI hallucination" occurs when a model generates             8
       [202]                                          content that has no basis in reality, creating entirely
                                                      made-up stories or facts.
182    Varghese    and       2023     Health          "AI hallucination" occurs when faced with topics             1
       Chapiro [203]                                  that the model has not received adequate training or
                                                      supervision for, resulting in fabricated output delivered
                                                      with a strong sense of certainty.
183    Dunn and Cian-        2023     Health          "AI hallucination" occurs as a limitation, characterized      1
       flone [204]                                    by the lack of transparency in how the output is
                                                      generated.
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

184    Theodosiou and          2023     Health          "AI hallucinations" is the phenomenon in which a gen-       5
       Read [205]                                       erative AI tool confidently asserts a factual inaccuracy.
185    Fesenmaier and          2023     Academia        "AI hallucination" occurs when ChatGPT appears to           0
       Wober [206]                                      create fake or inaccurate findings that seem plausible.
186    Sohail     et     al.   2023     ChatGPT         "AI hallucination" occurs when ChatGPT generates            5
       [207]                                            new data or information that does not exist.
187    Karakas     et    al.   2023     Health          "AI hallucination" occurs when the AI sometimes             0
       [14]                                             generates responses that sound plausible but are, in
                                                        fact, incorrect. Also called "confabulation."
188    Lyons et al. [208]      2023     Health          "AI hallucination" occurs when LLMs create re-              0
                                                        sponses to prompts by sampling from the language
                                                        distribution within their dataset, potentially leading
                                                        to the generation of incorrect information and the
                                                        propagation of biases.
189    Opdahl     et     al.   2023     Journalism      "AI hallucination" occurs when the model generates          1
       [209]                                            plausible-sounding nonsense, including texts that con-
                                                        tain elements not found in the input data.
190    Blanchard et al.        2023     Health/         "AI hallucination" occurs when probabilities from the       2
       [210]                            Academia        transformer model architecture can generate made-up
                                                        answers, taking numerous forms, from false references
                                                        in scientific reports to misinformation in journalistic
                                                        articles or incorrect formulas when debugging pro-
                                                        gramming code.
191    Lim et al. [211]        2023     Health          "AI hallucination" occurs when LLMs lack domain-            1
                                                        specific capabilities, rendering them susceptible to
                                                        generating convincing yet potentially inaccurate re-
                                                        sponses.
192    Kim et al. [212]        2023     Health          "AI hallucination" occurs as the possibility of provid-     0
                                                        ing incorrect information or exhibiting errors in the
                                                        inference process.
193    Alqahtani et al.        2023     Academia        "AI hallucination" refers to generating non-existent or     15
       [213]                                            incorrect content and other related concerns associ-
                                                        ated with limited contexts, reliability, and the lack of
                                                        learning from experience.
194    Jairoun    et     al.   2023     Health          "AI hallucination" occurs as statements that are factu-     3
       [214]                                            ally inaccurate yet appear plausible to the layman.
195    Šlapeta [215]           2023     Health          "AI hallucination" occurs when highly confident an-         12
                                                        swers are returned by AI that cannot be explained by
                                                        the training data alone.
196    Dillion    et     al.   2023     Science         "AI hallucination" occurs as outputs that appear sen-       28
       [216]                                            sical but are inaccurate.
197    Thomson et al.          2023     Text Sum-       "AI hallucination" occurs when the output text in-          3
       [217]                            marization      cludes an attribute that was not present in the input
                                                        data.
198    Balas     and    Ing    2023     Health          "AI hallucination" refers to producing confident re-        19
       [218]                                            sponses that sound plausible yet are factually incor-
                                                        rect.
199    Salah et al. [219]      2023     Science         "AI hallucination" refers to information that sounds        2
                                                        plausible but is entirely fabricated or not supported
                                                        by the input data.
200    Ilicki et al. [220]     2023     Health          "AI hallucination" refers to text responses that are        3
                                                        either nonsensical or unfaithful to the content they
                                                        should use.
201    Jansen     et     al.   2023     Survey Set-     "AI hallucination" refers to generating nonsensical or      2
       [221]                            ting            inappropriate responses to survey questions.
202    Casal         and       2023     Academia        "AI hallucination" refers to the tendency to invent         1
       Kessler [222]                                    content.
203    Qi et al. [223]         2023     Health          "AI hallucination" occurs when ChatGPT generates             11
                                                        responses that appear plausible but require correction,
                                                        including the invention of terms it is familiar with.
                                                                                                       Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                              Citation

204    Lin [224]               2023     Academia        "AI hallucination" occurs when LLMs fabricate facts,           22
                                                        creating confident-sounding statements and legitimate-
                                                        looking citations that are false.
205    Janssen      et   al.   2023     Health          "AI hallucination" occurs where the model generates            16
       [225]                                            text that is factually incorrect or nonsensical, despite
                                                        appearing confident in its ability.
206    Shen et al. [226]       2023     Health          "AI hallucination" occurs when LLMs produce seem-              235
                                                        ingly credible but incorrect responses, including the
                                                        invention of terms they should be familiar with.
207    Thirunavukarasu         2023     Health          "AI hallucination" occurs when chatGPT describes in-           23
       [227]                                            accurate information as lucidly as it does with correct
                                                        facts.
208    Qadir [228]             2023     Academia        "AI hallucination" occurs when ChatGPT generates               157
                                                        nonsensical or false information (misinformation).
209    Frieder      et   al.   2023     Mathematical    "AI hallucination" occurs when GPT, after answering            126
       [229]                            Setting         correctly or incorrectly, tells the user unrelated infor-
                                                        mation.
210    Borji [20]              2023     ChatGPT         "AI hallucination" refers to inaccuracies in informa-          158
                                                        tion or statements that are not in accordance with
                                                        reality or the truth, often unintentional but resulting
                                                        in incorrect or misleading information, particularly in
                                                        the context of chatbots. Also called "Factual errors."
211    OpenAI [230]            2023     ChatGPT         "AI hallucination" occurs when ChatGPT produces                0
                                                        content that is nonsensical or untruthful in relation to
                                                        certain sources.
212    Manakul et al.          2023     Large           "AI hallucination" refers to the tendency of LLMs to           48
       [231]                            Language        hallucinate facts and fabricate information.
                                        Model
213    Nori et al. [232]       2023     Health          "AI hallucination" refers to erroneous generations by          127
                                                        ChatGPT.
214    Li et al. [233]         2023     Large           "AI hallucination" occurs when LLMs generate con-              4
                                        Language        tent that conflicts with the source or cannot be verified
                                        Model           by factual knowledge.
215    Zhao et al. [234]       2023     Large           "Intrinsic hallucination" refers to the generated infor-       188
                                        Language        mation in conflict with the existing source.
                                        Model           "Extrinsic hallucination" refers to the generated infor-
                                                        mation that cannot be verified by the available source.
216    Adlakha et al.          2023     Question        "AI hallucination" occurs when conversational models           5
       [235]                            Answering       produce factually incorrect or unsupported statements.
217    Athavale et al.         2023     Health          "AI hallucination" refers to generating syntactically          0
       [236]                                            correct but factually incorrect responses that seem
                                                        plausible.
218    Chen et al. [237]       2023     Health          "AI hallucination" occurs when ChatGPT makes as-               1
                                                        sumptions on details not provided in the input data.
219    Stahl and Eke           2023     ChatGPT         "AI hallucination" refers to mistakes that ChatGPT             1
       [238]                                            makes when generating text that is semantically cor-
                                                        rect but factually incorrect or even nonsensical.
220    Walters      and        2023     Academia        "AI hallucination" occurs when ChatGPT provides                0
       Wilder [239]                                     factually incorrect responses.
221    Munro and Hope          2023     Health/         "AI hallucination" occurs when ChatGPT provided                0
       [240]                            Academia        confident responses that seemed faithful and nonsen-
                                                        sical.
222    Hashimoto and           2023     Health/         "AI hallucination" occurs when LLM-generated text               1
       Johnson [241]                    Academia        is based on the statistical associations of patterns of
                                                        words to those seen in training data and prompts, re-
                                                        sulting in elements such as unnecessary and unnatural
                                                        repetition, lack of clarity or specificity, out-of-context
                                                        content, inconsistent phrasing and arguments (particu-
                                                        larly across long essays), incorrect word selection, and
                                                        occasional incorrect statements presented as facts.
                                                                                                          Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                           Definition                           Citation

223    Jain [242]             2023     ChatGPT         "AI hallucination" occurs when ChatGPT occasionally        0
                                                       generates responses that appear correct but are nonsen-
                                                       sical and discordant from real-world data, resulting in
                                                       data inaccuracy.
224    Ho et al. [243]        2023     ChatGPT         "AI hallucination" occurs when ChatGPT "halluci-           0
                                                       nates" false citations that appear convincingly real but
                                                       cannot be located in any medical database.
225    Currie [244]           2023     ChatGPT         "AI hallucination" refer to false or misleading infor-     21
                                                       mation produced by AI. A hallucination is a plausible
                                                       response that is incorrect (it seems correct to ChatGPT
                                                       but is not).
226    Liu et al. [245]       2023     Question        "AI hallucination" is LLM-generated answer which is        1
                                       Answering       not factual-grounded and sometimes severely wrong.
227    Campbell      and      2023     Generative      "AI hallucination" refers to the propensity to generate    0
       Jovanović [246]                AI              assertions with no factual data, often deceives users
                                                       into believing they are accurate.
228    Kshetri [247]          2023     Generative      "AI hallucination" refers to ChatGPT’s results that are    0
                                       AI              incomplete or misleading.
229    Ali et al. [248]       2023     Health          "AI hallucination" occurs when AI creates un-              1
                                                       grounded, subtly incorrect information without self-
                                                       awareness.
230    Tay [249]              2023     Health/         "AI hallucination" refers to the phenomenon by which       0
                                       Academia        ChatGPT could convincingly produce factually inac-
                                                       curate statements
231    Xu and Cohen           2023     Text Sum-       "AI hallucination" occurs when the model confidently       1
       [250]                           marization      generates false information.
232    Kernan Ferier et       2023     Large           "AI hallucination" commonly refers to the                  0
       al. [251]                       Language        phenomenon where LLMs occasionally generate
                                       Model           outputs that, although appearing plausible, deviate
                                                       from the user input, previously generated context, or
                                                       factual knowledge.

                                                       "Intrinsic hallucination" refers to contradictions
                                                       between source material (e.g., training data and
                                                       prompts), where generated content contradicts the
                                                       information present in the source material.

                                                       "Extrinsic hallucination" refers to information
                                                       generated that cannot be verified by the source
                                                       material, and may include content that lacks support
                                                       or contradiction within the provided source data.
233    Tsai et al. [252]      2023     Legal           "AI hallucination" occurs when LLMs generate mis-          3
                                       Setting         leading text and information.
234    Xin et al. [253]       2023     Dialogue        "AI hallucination" refers to the occurrence where pre-     0
                                                       trained text generation models occasionally generate
                                                       text that is nonsensical or unfaithful to the provided
                                                       source input.
235    Murgia      et   al.   2023     ChatGPT         "AI hallucination" occurs when generative LLMs gen-        1
       [254]                                           erate unintended text, leading to degraded system per-
                                                       formance and unmet user expectations in real-world
                                                       scenarios.
236    Kunze     et     al.   2023     Large           "AI hallucination" refers to the phenomenon where the      0
       [255]                           Language        GPT model appears to be designed to provide incorrect
                                       Model           answers rather than admit this to its users.
237    Cascella [256]         2023     Academia        "AI hallucination" refers to ChatGPT generating an-        0
                                                       swers that sound credible but may be incorrect or
                                                       nonsensical.
238    Watanabe [257]         2023     Government      "AI hallucination" refers to the phenomenon where AI        0
                                                       text generators produce statements that are consistent
                                                       with the system’s internal logic but are not based on
                                                       any true context or source.
                                                                                                     Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                           Citation

239    Nakaura et al.        2023     Health          "AI hallucination" occurs when LLMs like GPT series         0
       [258]                                          have the potential to generate inaccurate content.
240    Feuerriegel et al.    2023     Generative      "AI hallucination" refers to mistakes in the generated      0
       [259]                          AI              text that are semantically or syntactically plausible but
                                                      are actually nonsensical or incorrect. In other words,
                                                      the generative AI model produces content that is not
                                                      based on any facts or evidence, but rather on its
                                                      own assumptions or biases. Moreover, the output of
                                                      generative AI, especially that of LLMs, is typically
                                                      not easily verifiable.
241    Hryciw    et    al.   2023     Health          "AI hallucination" occurs when AI algorithms, which         0
       [260]                                          are not infallible, produce false or misleading infor-
                                                      mation.
242    Bran et al. [261]     2023     Generative      "AI hallucination" refers to nonsense expressed in an       5
                                      AI              authoritative tone and filling knowledge gaps with
                                                      falsehoods.
243    Sovrano    et   al.   2023     ChatGPT         "AI hallucination" occurs when content is generated         0
       [262]                                          that does not maintain fidelity to a given context or
                                                      source.
244    Mishra    et    al.   2023     ChatGPT         "AI hallucination" occurs when models produce out-          0
       [263]                                          puts that are not grounded in their training data.
245    Lam et al. [264]      2023     Legal           "AI hallucination" refers to LLMs output results are        1
                                      Setting         not realistic, do not follow user given context or match
                                                      any data patterns that it has been trained on.
246    Wan et al. [265]      2023     Text Sum-       "AI hallucination" occurs when the generated sum-           4
                                      marization      mary contains facts or entities not present in the
                                                      original document.
247    Houston     and       2023     Academia        "AI hallucination" commonly refers to the phe-              2
       Corrado [266]                                  nomenon where LLMs occasionally generate outputs
                                                      that, although appearing plausible, deviate from the
                                                      user input, previously generated context, or factual
                                                      knowledge.
248    González-Mora         2023     Natural         "AI hallucination" refers to the generation of texts that   2
       et al. [267]                   Language        are apparently well-written but unsubstantiated and not
                                      Generation      faithful to the provided data.
249    Lim et al. [268]      2023     Dialogue        "AI hallucination" refers to situations where the gen-      0
                                                      erated output contradicts the reference knowledge and
                                                      includes instances when the generated output cannot
                                                      be confirmed from the knowledge source.
250    Alowais    et   al.   2023     Health          "AI hallucination" refers to the tendency of AI-            2
       [269]                                          generated data and/or analysis to fabricate and create
                                                      false information that cannot be supported by existing
                                                      evidence, even though it may appear realistic and
                                                      convincing.
251    Liu et al. [270]      2023     Health          "AI hallucination" can be defined as the capability of      33
                                                      LLM-based chatbots to convey false information as
                                                      though it were true.
252    Xie et al. [271]      2023     Health          "AI hallucination" refers to when a medical AI              0
                                                      system is considered unfaithful or to have a factual
                                                      inconsistency issue, as it generates content that is not
                                                      supported by existing knowledge, reference, or data.

                                                      Intrinsic Error: The generated output contradicts
                                                      with existing knowledge, reference, or data.

                                                      Extrinsic Error: The generated output cannot be
                                                      confirmed (either supported or contradicted) by
                                                      existing knowledge, reference, or data.
253    Bernstein et al.      2023     Health          "AI hallucination" refers to chatbot outputs that sound      0
       [272]                                          convincingly plausible yet are factually inaccurate.
                                                                                                     Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                           Definition                           Citation

254    Heck [273]              2023     Health          "AI hallucination" refers to ChatGPT providing text        3
                                                        that combines real and fabricated evidence, resulting
                                                        in plausible answers, occasionally appearing as
                                                        nonsense when assessed according to the common
                                                        knowledge of experts in different areas, also called
                                                        "mistakes."

                                                        “Intrinsic hallucinations” occur when the generation
                                                        output contradicts the source content.

                                                        “Extrinsic hallucinations” occur when the output
                                                        can neither be supported nor contradicted by the
                                                        source.
255    Ray             and     2023     Health          "AI hallucination" refers to the generation of inaccu-     0
       Majumder                                         rate or false information by AI-based systems such as
       [274]                                            ChatGPT.
256    Ang et al. [275]        2023     Health/         "AI hallucination" refers to the phenomenon where          5
                                        Academia        text generated by ChatGPT may appear credible but
                                                        can be pure confabulation, containing a combination
                                                        of both facts and fabricated information, or entirely
                                                        fictitious pseudoscientific material.
257    Talyshinskii       et   2023     Health/         "AI hallucination" refers to a phenomenon in writing       0
       al. [276]                        Academia        influenced more by learned patterns than scientific
                                                        facts, which leads to mistakes.
258    Kung et al. [277]       2023     Health          "AI hallucination" refers to the phenomenon where          0
                                                        ChatGPT, while citing a verifiable source, may draw
                                                        information that is outdated or entirely incorrect, de-
                                                        spite providing logical justifications for its answer
                                                        choices, thus leading to logical errors and assertions
                                                        of false facts.
259    Friederichs et al.      2023     Health/         "AI hallucination" refers to the behavior in which         5
       [278]                            Academia        wrong answers are just as convincingly justified as
                                                        correct ones, a phenomenon not uncommon in large
                                                        language models.
260    Ghorashi et al.         2023     Health/         "AI hallucination" occurs when chatbots have the           0
       [279]                            Academia        potential to falsify and create references.
261    McBee     et     al.    2023     Sport           "AI hallucination" occurs as the generation of unsup-      0
       [280]                                            ported or false information, which is a prevalent issue
                                                        with LLM-based chatbots.
262    Reis [281]              2023     Health          "AI hallucination" may generate plausible sounding         1
                                                        but incorrect or nonsensical answers that does not
                                                        seem to be justified by its training data, such as claim
                                                        to be human.
263    Joachimiak et al.       2023     Genetic         "AI hallucination" ocuurse when LLM model fabri-           0
       [282]                                            cated a term for a gene set.
264    Chen [283]              2023     Academia        "AI hallucination" refers to false or nonsense informa-    0
                                                        tion presented as fact by an LLM.
265    Delsoz    et     al.    2023     Health/         "AI hallucination" occurs when ChatGPT generates           0
       [284]                            ChatGPT         responses that appear fluent and believable but may
                                                        contain factual inaccuracies.
266    Takagi    et     al.    2023     Health/         "AI hallucination" is defined as producing nonsensical     11
       [285]                            ChatGPT         or untruthful content concerning certain sources.
267    Kaneda     et    al.    2023     Health/         "AI hallucination" occurs when ChatGPT provides            3
       [286]                            ChatGPT         erroneous information in a naturalistic manner.
268    Ahn [287]               2023     Health          "AI hallucination" refers to the generation of false       12
                                                        or logically incorrect text that appears plausible and
                                                        grammatically correct.
269    Jin et al. [288]        2023     Health          "AI hallucination" refers to the occurrence where        0
                                                        LLMs usually produce plausible-sounding but incor-
                                                        rect outputs.
                                                                                                   Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                           Definition                           Citation

270    Ahn [289]               2023     Health/         "AI hallucination" refers to a phenomenon where            0
                                        ChatGPT         outputs may deviate from factual accuracy or provided
                                                        context.
271    Liu et al. [290]        2023     Health/         “AI hallucination” refers to the fact that the content     20
                                        ChatGPT         generated by the model is not based on reality, creating
                                                        a completely fabricated story or fact.
272    Doorn [291]             2023     Water           "AI hallucination" refers to LLMs’ tendency to gen-        0
                                        Domain          erate factually nonsensical or incorrect text.
273    Hiesinger et al.        2023     Health          "AI hallucination" refers to the propensity of LLMs        3
       [292]                                            to generate factually incorrect statements.
274    Palal et al. [293]      2023     Health/         "AI hallucination" occurs when ChatGPT generates           0
                                        ChatGPT         inaccurate or contradictory information.
275    Coskun     et     al.   2023     Question        "AI hallucination" refers to an instance where the         0
       [294]                            Answering/      model generates information that is not supported by
                                        ChatGPT         existing evidence or factual data.
276    Baldassarre et al.      2023     ChatGPT         "AI hallucination" refers to the phenomenon of inac-       0
       [295]                                            curate information.
277    Boujemaa et al.         2023     Retail          "AI hallucination" occures where the model produces        0
       [296]                                            untruthful or misleading information.
278    Li et al. [297]         2023     Dialogue        "AI hallucination" are NLP generated content that          0
                                                        appear to be relevant bbut are not actually faithful to
                                                        the underlting text.
279    Urban     et      al.   2023     Database        "AI hallucination" is a phenomenon where LLMs              1
       [298]                                            generate non-factual statements.
280    Mahon      et     al.   2023     Academia        "AI hallucination" is the generation of output that        0
       [299]                                            appears convincing but is factually untrue or unrelated
                                                        to the current context.
281    August     et     al.   2023     Health          "AI hallucination" refers to the limitation in current     20
       [300]                                            text generation capabilities, which carries the risk of
                                                        generating factually incorrect or inconsistent text.
282    Fischer [301]           2023     Legal           "AI hallucination" refers to a phenomenon where an         0
                                        Setting         LM, or Language Model, is a system for haphazardly
                                                        stitching together sequences of linguistic forms it
                                                        has observed in its vast training data, according to
                                                        probabilistic information about how they combine,
                                                        but without any reference to meaning. Also called
                                                        "Stochastic Parrots."
283    Lee et al. [302]        2023     Dialogue        "AI hallucination" are LLMs generated information          3
                                                        that is non-factual or nonsensical.
284    Scotti et al. [303]     2023     Chatbot         "AI hallucination" refers to the case where agents         0
                                                        generate responses without actually knowing the infor-
                                                        mation it is talking about or without referring to some
                                                        knowledge base, leading to possibly wrong/misleading
                                                        information.
285    Zhan et al. [304]       2023     ChatGPT         "AI hallucination" refers to the phenomenon, also          0
                                                        known as the "hallucination effect," where chatbots
                                                        like ChatGPT may generate misleading and deceptive
                                                        information that can have adverse impacts on users
                                                        who may struggle to distinguish fact from fiction.
286    Vargas-Murillo          2023     Academia        "AI hallucination" refers to the phenomenon, also          0
       et al. [305]                                     known as the "hallucination effect," which causes an
                                                        AI to invent familiar terms.
287    White et al. [306]      2023     Text Sum-       "AI hallucination" refers to the tendency to produce       0
                                        marization      content that is nonsensical or untruthful in relation to
                                                        certain sources.
288    Cusumano [307]          2023     Technology      "AI hallucination" occurs when LLMs, in the absence       0
                                                        of an answer to a query, use predictive analytics
                                                        to make up reasonable but sometimes incorrect re-
                                                        sponses.
                                                                                                    Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

289    Nashid       et   al.   2023     Large           "AI hallucination" refers to ChatGPT’s outputs that are     12
       [308]                            Language        incorrect or nonsensical.
                                        Model
290    Pataranutaporn et       2023     Technology      "AI hallucination" refers to the phenomenon where           2
       al. [309]                                        ChatGPT generates content that is nonsensical or
                                                        unfaithful to the provided source content.
291    Faggioli     et   al.   2023     Large           "AI hallucination" occurs when LLMs generate text           0
       [310]                            Language        that contains inaccurate or false information, often
                                        Model           in an affirmative manner that makes it difficult for
                                                        humans to even suspect errors.
292    Marczak-Czajka          2023     Human-          "AI hallucination" occurs when generative models pro-       0
       and    Cleland-                  Value Story     duce output that is nonsensical with incorrect, bizarre
       Huang [311]                                      logic.
293    Stefńnska et al.       2023     Quantum         "AI hallucination" occurs when LLMs generate data           0
       [312]                            Physics         not observed in the training dataset.
294    Ezenkwu [313]           2023     Customer        "AI hallucination" occurs when ChatGPT generates            0
                                        Service         responses that are irrelevant or incorrect.
295    Fayyad [314]            2023     ChatGPT         "AI hallucination" occurs when generative AI models         0
                                                        lose track of the source of information, lacking reason-
                                                        ing capability or semantic understanding, and instead
                                                        resort to autocompletion through pattern matching,
                                                        resulting in making things up.
296    Mrabet      and         2023     ChatGPT         "AI hallucination" refers to The AI’s inability to          2
       Studholme [315]                                  understand what it has written is clear.
297    Pitt [316]              2023     Academia        "AI hallucination" occurs when the AI/LLM produces          0
                                                        a plausible output that, however, does not seem to be
                                                        warranted by the training data.
298    Crosthwaite and         2023     ChatGPT         "AI hallucination" occurs when ChatGPT invents              1
       Baisa [317]                                      terms that lie outside of its training data.
299    Vinny [318]             2023     Health          "AI hallucination" occurs when LLMs generate erro-          0
                                                        neous medical information to support their opinions
300    Solyman et al.          2023     Grammar         "AI hallucination" occurs when the system produces          4
       [319]                                            translations that are completely inadequate due to an
                                                        overreliance on the target context in NMT.
301    Waqas        et   al.   2023     Health          "AI hallucination" refers to a known limitation of          0
       [320]                                            generative AI, encompassing mistakes in the generated
                                                        text or images that are semantically, syntactically, or
                                                        visually plausible but are, in fact, incorrect, nonsensi-
                                                        cal, and do not refer to any real-world concepts.
302    Stephens et al.         2023     Health          "AI hallucination" occurs when chatbots may propa-          0
       [321]                                            gate erroneous information or even make up informa-
                                                        tion.
303    Dien [322]              2023     Health/         "AI hallucination" refers to the highly susceptible         5
                                        Academia        nature of ChatGPT to produce erroneous outputs.
304    Abu-Farha et al.        2023     Health          "AI hallucination" refers to the generation of scien-       0
       [323]                                            tifically false content that might seem convincing to
                                                        nonexperts.
305    Tippareddy et al.       2023     Health/         "AI hallucination" refers to confident responses gen-       0
       [324]                            Academia        erated by ChatGPT without being justified by training
                                                        data.
306    Oviedo-                 2023     ChatGPT         "AI hallucination" refers to the occurrence where           18
       Trespalacios                                     ChatGPT can produce answers that appear credible
       et al. [325]                                     but may be incorrect or nonsensical.
307    Sarraju      et   al.   2023     Health/         "AI hallucination" refers to Inaccurate information         0
       [326]                            ChatGPT         may be presented in a confident manner, including
                                                        nonexistent references to scientific literature. Also
                                                        called "confabulation."
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

308    Coskun    et      al.   2023     Health/         "AI hallucination" refers to situations where the AI        0
       [327]                            ChatGPT         generates responses that are not derived from its
                                                        training data, potentially leading to inaccuracies or
                                                        misunderstandings.
309    Lam [328]               2023     Health          "AI hallucination" refers to occurrences where AI           0
                                                        models output plausible but factually incorrect infor-
                                                        mation.
310    Pantanowitz             2023     Health/         "AI hallucination" occurs when the AI imagines facts        0
       and Pantanowitz                  ChatGPT         that are not real as being real or makes reasoning errors
       [329]                                            that it should not be making.
311    Boussen et al.          2023     Health/         "AI hallucination" refers to the occurrence where a         0
       [330]                            Academia        model might ’invent’ information that seems plausible
                                                        based on the patterns and structures it has learned.
312    Schlam    et      al.   2023     Health/         "AI hallucination" refers to the phenomenon where           0
       [331]                            Academia        NLPs often make subtle mistakes.
313    Lyu and        Wu       2023     Academia        "AI hallucination" occurs when LLM responses can            0
       [332]                                            sometimes be repetitive or contain non-factual infor-
                                                        mation.
314    Sparkes [333]           2023     Chatbot         "AI hallucination" refers to the phenomenon where an        0
                                                        AI, in response to prompts, will produce convincing
                                                        statements that are actually inaccurate or totally false
315    Bhatia       and        2023     Academia        "AI hallucination" occurs when the AI sometimes             0
       Kulkarni [334]                                   writes plausible-sounding but incorrect, nonsensical
                                                        answers.
316    Chatelan et al.         2023     Health          "AI hallucination" occurs when ChatGPT makes up             0
       [335]                                            or distorts facts, including the creation of made-up
                                                        references.
317    Lareyre   et      al.   2023     Health          "AI hallucination" refers to the phenomenon where the       0
       [336]                                            model can generate content output that is incorrect or
                                                        nonsensical, despite appearing reliable.
318    Wilkins [337]           2023     Health          "AI hallucination" occurs when the system erro-             0
                                                        neously generates “fantastical, unfaithful, or nonsen-
                                                        sical outputs.”
319    Piazza    et      al.   2023     Academia        "AI hallucination" refers to the phenomenon where           0
       [338]                                            ChatGPT may generate output that is grammatically
                                                        correct and coherent but may not be appropriate for
                                                        the intended audience or purpose.
320    Scanlon   et      al.   2023     Academia        "AI hallucination" refers to the phenomenon where           2
       [339]                                            ChatGPT prioritize generating humanlike text in re-
                                                        sponse to a prompt, often leading them to focus
                                                        on providing an answer rather than the correct one,
                                                        resulting in inaccurate or incorrect responses presented
                                                        to users with an unfounded confidence, and even gen-
                                                        erating fake bibliographic information when asked for
                                                        references, which could be misleading if not critically
                                                        examined.
321    Kaneda [340]            2023     Health          "AI hallucination" refers to a phenomenon where             1
                                                        ChatGPT generates plausible but untrue responses.
322    Tan et al. [341]        2023     Health          "AI hallucination" refers to the occurrence where in-       0
                                                        vented, inaccurate statements are presented as lucidly
                                                        as accurate information. Also called "fact fabrication."
323    Ai et al. [342]         2023     Information     "AI hallucination" occurs when LLMs may occasion-           3
                                        Retrieval       ally generate erroneous or nonsensical responses.
324    Ruma et al. [343]       2023     Natural         "AI hallucination" occurs when an NLG system gen-           0
                                        Language        erates unfaithful or nonfactual content.
                                        Generation
325    Mao et al. [344]        2023     Large           "AI hallucination" refers to a scenario where the       0
                                        Language        generated content by LLMs appears plausible but is,
                                        Model           in fact, entirely fictional.
                                                                                                  Continued on next page
                                 TABLE V – continued from previous page
Num.      Author(s)       Year     Application                           Definition                           Citation

326    Chaiken [345]      2023     Health          "Hallucination in AI" refers to the generation of out-     0
                                                   puts that may sound plausible but are either factually
                                                   incorrect or unrelated to the given context. These out-
                                                   puts often emerge from the AI model’s inherent biases,
                                                   lack of real-world understanding, or training data lim-
                                                   itations. In other words, the AI system "hallucinates"
                                                   information that it has not been explicitly trained on,
                                                   leading to unreliable or misleading responses.
327    Otaki [346]        2023     Academia        "AI hallucination" refers to the production of inaccu-     0
                                                   rate or logically incorrect text that appears believable
                                                   and grammatically sound.
328    Triguero et al.    2023     Academia        "AI hallucination" occurs when in LLMs, the system         0
       [347]                                       may output untrue statements with high confidence.
329    Huang [348]        2023     Health/         "AI hallucination" occurs when ChatGPT produces            0
                                   Academia        content that may cause users to depend on measures
                                                   making it challenging to determine the accuracy of
                                                   specific information.
330    Muranga et al.     2023     Academia        "AI hallucination" refers to the phenomenon where AI       0
       [349]                                       often produces completely false information conveyed
                                                   in a convincing manner, including the invention of
                                                   items such as references and citations.
331    Polverini    and   2023     Physics         "AI hallucination" refers to the occurrence where          0
       Gregorcic [350]                             LLM-generated output contains factually incorrect
                                                   statements.
332    Rajendran et al.   2023     Hardware        "AI hallucination" refers to a phenomenon where AI         0
       [351]                       Tech            models create outputs that appear seemingly correct
                                                   and confident but are, in fact, factually wrong.
333    Lobentanzer and    2023     Biomedicine     "AI hallucination" refers to the phenomenon where          0
       Saez-Rodriguez                              LLMs make up facts as they go along, and, to make
       [352]                                       matters worse, are convinced - and convincing - re-
                                                   garding the truth of their hallucinations.
