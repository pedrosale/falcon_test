                                            AI Hallucinations: A Misnomer Worth Clarifying
                                                               Negar Maleki                                Balaji Padmanabhan                           Kaushik Dutta
                                                       University of South Florida                        University of Maryland                  University of South Florida
                                                         negarmaleki@usf.edu                               bpadmana@umd.edu                            duttak@usf.edu




                                           Abstract—As large language models continue to advance in                           To date, a precise and universally accepted definition of
                                        Artificial Intelligence (AI), text generation systems have been                    "hallucination" remains absent in the discussions related to this
                                        shown to suffer from a problematic phenomenon termed often                         in the increasingly broader field of AI [8]. Diverse definitions,
                                        as “hallucination.” However, with AI’s increasing presence across
arXiv:2401.06796v1 [cs.CL] 9 Jan 2024




                                        various domains including medicine, concerns have arisen re-                       or implied interpretations, persist; sometimes even contradic-
                                        garding the use of the term itself. In this study, we conducted a                  tory, as previously highlighted within the field of computer
                                        systematic review to identify papers defining “AI hallucination”                   vision where multiple, disparate interpretations coexist under
                                        across fourteen databases. We present and analyze definitions                      the same term.
                                        obtained across all databases, categorize them based on their                         Beyond the AI context, and specifically in the medical
                                        applications, and extract key points within each category. Our
                                        results highlight a lack of consistency in how the term is used,                   domain, the term "hallucination" is a psychological concept
                                        but also help identify several alternative terms in the literature.                denoting a specific form of sensory experience [9]. Ji et al.
                                        We discuss implications of these and call for a more unified effort                [10], from the computer science perspective (in ACM Comput-
                                        to bring consistency to an important contemporary AI issue that                    ing Surveys), rationalized the use of the term "hallucination"
                                        can affect multiple domains significantly1 .                                       as "an unreal perception that feels real" by drawing from
                                           Index Terms—AI, Hallucination, Generative AI
                                                                                                                           Blom’s definition — "a percept, experienced by a waking
                                                                                                                           individual, in the absence of an appropriate stimulus from the
                                                                   I. I NTRODUCTION                                        extracorporeal world." On the other hand, Østergaard et al.
                                           One of the early uses of the term "hallucination" in the field                  [11], from the medical perspective (in Schizophrenia Bulletin,
                                        of Artificial Intelligence (AI) was in computer vision, in 2000                    one of the leading journals in the discipline), raised critical
                                        [1], where it was associated with constructive implications                        concerns regarding even the adoption of the "hallucination"
                                        such as super-resolution [1], image inpainting [2], and image                      terminology in AI for two primary reasons: 1) The "halluci-
                                        synthesis [3]. Interestingly, in this context hallucination was                    nation" metaphor in AI from this perspective is a misnomer,
                                        regarded as a valuable asset in computer vision rather than                        as AI lacks sensory perceptions, and errors arise from data
                                        an issue to be circumvented. For instance, an image with low                       and prompts rather than the absence of stimuli, and 2) this
                                        resolution might have been rendered more useful with careful                       metaphor is highly stigmatizing, as it associates negative issues
                                        hallucination [1] that generated additional pixels specifically                    in AI with a specific issue in mental illness, particularly
                                        for this purpose.                                                                  schizophrenia, thereby possibly undermining many efforts to
                                           Despite this (more positive) beginning, recent research has                     reduce stigma in psychiatry and mental health.
                                        started to employ the term "hallucination" to describe a                              Given AI’s increasing presence across various domains,
                                        specific type of error in image captioning [4] and adversarial                     including the medical field, concerns have arisen regarding
                                        attack in object detection [5]. In this context, "hallucination"                   the multifaceted, possibly inappropriate and potentially even
                                        refers to instances where non-existent objects are erroneously                     harmful use of the term "hallucination" [11], [12]. To address
                                        detected or incorrectly localized at their anticipated positions.                  this issue effectively, two potential paths of work offer some
                                        This latter (more negative) interpretation of "hallucination" in                   promise: 1) The establishment of a consistent and univer-
                                        computer vision mirrors its analogous usage in language mod-                       sally applicable terminologies that can be uniformly adopted
                                        els. For instance, in 2017, researchers highlighted challenges                     across all AI-impacted domains will help, particularly if such
                                        in language models, such as "the output of the Neural Machine                      terminologies lead to the use of more specific and nuanced
                                        Translation (NMT) system is often quite fluent but entirely                        terms that actually describe the issues they highlight (as we
                                        unrelated to the input" [6], or "language models presume                           will show later, such vocabulary does exist, but needs more
                                        likelihood, but the generated content is ultimately incorrect                      consistent use) and 2) The formulation of a robust and formal
                                        and unsupported by any information" [7], which is interpreted                      definition of "AI hallucination" within the context of AI.
                                        as a form of hallucination in AI.                                                  These measures are essential to promote clarity and coherence
                                                                                                                           in discussions and research related to "hallucination" in AI,
                                           1 © 20xx IEEE. Personal use of this material is permitted. Permission from      and to mitigate potential confusion and ambiguity in cross-
                                        IEEE must be obtained for all other uses, in any current or future media,          disciplinary applications.
                                        including reprinting/republishing this material for advertising or promotional
                                        purposes, creating new collective works, for resale or redistribution to servers      Motivated by these issues, in this paper, we conduct a
                                        or lists, or reuse of any copyrighted component of this work in other works.       systematic review of the use of "AI hallucination" across 14
databases with a focus on identifying various definitions that     "AI+hallucination" resulted in a total of 1445 records across all
have been used in the literature so far (our review covers more    fields over the same 10-year period. To manage this extensive
fields than just healthcare and computer science, including        dataset, we refined our search criteria to focus on the Title,
ethical and legal settings, and domains as diverse as physics,     Abstract, or Introduction, which reduced the results to 483
sports, etc. in order to explore any broader issues). Recently,    relevant records. A detailed review of each abstract led us to
two papers ( [10], [13]) explored the concept of hallucina-        download papers that appeared pertinent to AI hallucination.
tion in Natural Language Generation- (NLG-) specific tasks         This approach significantly differed from searching within
(e.g., text translation, text summarization, knowledge graph,      abstracts alone, which produced only 49 records and missed
etc.). Our work builds on these studies to also consider the       some relevant documents.
application of NLG in diverse domains. The pervasive nature           In PubMed Central (PMC), the query "AI+hallucination"
of AI extends beyond these specific tasks, affecting numerous      yielded just 1 paper. Consequently, we conducted an advanced
domains and applications. Consequently, our broader review         search using the keywords "Artificial Intelligence" AND "Hal-
done here reveals the extensive utilization of Large Language      lucination" within the "Text Word" category, uncovering 371
Models (LLMs) across almost a much broader space of                records from the past decade. PMC does not provide abstracts,
domains to date, and provides a comprehensive understanding        necessitating the manual examination of each paper to assess
of how the term has been leveraged across various fields.          its relevance to AI hallucination.
Generally we see that research attempting to define "AI               In both the MEDLINE and Web of Science databases,
hallucination" does so based on their individual understanding     we employed the term "AI hallucination" within the "All
and the challenges encountered within their respective fields.     field" category, yielding 157 and 139 records, respectively,
The findings from our systematic and broad review underscore       spanning the last 10 years. In the case of MEDLINE, each
the challenge that the term "AI hallucination" lacks a precise,    record underwent a thorough review, and records containing
universally accepted definition, resulting in the observation      definitions for AI hallucination were downloaded. Conversely,
of various characteristics associated with this term across        Web of Science offered abstracts for the records, enabling us
different applications. We present a summary of these different    to screen them individually and select those relevant to AI
interpretations and provide some guidance going forward.           hallucination.
                                                                      Within the BioMed Central (BMC) database, a search for
                     II. M ETHODOLOGY                              "AI hallucination" led to the retrieval of 76 papers published
   Our systematic review covered an extensive database search      within the last 10 years. Subsequently, we accessed each
across various domains, including computer science and             paper individually and downloaded those containing pertinent
health, with a focus on the following databases: PubMed,           definitions of AI hallucination. In the Embase database, our
MEDLINE, Scopus, PubMed Central, Web of Science,                   search for "AI hallucination" produced 80 records published
BioMed Central, Embase, PLOS, CINAHL, ACM, IEEEX-                  within the last 10 years. These records underwent meticulous
plore, ScienceDirect, Google Scholar, and arXiv (no relevant       abstract review, and those relevant to AI hallucination were
documents were found in MedlinePlus, Cochrane Library, and         selectively downloaded for further analysis.
UpToDate databases, so those were excluded).                          In the PLOS database, our initial search with "AI hallu-
   Our search methodology was tailored to adapt to the volume      cination" resulted in a substantial 1064 records across all
of results as well as the relevance of the papers to our           fields for the past 10 years. Given this large dataset, we
research objectives. We manually reviewed every paper that         refined our search to focus on the "Body" section, yielding
made it through this process in order to identify possible defi-   885 records. We proceeded to review the abstracts of each
nitions/usage of the term "hallucination" in AI. Given this goal   paper and downloaded those demonstrating relevance to AI
we had to adapt the search in some cases to identify papers        hallucination. Within the CINAHL database, we conducted
most closely relevant to this objective as noted further below.    an advanced search utilizing the terms "AI" or "Artificial
Also, given differences in how search queries are interpreted      Intelligence" combined with "Hallucination" within the "All
across the different databases we had to iteratively modify the    field" category, yielding 34 records published in the past 10
search term within each database as well in many cases. For        years.
clarity, we present all the specific details of this below (in        In the ACM and IEEEXplore databases, we performed
order to be transparent about how we created the subset of         advanced searches using the terms "AI" AND "hallucination"
papers from which to examine the definitions). However, the        within the "Full Text" category, resulting in 264 and 257
summary of these is provided in Table I, including details of      records, respectively, spanning the past 10 years. Each record
the study period for each database.                                underwent individual review, and those containing definitions
   In the PubMed Database, we initiated an advanced                related to AI hallucination, particularly within the field of
search employing the keywords "Artificial Intelligence" AND        LLMs, were downloaded.
"Hallucination" within the "All field" category, yielding             In the ScienceDirect database, searches for "AI halluci-
103 papers within the last 10 years. However, the query            nation" or ("AI" AND "hallucination") yielded an identical
"AI+hallucination" yielded only 3 papers. Conversely, within       number of records, specifically 769 English records, spanning
the Scopus database, searches for "AI hallucination" or            the last decade. Each record underwent meticulous examina-
                                                                                  TABLE I
                                                                             M ETHOD SUMMARY

     Source/                           Search Category                                           Query Terms                                  Num. of    Study
     Database                                                                                                                                 Papers    End Date*
     PubMed                                All Field                              "Artificial Intelligence" AND "Hallucination"                 103     09/27/2023
     MEDLINE                               All Field                                           "AI hallucination"                               157     09/28/2023
     Scopus                     Title, Abstract, or Introduction                    "AI+hallucination" OR "AI hallucination"                    483     09/27/2023
     PubMed Central                       Text Word                               "Artificial Intelligence" AND "Hallucination"                 371     09/28/2023
     Web of Science                        All Field                                           "AI hallucination"                               139     09/28/2023
     BioMed Central                        All Field                                           "AI hallucination"                               76      09/29/2023
     Embase                                All Field                                           "AI hallucination"                               80      09/29/2023
     PLOS                                    Body                                              "AI hallucination"                               885     09/29/2023
     CINAHL                                All Field                        "Hallucination" AND ("AI" OR "Artificial Intelligence")             34      09/29/2023
     ACM                                   Full Text                                       "AI" AND "hallucination"                             264     09/30/2023
     IEEEXplore                            Full Text                                       "AI" AND "hallucination"                             257     09/30/2023
     ScienceDirect                         All Field                           "AI hallucination" OR ("AI" AND "hallucination")                 769     09/30/2023
     Google Scholar                        All Field                              "AI hallucination" AND "hallucination in AI"                  89      10/01/2023
     arXiv                                 All Field                                       "AI" AND "hallucination"                             40      10/01/2023
 *
     The start date is the same for all databases: 01/01/2013 (Date format: mm/dd/yyyy).



tion, and we selectively downloaded papers containing defined                              AI hallucination.
concepts of AI hallucination within the LLMs domain.                                          Our exclusion criterion was limited to non-English records.
   Searching for "AI hallucination" on Google Scholar yielded                              The precise database search strategy encompassed all available
17,000 records from the last 10 years, rendering a com-                                    documents from January 1st , 2013, to October 1st , 2023. In
prehensive review unfeasible. To address this challenge, we                                total, we identified 333 records that provide a definition either
employed Google Scholar’s advanced search feature, identi-                                 independently or by inference from a referenced paper. The
fying records containing the exact phrases "AI hallucination"                              summary of the methodology is provided in Table I, including
and "hallucination in AI," which reduced the results to 89                                 details of the study period for each database. While our review
records from the last decade. Subsequently, we conducted                                   of this work is one of the broadest to date, we acknowledge
a meticulous screening of these records to identify those                                  limitations that are implicit in the methodology above - par-
providing definitions for AI hallucination.                                                ticularly ones where we had to reduce the retrieved number
   Similarly, within the arXiv database, we conducted an ad-                               of papers to focus on potentially more relevant ones due to
vanced search using the keywords "AI" AND "hallucination"                                  the manual nature of our review (i.e. where we individually
within the "All field" category, resulting in the retrieval of                             reviewed each paper to identify how the term was used and
40 relevant papers. As with our prior search, we meticulously                              extract the relevant definition in the proper context). Therefore,
examined each paper and downloaded those containing defi-                                  while the definitions presented here are certainly those that
nitions for AI hallucination.                                                              were used it is possible we may have missed a few other
   The eligibility criteria encompassed any type of published                              definitions that may have newer connotations not identified
scientific research or preprints, such as articles, reviews,                               in our work here. All the 333 definitions are provided in the
communications, editorials, and opinions, that contained the                               Appendix.
following search terms: "AI hallucination," "AI" AND "hal-
lucination," "Hallucination in AI," or ("AI" OR "Artificial In-                                                                III. R ESULT
telligence") AND "hallucination" in any part of the document,                                 We reviewed all retrieved papers and documented the
including the title, abstract, and full text. As explained for each                        definitions provided in each. One main takeaway was that
database, we employed the most appropriate search terms.                                   a formal and consistent definition of hallucination simply
   Initially, our search yielded 3753 records, in total, match-                            does not currently exist. There is also little agreement on
ing these criteria. However, we refined our search to focus                                the specific characteristics of AI hallucination. Depending on
exclusively on records that offered a definition of "AI hallu-                             the application, we observe varying characteristics, sometimes
cination" within the context of LLMs. It is essential to clarify                           even contradictory ones.
that we excluded other types of hallucination, such as face                                   For instance, in the context of text translation, Koehn and
hallucination, auditory voice/verbal hallucination, etc., as they                          Knowles [6] described hallucination as "fluent but irrelevant,"
were not the primary focus of this review. Our search involved                             or Miao et al. [26] characterized it as "fluent but inadequate,"
thorough examination of entire documents, and we collected                                 while Lee et al. [27] attributed "abnormal and unrelated" char-
any documents that indicated the presence of a definition for                              acteristics to it, thus illustrating different attributes within the
                                                                          TABLE II
                                                                   A LTERNATIVE TERMS USED

 Alternative Terms               Definitions                                                                                                                   References
                                 AI generated responses that sound plausible but are, in fact, incorrect.                                                      [14]
 Confabulation                   Definition was not provided.                                                                                                  [15]
 Delusion                        AI generated responses that are false.                                                                                        [16]
                                 The repetition of training data or its patterns, rather than actual understanding or reasoning.                               [17]
 Stochastic Parroting            LLM model generates confident, specific, and fluent answers that are factually completely wrong.                              [18]
                                 Definition was not provided.                                                                                                  [19]
 Factual Errors                  Inaccuracies in information or statements that are not in accordance with reality or the truth, often unintentional but       [20]
                                 resulting in incorrect or misleading information.
 Fact Fabrication                The occurrence where inaccurate information is invented, not represented in the training dataset, and is presented lucidly.   [21]
                                 The phenomenon where, as a generative AI, ChatGPT generates outputs based on statistical prediction of the text without       [22]
 Fabrication                     human-like reasoning, potentially resulting in plausible-sounding but inaccurate responses.
                                 The phenomenon in ChatGPT output where the text is cogent but not necessarily true.                                           [23]
                                 Definition was not provided.                                                                                                  [24]
 Falsification and Fabrication   Definition was not provided.                                                                                                  [12]
 Mistakes, Blunders, False-      Answers that are fabricated when data are insufficient for an accurate response.                                              [25]
 hoods
 Hasty Generalizations, False    AI models making inferences that do not follow from the premises; also “hasty generalizations,” i.e., the fallacy of          [11]
 Analogy, False Dilemma          making (too) strong claims based on (too) limited data.




same context. In the text summarization context, hallucination                        stochastic parroting appears to be an appropriate descriptive
refers to generated content that is inconsistent with the source                      term for the reasons behind fact fabrication in Generative AI.
document [28], [29], with some studies categorizing it into                           While we need clarity in terms of distinguishing between
subtypes: "Intrinsic hallucination" and "Extrinsic hallucina-                         stated facts (in the training data) and inferences, the reference
tion" [30], [31], raising concerns, particularly regarding the                        to hasty generalization does start to capture such a distinction.
latter.                                                                                  Finally, since our focus here was on reviewing AI halluci-
   Before the launch of ChatGPT on November 30, 2022,                                 nations across various applications, we grouped all the final
we hardly observed definitions for AI hallucination in fields                         papers examined by category, extracted definitions related to
other than computer science. However, with the advent of                              AI hallucination, and used ChatGPT 3.5 [32] to extract key
ChatGPT, researchers have recognized the urgent need for                              points. The applications included chatbots, dialogue settings,
Large Language Models (LLMs) in various fields, including                             generative AI, academia, health, legal and ethical settings,
medicine. Therefore, over time, we have observed that the                             science, technology, text translation, question and answering,
definition has changed and seems to have become a problem                             text summarization, and others. As shown in Table III, the
more relevant to ChatGPT, albeit with different characteristics                       extracted summaries share similar characteristics, but highlight
under the same term across various applications.                                      different extents of inaccuracy, ranging from "deviating from
   In recent times, for reasons discussed earlier in this paper                       established knowledge", "factual incorrectness", "fictional" to
as well as broader concern about giving AI "human" char-                              "nonsensical" – offering further considerations for a robust
acteristics inadvertently by using this term, researchers have                        taxonomy that will be needed to bring out such nuances.
made efforts to replace the term ’hallucination,’ deeming it
unsuitable and advocating for its renaming or for alternatives.                                                        IV. D ISCUSSION
We have compiled many of the suggested terms found in                                    “Hallucinate” secured its position as the word of 2023 (
the literature in Table II, along with their definitions in the                       [38], [39]) and Dictionary.com noted a 46% surge in searches
respective papers. This is a start in the right direction perhaps -                   for the term over the past year. The popular press has also
in the search for specific definitions and specific characteristics                   keyed in on this (Table IV highlights topics some recent
that we want to model - but does illustrates the lack of                              articles discuss and the meaning of "AI hallucination" they
consistency in the literature that we pointed out in this paper.                      convey). These articles primarily feature interviews with CEOs
   Based on the alternate terms we found, some "old" problems                         of big tech companies, who discuss future efforts to prevent
appear to re-surface: the terms confabulation and delusional                          "hallucination" in chatbots’ outputs. Indeed, preventing such
for instance have connections to mental health conditions                             occurrences continues to be a key research goal, but few
as well. However, fabrication, stochastic parroting and hasty                         solutions have emerged so far. In the meanwhile, the Gen-
generalization together suggest three viable alternatives. Fact                       erative AI continues to expand its applications into multiple
fabrication captures many of the cases previously attributed                          domains, making the need for good solutions vital. As a
to ’hallucination’ without the negative connotations, while                           precursor to even developing solutions, this paper calls for
                                                                 TABLE III
     K EY POINTS OF "H ALLUCINATION " DEFINITIONS WITHIN EACH APPLICATION . T HE CHARACTERISTICS OF DEFINITIONS ARE PRESENTED IN B OLD ,
                                       ALTHOUGH THEY MAY BE SIMILAR ACROSS DIFFERENT APPLICATIONS .


    Application               Number          LLM Generated Key Points of Definitions
                                of
                              Papers
    Chatbot                      34           The definitions collectively highlight the central theme of AI-generated content deviating from factual correctness, at times even
                                              leading to entirely fictional or erroneous information. In essence, AI hallucination underscores the ongoing challenge of maintaining
                                              accuracy and reliability in AI-generated content within the context of chatbot applications.
    Dialogue Set-                8            The definitions collectively underscore the challenge of ensuring accuracy and reliability in dialogue systems, given the potential pitfalls
    ting                                      associated with generating content that is unsupported, nonsensical, or factually incorrect. These issues are particularly pertinent
                                              when deploying large pre-trained language models in dialogue applications, as they struggle with maintaining fidelity to the source
                                              material while generating coherent and accurate responses.
    Generative AI                50           The definitions collectively emphasize the complexity of ensuring factual accuracy and reliability in AI-generated content within
                                              generative AI applications, highlighting the potential pitfalls of deviating from adherence to factual correctness.
    Academia                     88           A common thread among these definitions is the generation of text or content by AI models that lacks fidelity to factual accuracy,
                                              reality, or the intended context.
    Health                       82           The key idea common to all the definitions is that "AI hallucination" occurs when AI systems generate information that deviates from
                                              factual accuracy, context, or established knowledge. In essence, AI hallucination manifests as the production of text that, though
                                              potentially plausible, deviates from established facts or knowledge in health applications.
    Legal         and            16           The definitions collectively emphasize the multifaceted challenges posed by AI hallucination in the legal and ethical context. They
    Ethical                                   highlight issues of accuracy, confidence, relevance, context, and potential misinformation, underscoring the critical importance of
    Setting                                   addressing these challenges to ensure the responsible and ethical use of AI systems.
    Science                      10           Across the definitions, the central theme is that AI hallucination involves the generation of text or information that deviates from factual
                                              accuracy, coherence, or faithfulness to the input or source content, with potential consequences for scientific accuracy and integrity.
    Technology                   8            The definitions reflect the multifaceted nature of AI hallucination in technology applications, encompassing accuracy, unpredictability,
                                              credibility, and the balance between reasonableness and correctness.
    Text Transla-                4            The definitions collectively emphasize the central theme of "AI hallucination" in text translation, which revolves around challenges
    tion                                      related to maintaining fidelity, coherence, and relevance in the generated translations to ensure accurate and meaningful output.
    Question and                 7            "AI hallucination" in question and answer applications raises concerns related to the accuracy, truthfulness, and potential spread of
    Answering                                 misinformation in AI-generated answers, emphasizing the need for improving the reliability of these systems.
    Text Summa-                  19           The definitions highlight the multifaceted challenges posed by "AI hallucination" in text summarization, encompassing issues related to
    rization                                  fidelity, coherence, factual accuracy, and the preservation of the original meaning in generated summaries.
              *
    Others                       7            These diverse applications collectively emphasize the challenge of maintaining accuracy, coherence, and trustworthiness in AI-
                                              generated content, highlighting the need for tailored approaches to address domain-specific concerns.
*
    Including: Investment portfolio, Journalism, Reinforcement Learning, Retail, Sport, and Survey Setting.


                                                                                 TABLE IV
                                                             S OME POPULAR PRESS ARTICLES ON AI HALLUCINATION

                  What the Press Article Discussed. . .                                            The Real Meaning the Press Article Conveys about "AI                    Source
                                                                                                   Hallucination"
    1             CNBC provided some examples where ChatGPT generated outputs that                 When an AI model “hallucinates,” it generates fabricated                CNBC [33]
                  sounded correct but weren’t actually true, such as a legal brief written         information in response to a user’s prompt, but presents it
                  by ChatGPT to a Manhattan federal judge                                          as if it’s factual and correct
    2             The New York Times asked ChatGPT, Google’s Brad, and Microsoft’s                 Chatbots provide inaccurate answers to questions; although              The New York
                  Bing: When did The New York Times first report on "artificial intelli-           false, the responses appear plausible as they blur and conflate         Times [34]
                  gence"?                                                                          people, events, and ideas
    3             The New York Times traced the evolution of the term "hallucination"              -                                                                       The New York
                  throughout the newspaper’s history                                                                                                                       Times [35]
    4             CNN addressed the major issue of "AI hallucination" and narrated on              AI-powered tools like ChatGPT impress with their ability to             CNN [36]
                  the responses of OpenAI’s and Google’s CEOs to the question: Can                 provide human-like responses, but a growing concern is their
                  hallucination be prevented?                                                      tendency to just make things up
    5             Forbes narrated the history of artificial neural networks, which started         "AI hallucination" refers to unrealistic ideas about achieving          Forbes [37]
                  around eight decades ago, when researchers sought to replicate the               "artificial general intelligence" (AGI), while understanding of
                  functioning of the brain                                                         how our brains work is limited




more systematic, consistent and semantically nuanced terms                                        options that are good alternatives. More work is needed to
that can replace "hallucinations" for the reasons noted here.                                     develop a systematic taxonomy that can be widely adopted as
As one step toward such a call, we presented a short summary                                      we discuss these issues in the context of AI applications.
from one of the broadest manual literature reviews on this topic
                                                                                                                                     A PPENDIX
to date. Our findings illustrate the current lack of consistency
                                                                                                        R EVIEW      OF THE      "AI H ALLUCINATION "                DEFINITIONS
and consensus on this issue, but also bring to light some recent
                                      TABLE V: AI hallucination definitions

Num.      Author(s)            Year   Application                              Definition                             Citation
1      Koehn       and         2017   Text Trans-       "AI hallucination" occurs when the output of the              1249
       Knowles [6]                    lation            Neural Machine Translation (NMT) system is often
                                                        quite fluent but entirely unrelated to the input.
2      Wiseman et al.          2017   Natural           "AI hallucination" occurs when a language model               556
       [7]                            Language          presumes likelihood, but the generated content is ulti-
                                      Generation        mately incorrect and unsupported by any information.
3      Lee et al. [40]         2018   Text Trans-       "AI hallucination" occurs when Neural Machine                 83
                                      lation            Translation (NMT) systems produce highly patholog-
                                                        ical translations that are completely untethered from
                                                        the source material.
4      Nie et al. [41]         2019   Large             "AI hallucination" refers to the problem where the            63
                                      Language          generated texts often contain information that is ir-
                                      Model             relevant to or contradicted by the input.
5      Tian et al. [42]        2019   Large             "AI hallucination" occurs when a language model               66
                                      Language          generates text that is fluent but unfaithful to the source.
                                      Model
6      Dušek et al. [43]       2019   Natural           "AI hallucination" occurs when a language model adds          89
                                      Language          information that is not grounded in the input.
                                      Generation
7      Ferreira   et     al.   2019   Natural           "AI hallucination" refers to a language model describ-        141
       [44]                           Language          ing non-linguistic representations that are not present
                                      Generation        in the input.
8      Martindale et al.       2019   Text Trans-       "AI hallucination" occurs when the machine transla-           29
       [45]                           lation            tion output contains more information than the refer-
                                                        ence text.
9      Dušek and Kas-          2020   Natural           "AI hallucination" occurs when expressions in the             46
       ner [46]                       Language          output do not correspond to input facts.
                                      Generation
10     Kang        and         2020   Natural           "AI hallucination" occurs when neural language mod-           72
       Hashimoto [47]                 Language          els often produce fluent text that is unfaithful to the
                                      Generation        source.
11     Parikh et al. [48]      2020   Natural           "AI hallucination" occurs when a language model               242
                                      Language          generates text that is fluent but not faithful to the
                                      Generation        source.
12     Maynez     et     al.   2020   Text Sum-         "AI hallucination" refers to content that is unfaithful       572
       [30]                           marization        to the input document.
                                                        "Intrinsic hallucinations" are consequences of synthe-
                                                        sizing content using the information present in the
                                                        input document.
                                                        "Extrinsic hallucinations" are model generations that
                                                        ignore the source material altogether. Hallucinate con-
                                                        tent that is unfaithful to the input document.
13     Durmus     et     al.   2020   Question          "AI hallucination" refers to the occurrence when AI           269
       [49]                           Answering         generates content inconsistent with the source docu-
                                                        ment, i.e., unfaithful.
14     Zhao et al. [28]        2020   Text Sum-         "AI hallucination" occurs when language models gen-           73
                                      marization        erate material that is not supported by the original text.
15     Dong et al. [29]        2020   Text Sum-         "AI hallucination" occurs when a language model               89
                                      marization        generates content that is factually inconsistent with the
                                                        source documents.
16     Filippova [8]           2020   Large             "AI hallucination" refers to the generated content            46
                                      Language          which is either unfaithful to the input or nonsensical.
                                      Model
17     Zhou et al. [50]        2020   Hallucination     "AI hallucination" occurs when the model generates            87
                                      Detection         additional content not supported by the input.
18     Elsahar    et     al.   2020   Hallucination     "AI hallucination" occurs when the fluency of natural         33
       [51]                           Mitigation        language generation models can be highly misleading,
                                                        as it often distracts from the wrong facts stated in the
                                                        generated text.
                                                                                                        Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

19     Tang et al. [52]        2021     Text Sum-       "AI hallucination" refers to content that is not sup-       32
                                        marization      ported by the source documents.
20     Cao et al. [53]         2021     Text Sum-       "AI hallucination" refers to content that is not directly   37
                                        marization      inferable from the source text.
21     Miao et al. [26]        2021     Text Trans-     "AI hallucination" refers to generating fluent but in-      21
                                        lation          adequate translations to the source sentences.
22     Chen et al. [54]        2021     Text Sum-       "Intrinsic" and "Extrinsic hallucinations", involving       62
                                        marization      the fabrication of untruthful text spans containing
                                                        information that may either be present or absent from
                                                        the source.
23     Wang et al. [55]        2021     Text Sum-       "AI hallucination" refers to where the model generates      14
                                        marization      fictional content.
24     Xiao and Wang           2021     Large           "AI hallucination" occurs where models generate de-         58
       [56]                             Language        scription tokens that are not supported by the source
                                        Model           inputs.
25     Dziri et al. [57]       2021     Knowledge       "AI hallucination" occurs when dialogue models, de-         51
                                        Graph           spite maintaining plausible general linguistic capabil-
                                                        ities, are still unable to fully discern facts and may
                                                        instead hallucinate factually invalid information.
26     Liu et al. [58]         2021     Hallucination   "AI hallucination" occurs when language models ex-          29
                                        Detection       hibit a propensity to hallucinate non-existent or incor-
                                                        rect content that is unacceptable in most user-oriented
                                                        applications.
27     Huang et al. [59]       2021     Text Sum-       "Intrinsic hallucination" is a fact that is contradicted    51
                                        marization      to the source document.
                                                        "Extrinsic hallucination" is the fact that is neutral to
                                                        the source document (i.e., the content that is neither
                                                        supported nor contradicted by the source document).
28     Lin [60]                2021     Question        "AI hallucination" refers to the tendency of LLMs to        250
                                        Answering       generate false statements.
29     Shuster    et     al.   2021     Large           "AI hallucination" refers to the occurrence where lan-      198
       [61]                             Language        guage models generate plausible-looking statements
                                        Model           that are factually incorrect.
30     Sekulić   et     al.   2021     Natural         "AI hallucination" occurs where generated responses         26
       [62]                             Language        do not correspond to the real-world.
                                        Generation
31     Ghosh et al. [63]       2021     Reinforcement   "AI hallucination" occurs when the generated text           5
                                        Learning        asserts information not present in the source.
32     Perez-Beltrachini       2021     Text Sum-       "AI hallucination" refers to neural summarization           18
       and Lapata [64]                  marization      models’ propensity to generate text that does not
                                                        preserve the meaning of the input.
33     Lyu et al. [31]         2022     Text Sum-       "AI hallucination" signifies the presence of distorting     0
                                        marization      or fabricating facts within generated summaries, re-
                                                        sulting in inconsistencies between a summary and the
                                                        corresponding original document.
                                                        "Extrinsic hallucination" entails adding information
                                                        not directly inferable from the input information.
                                                        "Intrinsic hallucination" involves manipulating the in-
                                                        formation present in the input document.
34     Ali et al. [65]         2022     Health          "AI hallucination" refers to scenarios in which a           29
                                                        Language Model (LLM) asserts inaccurate facts or
                                                        contextual data that it falsely believes to be correct
                                                        in its response.
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

35     Ando et al. [66]        2022     Health          "AI hallucination" occurs when abstractive summa-           2
                                                        rization may sometimes unintentionally generate un-
                                                        faithful descriptions.
                                                        "Intrinsic hallucination" is a phenomenon in which the
                                                        concept or term itself is in the source documents; its
                                                        synthesis misrepresents the information in the source,
                                                        and the meaning becomes inconsistent.
                                                        "Extrinsic hallucination" is content that is neither
                                                        supported nor contradicted by the source and is caused
                                                        by source documents with poor information.
36     Ando et al. [67]        2022     Health          "AI hallucination" refers to the phenomenon where the       2
                                                        abstractive approach frequently produces fake content
                                                        that does not align with the reference summary.
37     Rebuffel et al.         2022     Natural         "AI hallucination" refers to model outputs that are         43
       [68]                             Language        often subject to over-generation, where misaligned
                                        Generation      fragments from training instances, known as diver-
                                                        gences, can induce similarly misaligned outputs during
                                                        inference.
38     Wan and Bansal          2022     Text Sum-       "AI hallucination" refers to a scenario where a sum-        30
       [69]                             marization      mary contains facts or entities not present in the
                                                        original document.
39     Corbelle et al.         2022     Natural         "AI hallucination" refers to instances where neural         2
       [70]                             Language        models generate texts that are incoherent or unrelated
                                        Generation      to the input of a D2T system.
40     Lee et al. [71]         2022     Large           "AI hallucination" occurs when a model is making            30
                                        Language        factual errors, generating a named-entity that does not
                                        Model           appear in the ground-truth knowledge source.
41     Cabezudo et al.         2022     Natural         "AI hallucination" refers to text generated by pre-         0
       [72]                             Language        trained models that is irrelevant to or contradicted with
                                        Generation      the input.
42     Van et al. [73]         2022     Text Sum-       "AI hallucination" refers to the occurrence of adding       7
                                        marization      information to the output that was not present in the
                                                        original text.
43     Dziri et al. [74]       2022     Dialogue        "AI hallucination" occurs when large pre-trained lan-       51
                                                        guage models generate factually incorrect statements.
44     Dziri et al. [75]       2022     Dialogue        "AI hallucination" refers to the phenomenon where di-       24
                                                        alogue systems often produce unsupported utterances.
45     Koto et al. [76]        2022     Text Sum-       "AI hallucination" occurs where information is gener-       21
                                        marization      ated that does not exist in the source document, also
                                                        called "factual inconsistencies."
46     Goodman et al.          2022     Email Writ-     "AI hallucination" refers to factually incorrect or non-    10
       [77]                             ing             existent content generated by the LLM.
47     Gehrmann et al.         2022     Natural         "AI hallucination" refers to a situation where a model      62
       [78]                             Language        is not faithful as it adds information not present in the
                                        Generation      source document.
                                                        "Intrinsic hallucinations" misrepresent facts in the
                                                        input.
                                                        "Extrinsic hallucinations" ignore the input altogether.
48     Erdem et al. [79]       2022     Natural         "AI hallucination" refers to the generation of descrip-     20
                                        Language        tions or facts that are not fully supported by the input.
                                        Generation
49     Tun et al. [80]         2022     Dialogue        "AI hallucination" refers to large-scale pre-trained        0
                                                        language models generating text that is nonsensical
                                                        and struggling to remain true to the source content.
50     Gurrapu    et     al.   2022     Claim Veri-     "AI hallucination" refers to the phenomenon where           1
       [81]                             fication        natural language generation models introduce unin-
                                                        tended and irrelevant text during the generation pro-
                                                        cess.
51     Yang et al. [82]        2022     Text Sum-       "AI hallucination" occurs when models generate sum-          2
                                        marization      maries factually inconsistent with their original docu-
                                                        ment.
                                                                                                       Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                            Definition                            Citation

52     Ji et al. [10]         2023     Natural         "AI hallucination" refers to the generated content that      482
                                       Language        is nonsensical or unfaithful to the provided source
                                       Generation      content.
                                                       "Intrinsic hallucinations" encompasses the generated
                                                       output that contradicts the source content.
                                                       "Extrinsic hallucination" involves the generated output
                                                       that cannot be verified from the source content (i.e.,
                                                       output can neither be supported nor contradicted by
                                                       the source).
53     Narayanan et al.       2023     Natural         "AI hallucination" refers to situations in which AI          2
       [83]                            Language        models provide responses with confidence that appear
                                       Generation      faithful but are nonsensical when evaluated against
                                                       common knowledge.
54     Leiser et al. [84]     2023     Large           "Artificial hallucinations" are the distortion of per-       0
                                       Language        ception in Large Language Models (LLMs) as they
                                       Model           present incorrect information, disguising it as factual
                                                       responses.
55     Wang et al. [85]       2023     ChatGPT         "AI hallucination" denotes the tendency to generate          1
                                                       inaccurate outputs unfaithful to the training data.
56     Ma et al. [86]         2023     Software        "AI hallucination" refers to the phenomenon that can         8
                                       Engineering     result in the fabrication of elements that do not actually
                                                       exist.
57     Li [17]                2023     Large           "AI hallucination" occurs when LLMs generate text            11
                                       Language        based on their internal logic or patterns, rather than
                                       Model           the true context, leading to confidently but unjustified
                                                       and unverified deceptive responses.
58     Vaghefi    et    al.   2023     NLP     and     "AI hallucination" refers to mistakes in the generated       5
       [87]                            Climate         text that are semantically incorrect or unsupported by
                                       Change          the input text.
59     Daull et al. [88]      2023     Question        "AI hallucination" refers to confident generated re-         2
                                       Answering       sponses that contain false information not supported
                                                       by the model’s training data.
                                                       "Extrinsic hallucination" involves a model introducing
                                                       information not present in the source data.
                                                       "Intrinsic hallucination" occurs when the model dis-
                                                       torts information from the source data into a factually
                                                       incorrect representation.
60     Zhang et al. [89]      2023     Large           "AI hallucination" commonly refers to the phe-               3
                                       Language        nomenon where LLMs occasionally generate outputs
                                       Model           that, although appearing plausible, deviate from the
                                                       user input, previously generated context, or factual
                                                       knowledge.
61     Romanko et al.         2023     Investment      "AI hallucination" refers to instances where the AI,         3
       [90]                            Portfolio       although generating text based on its training, does so
                                                       without a solid conceptual framework behind it.
62     Henderson et al.       2023     AI Speech       "AI hallucination" refers to the act of generating text      2
       [91]                                            that includes factual claims that are untrue, and in
                                                       some cases, these claims may not have been present
                                                       in its training data.
63     Ni et al. [92]         2023     NLP     and     "AI hallucination" refers to the generation of answers       0
                                       Climate         in which some or all of the covered information is not
                                       Change          adequately supported by the report, particularly when
                                                       there is extrapolation or partial support involved, and
                                                       it may also relate to instances where the model fails to
                                                       honestly report its references while generating content
                                                       that lacks fidelity to the source.
64     Mahmood et al.         2023     Radiology       "AI hallucination" involves the generation of false          1
       [93]                            Report          findings in the produced reports by LLMs.
65     Goyal et al. [94]      2023     Question        "Hallucinations" signify the production of convincing         0
                                       Answering       yet incorrect text outputs by LLMs, with the potential
                                                       to distort scientific facts and disseminate misinforma-
                                                       tion.
                                                                                                       Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

66     Zhang [95]              2023     Robotic         "AI hallucination" refers to unpredictable outputs gen-     0
                                                        erated by LLMs.
67     Li et al. [96]          2023     Large           "AI hallucination" is the phenomenon where responses        1
                                        Language        are confidently generated but are incorrect.
                                        Model
68     Li et al. [97]          2023     Large           "AI hallucination" occurs when AI systems generate          1
                                        Language        outputs that are not aligned with the input context and
                                        Model           encounter challenges in efficiently capturing complex
                                                        dependencies.
69     Curran et al. [98]      2023     Legal           "AI hallucination" refers to tendency for generating        1
                                        Setting         false information.
70     Feldman et al.          2023     Large           "AI hallucination" in the context of LLMs refers to the     2
       [99]                             Language        phenomenon where these language models generate
                                        Model           non-factual statements, which could have a detrimental
                                                        effect on the trustworthiness of their outputs.
71     Mukherjee et al.        2023     Generative      "Hallucinations," characterized by AI responses con-        0
       [100]                            AI              taining random inaccuracies or falsehoods, emerge
                                                        when models prioritize novelty over usefulness.
72     Salvagno et al.         2023     Large           "AI hallucination" refers to the creation of seemingly      4
       [101]                            Language        accurate bibliographic references with recognized au-
                                        Model           thors and coherent titles, even though these references
                                                        are entirely fictitious and have no real existence.
73     Beutel    et      al.   2023     Large           "Hallucinations" are defined as the production of con-      20
       [102]                            Language        tent that does not accurately represent the provided
                                        Model           source and may appear nonsensical due to errors in
                                                        the encoding and decoding processes between text and
                                                        representations.
74     Azamfirei et al.        2023     Text Sum-       "AI hallucination" refers to receiving a response from      32
       [103]                            marization      the model when it lacks an appropriate answer, which
                                                        appears to be the most likely summary of the study,
                                                        despite its potential inaccuracy.
75     Meyer     et      al.   2023     Academia        "AI hallucination" can be defined as the capability of      8
       [104]                                            LLM-based chatbots to convey false information as
                                                        though it were true.
76     Hernigou      and       2023     Health          "AI hallucination" is a confident response that does        2
       Scarlat [105]                                    not seem in concordance with its training data.
77     Patil [106]             2023     Legal           "AI hallucination" is defined as a confident response       0
                                        Setting         by an AI system that lacks justification in its training
                                                        data. These responses can appear factual but are not
                                                        true, often simply being answers "made up" by the AI.
78     Alexander et al.        2023     Academia        "AI hallucination" refers to its tendency to make up        1
       [107]                                            facts and references that do not exist.
79     Lyell [108]             2023     Academia        "AI hallucination" describes the propensity of the          0
                                                        system to convincingly fabricate information.
80     Grassini [109]          2023     Academia        "AI hallucination" involves the generation of incorrect     14
                                                        or even fabricated information.
81     Brodeur    et     al.   2023     Legal           "AI hallucination" refers to the situation where an AI      0
       [110]                            Setting         system, due to its inability to correctly interpret data,
                                                        generates inaccurate or unusual outputs.
82     Lee and         Choi    2023     Health          "AI hallucination" occurs when AI generates informa-        0
       [111]                                            tion about things that are untrue, presenting them as
                                                        if they are true, which can diminish the reliability of
                                                        the generated information.
83     Chatfield [112]         2023     Chatbot         "AI hallucination" represents a response that may or        0
                                                        may not be plausible but is not rooted in reality.
84     McGowan et al.          2023     References      "AI hallucination" refers to the occurrence of mis-          4
       [113]                                            takes in the generated text that, while semantically
                                                        or syntactically plausible, are ultimately incorrect or
                                                        nonsensical upon closer examination.
                                                                                                       Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                            Definition                           Citation

85     Wu and      Dang       2023     Academia        "AI hallucination" refers to a scenario in which a          6
       [114]                                           machine generates seemingly realistic outputs without
                                                       any real-world input.
86     Wang et al. [115]      2023     Academia        "AI hallucination" refers to the generation of content      2
                                                       that is nonsensical or untrue in relation to certain
                                                       sources.
87     Hou and Ji [116]       2023     Health          "AI hallucination" refers to the situation where GPT        3
                                                       models provide confident answers that contradict the
                                                       truth.
88     Au et al. [117]        2023     Health          "AI hallucination" refers to the phenomenon in which        15
                                                       LLMs produce output that is nonsensical or unfaithful
                                                       to the provided input or "prompt," and this problem
                                                       is further amplified when the prompt contains insuf-
                                                       ficient or masked information, despite the high confi-
                                                       dence displayed in the generated output, as observed
                                                       in ChatGPT.
89     Hua et al. [118]       2023     Health/         "AI hallucination" is AI-generated outputs that deviate     1
                                       Academia        from its training data. These outputs may appear
                                                       syntactically or semantically plausible, but in reality,
                                                       they are incorrect or nonsensical.
90     Brameier et al.        2023     Health/         "AI hallucination" is the production of confident re-       1
       [119]                           Academia        sponses by an NLP tool that are nonsensical or that
                                                       seem realistic but are not based on any real-world data.
91     Lee et al. [120]       2023     Health          "AI hallucination" refers to the occurrence where GPT-      260
                                                       4 produces false responses, which are often presented
                                                       in a convincing manner, potentially leading the in-
                                                       quirer to believe their accuracy.
92     Long et al. [121]      2023     Health/         "AI hallucination" refers to the phenomenon where           0
                                       Academia        Language Models (LMs), including ChatGPT, produce
                                                       outputs characterized by blatant factual errors, signifi-
                                                       cant omissions, and erroneous information generation.
93     Zhang [122]            2023     Large           "AI hallucination" refers to the generation of responses    1
                                       Language        by ChatGPT that frequently contain incorrect informa-
                                       Model           tion, and it can even extend to the generation of fake
                                                       scientific abstracts and research papers.
94     Puchert   et   al.     2023     Health          "AI hallucination" refers to a common phenomenon            1
       [123]                                           where the model includes incorrect or false infor-
                                                       mation in its responses, despite providing eloquent
                                                       answers.
95     Wang et al. [124]      2023     Health          "AI hallucination" refers to misinformation, unreason-      0
                                                       able or illogical in common-sense knowledge
96     Garg et al. [125]      2023     Health          "AI hallucination" refers to the production of content      2
                                                       that sounds authoritative but can be inaccurate, incom-
                                                       plete, or biased in nature.
97     Han et al. [126]       2023     Health          "AI hallucination" occurs when AI confidently gener-        0
                                                       ates an impressive-sounding response that may not be
                                                       justified by its training data or may even be factually
                                                       incorrect.
98     Dolan and Freer        2023     Academia        "AI hallucination" refers to the phenomenon where           0
       [127]                                           AI systems may create or invent text and citations that
                                                       sound realistic but are not based on actual information,
                                                       and in certain instances, they may even fabricate
                                                       nonexistent works.
99     Scott-Branch      et   2023     Academia        "AI hallucination" refers to mistakes in the generated      0
       al. [128]                                       text that are semantically or syntactically plausible but
                                                       are in fact incorrect or nonsensical.
100    Larsen-Ledet           2023     Academia        "AI hallucination" refers to the generation of output,      0
       [129]                                           often text, containing falsities, which undermine the
                                                       intended factual nature of the service, intended to be
                                                       reliable and trustworthy.
                                                                                                     Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                           Citation

101    Kim et al. [130]      2023     Health/         "AI hallucination" refers to the phenomenon in which        9
                                      Academia        there is no true internal understanding of the language,
                                                      and LLMs generate text that sounds plausible based
                                                      on the training data they have been exposed to.
102    Tay et al. [131]      2023     Health          "AI hallucination" refers to the situation where GPT        1
                                                      confidently generates responses that lack any justifica-
                                                      tion from training data or correspondence to real-world
                                                      input.
103    Cai et al. [132]      2023     Health          "AI hallucination" refers to the issue of accuracy and      9
                                                      reliability in LLMs, particularly concerning the high
                                                      frequency of false information.
104    Jahic et al. [133]    2023     Academia        "AI hallucination" refers to the phenomenon where           2
                                                      an AI system generates unexpected or meaningless
                                                      outputs that appear coherent and plausible to human
                                                      observers.
105    Randell     and       2023     Academia        "AI hallucination" refers to the plausible yet erroneous    0
       Coghlan [134]                                  text generated by artificial intelligence systems, re-
                                                      sembling compression artifacts, which can only be
                                                      discerned through comparison with original sources
                                                      like the web or our existing knowledge.
106    Brender [135]         2023     Academia        "AI hallucination" refers to the unfaithful or nonsen-      1
                                                      sical text occasionally generated by large language
                                                      models.
107    Pedersen [136]        2023     Academia        "AI hallucination" refers to mistakes in the generated      0
                                                      text that are semantically or syntactically plausible but
                                                      are in fact incorrect or nonsensical.
108    Munoz      et   al.   2023     Academia        “AI hallucination” which results when the system            1
       [137]                                          provides a response that is not factual.
109    Hatem      et   al.   2023     Health          "AI hallucination" is inaccurate and stigmatizing to        0
       [138]                                          both AI systems and individuals who experience hallu-
                                                      cinations. Because of this, they suggest the alternative
                                                      term "AI misinformation" as they feel this is an
                                                      appropriate term to describe the phenomenon at hand
                                                      without attributing lifelike characteristics to AI.
110    Athaluri   et   al.   2023     Health/         "AI hallucination" is a phenomenon where AI gener-          16
       [139]                          Academia        ates a convincing but completely made-up answer.
111    Gorichanaz            2023     Academia        "AI hallucination" refers to generating misinformation,     1
       [140]                                          including making false statements, citing sources that
                                                      do not exist and creating code that doesn’t work.
112    Cox et al. [141]      2023     Health          "AI hallucination" occurs when Language Models              0
                                                      (LLMs) misunderstand medical vocabulary or provide
                                                      advice that does not align with established medical
                                                      guidelines.
113    Yadava [142]          2023     Health/         "AI hallucination" is interestingly labeled as a phe-       11
                                      Academia        nomenon where responses from ChatGPT are some-
                                                      times ambiguous, nonsensical, and undesirable.
114    Dai et al. [143]      2023     Academia        "AI hallucination" is a phenomenon in which it some-        11
                                                      times produces confident but irrelevant or inaccurate
                                                      responses.
115    Lingard [144]         2023     Academia        "AI hallucination" refers to the phenomenon where           1
                                                      AI generates content that confidently presents as
                                                      legitimate-sounding material but is not real.
116    Woodland [145]        2023     Health/         "AI hallucination" refers to a situation where the          0
                                      Academia        generated text is not derived from a factual source
                                                      but instead arises from statistical predictions of words
                                                      that are likely to follow the given input.
117    Kashangura            2023     Health          "AI hallucination" is the generation or production of a     0
       [146]                                          factually invalid statement, characterized by confident
                                                      output from AI that is not correct or not real.
118    Walker     et   al.   2023     Health          “AI hallucinations” known as wrong or out of context      6
       [147]                                          answers generated by ChatGPT AI.
                                                                                                  Continued on next page
                                   TABLE V – continued from previous page
Num.      Author(s)         Year     Application                           Definition                           Citation

119    Tenge   Hansen       2023     Academia        “AI hallucination” is defined as a phenomenon where        0
       and Røsand Valø                               AI generates a convincing but completely made-up
       [148]                                         answer, often incorporating fake references for added
                                                     persuasiveness.
                                                     “Intrinsic hallucination” refers to the LLM generation
                                                     that contradicts the source/input.
                                                     “Extrinsic hallucination” refers to the LLM genera-
                                                     tions that cannot be verified from the source/input
                                                     content, i.e., output that can neither be supported nor
                                                     contradicted by the source.
120    Tupper    et   al.   2023     Academia        "AI hallucination" refers to a phenomenon wherein          0
       [149]                                         incorrect or nonsensical responses are generated in
                                                     response to prompts.
121    Borkowski et al.     2023     Health          "AI Hallucination" refers to the phenomenon often          0
       [150]                                         observed in language models like ChatGPT, charac-
                                                     terized by the generation of inaccurate information,
                                                     which result from the model’s inability to differentiate
                                                     between real and fake information sources.
122    Subramanya and       2023     Legal           "AI hallucination" occurs when AI generates content        0
       Furlong [151]                 Setting         that appears accurate but includes false references.
123    Tenzer [152]         2023     Legal           "AI hallucination" occurs when a chatbot produces          0
                                     Setting         a confident but inaccurate response to a question,
                                                     contributing to the erratic and unreliable behavior of
                                                     A.I. large language models (L.L.M.s), which may in-
                                                     clude providing false information and acting strangely
                                                     toward users.
124    Alarie and Mc-       2023     Legal           "AI hallucination" occurs when AI generates untrue         0
       Creight [153]                 Setting         information that is not backed up by real-world data.
125    Han et al. [154]     2023     Academia        "AI hallucination" refers to the situation where the       0
                                                     AI, while not presenting fictitious information like
                                                     a hallucination, draws from sources other than the
                                                     requested one and fails to indicate this, potentially
                                                     leading to incorrect conclusions when users rely on the
                                                     AI-generated text for understanding a specific paper.
126    Picht [155]          2023     Legal           "AI Hallucination" refers to the phenomenon in which       0
                                     Setting         ChatGPT generates responses that sound confident and
                                                     compelling, despite its inability to genuinely answer
                                                     the posed question.
127    Treleaven et al.     2023     Legal           "AI hallucination" is defined as a confident response      0
       [156]                         Setting         that is biased, too specialized, or even entirely incor-
                                                     rect, with the model fabricating a seemingly plausible
                                                     but factually inaccurate answer.
128    Ariyaratne [157]     2023     Legal           "AI Hallucination" refers to the term used to describe     0
                                     Setting         the phenomena of generative algorithms making up
                                                     facts.
129    Wu et al. [158]      2023     Large           "AI hallucination" occurs when the replies generated       40
                                     Language        by ChatGPT frequently contain factual errors.
                                     Model
130    Wang et al. [159]    2023     Large           "AI hallucination" occurs when ChatGPT produces re-        7
                                     Language        sponses that, despite sounding plausible, are ultimately
                                     Model           incorrect or nonsensical.
131    Amaro     et   al.   2023     Large           "AI hallucination" occurs when ChatGPT generates           0
       [160]                         Language        outputs that invent facts and concepts, as it lacks
                                     Model           objective training to assess the factual correctness of
                                                     its responses.
132    Skrodelis et al.     2023     Natural         "AI hallucination" occurs when NLGs frequently pro-        0
       [161]                         Language        duce text that is either nonsensical or not true to the
                                     Generation      original input.
133    Gupta et al. [162]   2023     Cybersecurity   "AI hallucination" refers to a phenomenon in which       7
                                                     the AI model generates inaccurate or outright false
                                                     information.
                                                                                                Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                            Citation

134    Bahrini    et   al.   2023     Chatbot         "AI hallucination" refers to ChatGPT’s limitations and       28
       [163]                                          challenges, which encompass bias and the occasional
                                                      generation of nonsensical output
135    Hamid [164]           2023     Chatbot         "AI hallucination" refers to the ’hallucinations,’ or        1
                                                      incoherent responses, observed in Language Models
                                                      (LLMs), which are essentially errors in the model’s
                                                      output as it is designed to prioritize coherence rather
                                                      than truth.
136    De Silva et al.       2023     Chatbot         "AI hallucination" occurs when ChatGPT exhibits              3
       [165]                                          anomalous behavior, producing factual inaccuracies,
                                                      logical fallacies, bias, and plagiarism in its responses,
                                                      a phenomenon popularly referred to as ’AI hallucina-
                                                      tions’ or ’stochastic parroting.’
137    Atallah [166]         2023     Health          "AI hallucination" occurs when modern LLMs pro-              1
                                                      duce fluent and grammatically correct textual outputs
                                                      that are categorically false, and in some instances,
                                                      fictitious.
138    Byrne [167]           2023     Large           "AI hallucination" refers to outputs that reflect misin-     0
                                      Language        terpretations and falsehoods.
                                      Model
139    Thirunavukarasu       2023     Health          "AI hallucination" refers to the occurrence where            30
       et al. [21]                                    inaccurate information is invented, not represented in
                                                      the training dataset, and is presented lucidly, with an
                                                      alternative term like ’fact fabrication’ being preferred.
140    Ting et al. [22]      2023     Health          AI Hallucination refers to the phenomenon where, as          5
                                                      a generative AI, ChatGPT generates outputs based on
                                                      statistical prediction of the text without human-like
                                                      reasoning, potentially resulting in plausible-sounding
                                                      but inaccurate responses, also known as "fabrication."
141    Beam et al. [168]     2023     Health          "AI hallucination" occurs when a model generates in-         0
                                                      formation that isn’t present in the input data, leading to
                                                      outputs that seem plausible but are factually incorrect
                                                      or nonsensical.
142    Madden     et   al.   2023     Health          "AI hallucination" occurs when ChatGPT generates             2
       [16]                                           false information, also called "delusions."
143    Komorowski            2023     Health          "AI hallucination" refers to the phenomenon where            4
       [169]                                          ChatGPT has the ability to confidently produce an-
                                                      swers that appear believable but may be incorrect or
                                                      nonsensical.
144    Sallam [170]          2023     Health/         "AI hallucination" refers to concerns arising from           334
                                      Academia        possible bias in ChatGPT’s training datasets, limiting
                                                      its capabilities and potentially causing factual inaccu-
                                                      racies that, surprisingly, seem scientifically plausible.
145    Rothschild [171]      2023     Health/         "AI hallucination" refers to false responses provided        0
                                      Academia        by ChatGPT3.
146    Im [172]              2023     Health          "AI hallucination" occurs when ChatGPT generates             0
                                                      responses that are unfaithful to the provided training
                                                      data.
147    Hulman     et   al.   2023     Health          "AI hallucination" refers to the occurrence of making        5
       [173]                                          up completely false information supported by fictitious
                                                      citations.
148    Cheung     et   al.   2023     Health          "AI hallucination" occurs as a phenomenon where              0
       [174]                                          the AI generates responses that are nonsensical or
                                                      unfaithful to the provided source input.
149    Moskatel    and       2023     Academia        "AI hallucination" is an inaccurate response by an AI        0
       Zhang [175]                                    that is not justified by its training data.
150    Javid et al. [176]    2023     Health/         "AI hallucination" refers to the phenomenon where the       0
                                      Academia        model produces text that is either factually incorrect
                                                      or nonsensical.
                                                                                                    Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

151    Buholayka et al.        2023     Health/         "AI hallucination" occurs when AI-generated content         4
       [177]                            Academia        is nonsensical or unfaithful to the provided source
                                                        content.
152    Alkaissi and Mc-        2023     Health/         "AI hallucination" occurs when ChatGPT provides             230
       Farlane [178]                    Academia        confident responses that appear faithful but are non-
                                                        sensical when evaluated against common knowledge.
153    Eysenbach [179]         2023     Health/         "AI hallucination" is a confident response by an ar-        128
                                        Academia        tificial intelligence system that does not seem to be
                                                        justified by its training data.
154    Østergaard and          2023     Health          "Non sequitur", Latin for “it does not follow,” is a term   0
       Nielbo [11]                                      commonly used in philosophy and rhetoric to describe
                                                        inferences not following from the premises.
                                                        They acknowledged that “non sequitur” does not cover
                                                        all false responses generated by AI models. Indeed, AI
                                                        models can also make “hasty generalizations,” i.e., the
                                                        fallacy of making (too) strong claims based on (too)
                                                        limited data.
                                                        They realized that many other terms from philosophy
                                                        and rhetoric describing logical fallacies of different
                                                        types (e.g., “false analogy,” “appeal to authority” and
                                                        “false dilemma”) could also be used to label false
                                                        responses from AI models.
155    Thorne [180]            2023     Academia        "AI hallucination" refers to occurrences where Chat-        0
                                                        GPT produces statements that are difficult to verify,
                                                        appear plausible, but do not withstand scrutiny, often
                                                        being arbitrary in nature.
156    Huang     et      al.   2023     Health/         "AI hallucination" refers to responses generated by         5
       [181]                            Academia        LLMs in a convincing appearance but are actually
                                                        incorrect statements.
157    Wecel     et      al.   2023     Large           "AI hallucination" refers to the occurrence where AI        0
       [182]                            Language        produces texts that are not consistent with reality and
                                        Model           contain confused facts.
158    Ge and Lai [18]         2023     Health/         "AI hallucination" occurs where the LLM model gen-          21
                                        Academia        erates confident, specific, and fluent answers that are
                                                        factually completely wrong. Also called "stochastic
                                                        parroting."
159    Bhatti [183]            2023     Health/         "AI hallucination" occurs when the information gen-         0
                                        Academia        erated by ChatGPT may not always be correct.
160    Bryant [25]             2023     Academia        "AI hallucination" refers to answers that are fabricated    2
                                                        when data are insufficient for an accurate response.
161    Mahyoob et al.          2023     Academia        "AI hallucination" occurs when it generates ideas with      0
       [184]                                            human-like fluency and persuasiveness but without
                                                        truth or factual accuracy.
162    Lee [185]               2023     Mathematical    "AI hallucination" occurs when GPT models generate          14
                                        Setting         outputs that are contextually implausible or inconsis-
                                                        tent with the real world.
163    Piñeiro-Martín et       2023     Ethical Set-    "AI hallucination" occurs when the LLM generates            0
       al. [186]                        ting            text that goes beyond the scope of the provided input
                                                        or fabricates information that is factually incorrect.
164    Alhaidry et al.         2023     Health          "AI hallucination" occurs when ChatGPT can produce          10
       [187]                                            answers that appear reliable but are incorrect.
165    Khurana     and         2023     Health/         "AI hallucination" occurs when AI generates sentences       1
       Vaddi [188]                      Academia        with the intent to convince the reader, which can be
                                                        misleading, particularly to inexpert readers.
166    Li et al. [189]         2023     Health          "AI hallucination" occurs when LLMs occasionally         11
                                                        generate fallacious and harmful assertions beyond
                                                        their knowledge expertise.
                                                                                                   Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                            Citation

167    Walczak       and     2023     Academia        "AI hallucination" occurs as a primary limitation asso-      0
       Cellary [190]                                  ciated with LLMs, manifesting as the tendency to gen-
                                                      erate errors, including mathematical, computational,
                                                      and conceptual inaccuracies, without any prior indica-
                                                      tion, often characterized by their deceptive plausibility,
                                                      alignment with truthful information, and conveyed in
                                                      a persuasive and self-assured manner, making their
                                                      detection challenging without careful scrutiny and
                                                      rigorous fact verification.
168    Marinaccio et al.     2023     Legal           "AI hallucination" occurs when the output is delivered       0
       [191]                          Setting         in an authoritative and convincing manner, potentially
                                                      leading uninformed users to blindly accept it as truth,
                                                      despite its inaccuracies or falsehoods.
169    Gebrael    et   al.   2023     Health          "AI hallucination" occurs when AI models, particu-           3
       [192]                                          larly in cases like ChatGPT, generate outputs that ap-
                                                      pear plausible but are factually incorrect or unrelated
                                                      to the input context.
170    Tiwari     et   al.   2023     Health          "AI hallucination" occurs as false knowledge that            5
       [193]                                          appears convincing from a scientific perspective.
171    Bhattacharyya         2023     Health/         "AI hallucination" occurs as a phenomenon where              10
       [194]                          Academia        nonsensical or inaccurate content is generated.
172    Sriwastwa et al.      2023     Academia        "AI hallucination" refers to the phenomenon in Chat-         0
       [23]                                           GPT output where the text is cogent but not necessar-
                                                      ily true, often presenting as a complete fabrication.
173    Dossantos et al.      2023     Health          "AI hallucination" occurs when ChatGPT produces in-          1
       [195]                                          accurate results, particularly in specialized topics, due
                                                      to a lack of depth and inaccurate details retrieved from
                                                      the LLM’s database, with no assurance that ChatGPT’s
                                                      suggestions adhere to evidence-based guidelines or
                                                      best practices.
174    Loos et al. [196]     2023     Academia        "AI hallucination" occurs as a phenomenon charac-            0
                                                      terized by the generation of incorrect or outdated
                                                      information, accompanied by the failure to provide
                                                      reliable sources to evaluate the generated content.
175    Koga [197]            2023     Academia        "AI hallucination" occurs when LLMs generate seem-           0
                                                      ingly credible but fabricated information, particularly
                                                      concerning when they create fictitious citations.
176    Birenbaum [198]       2023     Academia        "AI hallucination" occurs when the model fabricates          3
                                                      information and provides untraceable references to
                                                      support its claims.
177    Cascella    et al.    2023     Health          "AI hallucination" refers to the ability of ChatGPT          182
       [199]                                          to produce answers that sound believable but may be
                                                      incorrect or nonsensical.
178    Aronson [200]         2023     Health/         "AI hallucination" refers to all the pieces of misinfor-     0
                                      Academia        mation generated.
179    Kumar      et   al.   2023     Academia        "AI hallucination" refers to instances where an AI           0
       [201]                                          chatbot generates fictional, erroneous, or unsubstan-
                                                      tiated information in response to queries.
180    Gravel et al. [24]    2023     Academia        ChatGPT fabricated a convincing response that con-           19
                                                      tained several factual errors.
181    Ferres     et   al.   2023     Health          "AI hallucination" occurs when a model generates             8
       [202]                                          content that has no basis in reality, creating entirely
                                                      made-up stories or facts.
182    Varghese    and       2023     Health          "AI hallucination" occurs when faced with topics             1
       Chapiro [203]                                  that the model has not received adequate training or
                                                      supervision for, resulting in fabricated output delivered
                                                      with a strong sense of certainty.
183    Dunn and Cian-        2023     Health          "AI hallucination" occurs as a limitation, characterized      1
       flone [204]                                    by the lack of transparency in how the output is
                                                      generated.
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

184    Theodosiou and          2023     Health          "AI hallucinations" is the phenomenon in which a gen-       5
       Read [205]                                       erative AI tool confidently asserts a factual inaccuracy.
185    Fesenmaier and          2023     Academia        "AI hallucination" occurs when ChatGPT appears to           0
       Wober [206]                                      create fake or inaccurate findings that seem plausible.
186    Sohail     et     al.   2023     ChatGPT         "AI hallucination" occurs when ChatGPT generates            5
       [207]                                            new data or information that does not exist.
187    Karakas     et    al.   2023     Health          "AI hallucination" occurs when the AI sometimes             0
       [14]                                             generates responses that sound plausible but are, in
                                                        fact, incorrect. Also called "confabulation."
188    Lyons et al. [208]      2023     Health          "AI hallucination" occurs when LLMs create re-              0
                                                        sponses to prompts by sampling from the language
                                                        distribution within their dataset, potentially leading
                                                        to the generation of incorrect information and the
                                                        propagation of biases.
189    Opdahl     et     al.   2023     Journalism      "AI hallucination" occurs when the model generates          1
       [209]                                            plausible-sounding nonsense, including texts that con-
                                                        tain elements not found in the input data.
190    Blanchard et al.        2023     Health/         "AI hallucination" occurs when probabilities from the       2
       [210]                            Academia        transformer model architecture can generate made-up
                                                        answers, taking numerous forms, from false references
                                                        in scientific reports to misinformation in journalistic
                                                        articles or incorrect formulas when debugging pro-
                                                        gramming code.
191    Lim et al. [211]        2023     Health          "AI hallucination" occurs when LLMs lack domain-            1
                                                        specific capabilities, rendering them susceptible to
                                                        generating convincing yet potentially inaccurate re-
                                                        sponses.
192    Kim et al. [212]        2023     Health          "AI hallucination" occurs as the possibility of provid-     0
                                                        ing incorrect information or exhibiting errors in the
                                                        inference process.
193    Alqahtani et al.        2023     Academia        "AI hallucination" refers to generating non-existent or     15
       [213]                                            incorrect content and other related concerns associ-
                                                        ated with limited contexts, reliability, and the lack of
                                                        learning from experience.
194    Jairoun    et     al.   2023     Health          "AI hallucination" occurs as statements that are factu-     3
       [214]                                            ally inaccurate yet appear plausible to the layman.
195    Šlapeta [215]           2023     Health          "AI hallucination" occurs when highly confident an-         12
                                                        swers are returned by AI that cannot be explained by
                                                        the training data alone.
196    Dillion    et     al.   2023     Science         "AI hallucination" occurs as outputs that appear sen-       28
       [216]                                            sical but are inaccurate.
197    Thomson et al.          2023     Text Sum-       "AI hallucination" occurs when the output text in-          3
       [217]                            marization      cludes an attribute that was not present in the input
                                                        data.
198    Balas     and    Ing    2023     Health          "AI hallucination" refers to producing confident re-        19
       [218]                                            sponses that sound plausible yet are factually incor-
                                                        rect.
199    Salah et al. [219]      2023     Science         "AI hallucination" refers to information that sounds        2
                                                        plausible but is entirely fabricated or not supported
                                                        by the input data.
200    Ilicki et al. [220]     2023     Health          "AI hallucination" refers to text responses that are        3
                                                        either nonsensical or unfaithful to the content they
                                                        should use.
201    Jansen     et     al.   2023     Survey Set-     "AI hallucination" refers to generating nonsensical or      2
       [221]                            ting            inappropriate responses to survey questions.
202    Casal         and       2023     Academia        "AI hallucination" refers to the tendency to invent         1
       Kessler [222]                                    content.
203    Qi et al. [223]         2023     Health          "AI hallucination" occurs when ChatGPT generates             11
                                                        responses that appear plausible but require correction,
                                                        including the invention of terms it is familiar with.
                                                                                                       Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                              Citation

204    Lin [224]               2023     Academia        "AI hallucination" occurs when LLMs fabricate facts,           22
                                                        creating confident-sounding statements and legitimate-
                                                        looking citations that are false.
205    Janssen      et   al.   2023     Health          "AI hallucination" occurs where the model generates            16
       [225]                                            text that is factually incorrect or nonsensical, despite
                                                        appearing confident in its ability.
206    Shen et al. [226]       2023     Health          "AI hallucination" occurs when LLMs produce seem-              235
                                                        ingly credible but incorrect responses, including the
                                                        invention of terms they should be familiar with.
207    Thirunavukarasu         2023     Health          "AI hallucination" occurs when chatGPT describes in-           23
       [227]                                            accurate information as lucidly as it does with correct
                                                        facts.
208    Qadir [228]             2023     Academia        "AI hallucination" occurs when ChatGPT generates               157
                                                        nonsensical or false information (misinformation).
209    Frieder      et   al.   2023     Mathematical    "AI hallucination" occurs when GPT, after answering            126
       [229]                            Setting         correctly or incorrectly, tells the user unrelated infor-
                                                        mation.
210    Borji [20]              2023     ChatGPT         "AI hallucination" refers to inaccuracies in informa-          158
                                                        tion or statements that are not in accordance with
                                                        reality or the truth, often unintentional but resulting
                                                        in incorrect or misleading information, particularly in
                                                        the context of chatbots. Also called "Factual errors."
211    OpenAI [230]            2023     ChatGPT         "AI hallucination" occurs when ChatGPT produces                0
                                                        content that is nonsensical or untruthful in relation to
                                                        certain sources.
212    Manakul et al.          2023     Large           "AI hallucination" refers to the tendency of LLMs to           48
       [231]                            Language        hallucinate facts and fabricate information.
                                        Model
213    Nori et al. [232]       2023     Health          "AI hallucination" refers to erroneous generations by          127
                                                        ChatGPT.
214    Li et al. [233]         2023     Large           "AI hallucination" occurs when LLMs generate con-              4
                                        Language        tent that conflicts with the source or cannot be verified
                                        Model           by factual knowledge.
215    Zhao et al. [234]       2023     Large           "Intrinsic hallucination" refers to the generated infor-       188
                                        Language        mation in conflict with the existing source.
                                        Model           "Extrinsic hallucination" refers to the generated infor-
                                                        mation that cannot be verified by the available source.
216    Adlakha et al.          2023     Question        "AI hallucination" occurs when conversational models           5
       [235]                            Answering       produce factually incorrect or unsupported statements.
217    Athavale et al.         2023     Health          "AI hallucination" refers to generating syntactically          0
       [236]                                            correct but factually incorrect responses that seem
                                                        plausible.
218    Chen et al. [237]       2023     Health          "AI hallucination" occurs when ChatGPT makes as-               1
                                                        sumptions on details not provided in the input data.
219    Stahl and Eke           2023     ChatGPT         "AI hallucination" refers to mistakes that ChatGPT             1
       [238]                                            makes when generating text that is semantically cor-
                                                        rect but factually incorrect or even nonsensical.
220    Walters      and        2023     Academia        "AI hallucination" occurs when ChatGPT provides                0
       Wilder [239]                                     factually incorrect responses.
221    Munro and Hope          2023     Health/         "AI hallucination" occurs when ChatGPT provided                0
       [240]                            Academia        confident responses that seemed faithful and nonsen-
                                                        sical.
222    Hashimoto and           2023     Health/         "AI hallucination" occurs when LLM-generated text               1
       Johnson [241]                    Academia        is based on the statistical associations of patterns of
                                                        words to those seen in training data and prompts, re-
                                                        sulting in elements such as unnecessary and unnatural
                                                        repetition, lack of clarity or specificity, out-of-context
                                                        content, inconsistent phrasing and arguments (particu-
                                                        larly across long essays), incorrect word selection, and
                                                        occasional incorrect statements presented as facts.
                                                                                                          Continued on next page
                                     TABLE V – continued from previous page
Num.      Author(s)           Year     Application                           Definition                           Citation

223    Jain [242]             2023     ChatGPT         "AI hallucination" occurs when ChatGPT occasionally        0
                                                       generates responses that appear correct but are nonsen-
                                                       sical and discordant from real-world data, resulting in
                                                       data inaccuracy.
224    Ho et al. [243]        2023     ChatGPT         "AI hallucination" occurs when ChatGPT "halluci-           0
                                                       nates" false citations that appear convincingly real but
                                                       cannot be located in any medical database.
225    Currie [244]           2023     ChatGPT         "AI hallucination" refer to false or misleading infor-     21
                                                       mation produced by AI. A hallucination is a plausible
                                                       response that is incorrect (it seems correct to ChatGPT
                                                       but is not).
226    Liu et al. [245]       2023     Question        "AI hallucination" is LLM-generated answer which is        1
                                       Answering       not factual-grounded and sometimes severely wrong.
227    Campbell      and      2023     Generative      "AI hallucination" refers to the propensity to generate    0
       Jovanović [246]                AI              assertions with no factual data, often deceives users
                                                       into believing they are accurate.
228    Kshetri [247]          2023     Generative      "AI hallucination" refers to ChatGPT’s results that are    0
                                       AI              incomplete or misleading.
229    Ali et al. [248]       2023     Health          "AI hallucination" occurs when AI creates un-              1
                                                       grounded, subtly incorrect information without self-
                                                       awareness.
230    Tay [249]              2023     Health/         "AI hallucination" refers to the phenomenon by which       0
                                       Academia        ChatGPT could convincingly produce factually inac-
                                                       curate statements
231    Xu and Cohen           2023     Text Sum-       "AI hallucination" occurs when the model confidently       1
       [250]                           marization      generates false information.
232    Kernan Ferier et       2023     Large           "AI hallucination" commonly refers to the                  0
       al. [251]                       Language        phenomenon where LLMs occasionally generate
                                       Model           outputs that, although appearing plausible, deviate
                                                       from the user input, previously generated context, or
                                                       factual knowledge.

                                                       "Intrinsic hallucination" refers to contradictions
                                                       between source material (e.g., training data and
                                                       prompts), where generated content contradicts the
                                                       information present in the source material.

                                                       "Extrinsic hallucination" refers to information
                                                       generated that cannot be verified by the source
                                                       material, and may include content that lacks support
                                                       or contradiction within the provided source data.
233    Tsai et al. [252]      2023     Legal           "AI hallucination" occurs when LLMs generate mis-          3
                                       Setting         leading text and information.
234    Xin et al. [253]       2023     Dialogue        "AI hallucination" refers to the occurrence where pre-     0
                                                       trained text generation models occasionally generate
                                                       text that is nonsensical or unfaithful to the provided
                                                       source input.
235    Murgia      et   al.   2023     ChatGPT         "AI hallucination" occurs when generative LLMs gen-        1
       [254]                                           erate unintended text, leading to degraded system per-
                                                       formance and unmet user expectations in real-world
                                                       scenarios.
236    Kunze     et     al.   2023     Large           "AI hallucination" refers to the phenomenon where the      0
       [255]                           Language        GPT model appears to be designed to provide incorrect
                                       Model           answers rather than admit this to its users.
237    Cascella [256]         2023     Academia        "AI hallucination" refers to ChatGPT generating an-        0
                                                       swers that sound credible but may be incorrect or
                                                       nonsensical.
238    Watanabe [257]         2023     Government      "AI hallucination" refers to the phenomenon where AI        0
                                                       text generators produce statements that are consistent
                                                       with the system’s internal logic but are not based on
                                                       any true context or source.
                                                                                                     Continued on next page
                                    TABLE V – continued from previous page
Num.      Author(s)          Year     Application                            Definition                           Citation

239    Nakaura et al.        2023     Health          "AI hallucination" occurs when LLMs like GPT series         0
       [258]                                          have the potential to generate inaccurate content.
240    Feuerriegel et al.    2023     Generative      "AI hallucination" refers to mistakes in the generated      0
       [259]                          AI              text that are semantically or syntactically plausible but
                                                      are actually nonsensical or incorrect. In other words,
                                                      the generative AI model produces content that is not
                                                      based on any facts or evidence, but rather on its
                                                      own assumptions or biases. Moreover, the output of
                                                      generative AI, especially that of LLMs, is typically
                                                      not easily verifiable.
241    Hryciw    et    al.   2023     Health          "AI hallucination" occurs when AI algorithms, which         0
       [260]                                          are not infallible, produce false or misleading infor-
                                                      mation.
242    Bran et al. [261]     2023     Generative      "AI hallucination" refers to nonsense expressed in an       5
                                      AI              authoritative tone and filling knowledge gaps with
                                                      falsehoods.
243    Sovrano    et   al.   2023     ChatGPT         "AI hallucination" occurs when content is generated         0
       [262]                                          that does not maintain fidelity to a given context or
                                                      source.
244    Mishra    et    al.   2023     ChatGPT         "AI hallucination" occurs when models produce out-          0
       [263]                                          puts that are not grounded in their training data.
245    Lam et al. [264]      2023     Legal           "AI hallucination" refers to LLMs output results are        1
                                      Setting         not realistic, do not follow user given context or match
                                                      any data patterns that it has been trained on.
246    Wan et al. [265]      2023     Text Sum-       "AI hallucination" occurs when the generated sum-           4
                                      marization      mary contains facts or entities not present in the
                                                      original document.
247    Houston     and       2023     Academia        "AI hallucination" commonly refers to the phe-              2
       Corrado [266]                                  nomenon where LLMs occasionally generate outputs
                                                      that, although appearing plausible, deviate from the
                                                      user input, previously generated context, or factual
                                                      knowledge.
248    González-Mora         2023     Natural         "AI hallucination" refers to the generation of texts that   2
       et al. [267]                   Language        are apparently well-written but unsubstantiated and not
                                      Generation      faithful to the provided data.
249    Lim et al. [268]      2023     Dialogue        "AI hallucination" refers to situations where the gen-      0
                                                      erated output contradicts the reference knowledge and
                                                      includes instances when the generated output cannot
                                                      be confirmed from the knowledge source.
250    Alowais    et   al.   2023     Health          "AI hallucination" refers to the tendency of AI-            2
       [269]                                          generated data and/or analysis to fabricate and create
                                                      false information that cannot be supported by existing
                                                      evidence, even though it may appear realistic and
                                                      convincing.
251    Liu et al. [270]      2023     Health          "AI hallucination" can be defined as the capability of      33
                                                      LLM-based chatbots to convey false information as
                                                      though it were true.
252    Xie et al. [271]      2023     Health          "AI hallucination" refers to when a medical AI              0
                                                      system is considered unfaithful or to have a factual
                                                      inconsistency issue, as it generates content that is not
                                                      supported by existing knowledge, reference, or data.

                                                      Intrinsic Error: The generated output contradicts
                                                      with existing knowledge, reference, or data.

                                                      Extrinsic Error: The generated output cannot be
                                                      confirmed (either supported or contradicted) by
                                                      existing knowledge, reference, or data.
253    Bernstein et al.      2023     Health          "AI hallucination" refers to chatbot outputs that sound      0
       [272]                                          convincingly plausible yet are factually inaccurate.
                                                                                                     Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                           Definition                           Citation

254    Heck [273]              2023     Health          "AI hallucination" refers to ChatGPT providing text        3
                                                        that combines real and fabricated evidence, resulting
                                                        in plausible answers, occasionally appearing as
                                                        nonsense when assessed according to the common
                                                        knowledge of experts in different areas, also called
                                                        "mistakes."

                                                        “Intrinsic hallucinations” occur when the generation
                                                        output contradicts the source content.

                                                        “Extrinsic hallucinations” occur when the output
                                                        can neither be supported nor contradicted by the
                                                        source.
255    Ray             and     2023     Health          "AI hallucination" refers to the generation of inaccu-     0
       Majumder                                         rate or false information by AI-based systems such as
       [274]                                            ChatGPT.
256    Ang et al. [275]        2023     Health/         "AI hallucination" refers to the phenomenon where          5
                                        Academia        text generated by ChatGPT may appear credible but
                                                        can be pure confabulation, containing a combination
                                                        of both facts and fabricated information, or entirely
                                                        fictitious pseudoscientific material.
257    Talyshinskii       et   2023     Health/         "AI hallucination" refers to a phenomenon in writing       0
       al. [276]                        Academia        influenced more by learned patterns than scientific
                                                        facts, which leads to mistakes.
258    Kung et al. [277]       2023     Health          "AI hallucination" refers to the phenomenon where          0
                                                        ChatGPT, while citing a verifiable source, may draw
                                                        information that is outdated or entirely incorrect, de-
                                                        spite providing logical justifications for its answer
                                                        choices, thus leading to logical errors and assertions
                                                        of false facts.
259    Friederichs et al.      2023     Health/         "AI hallucination" refers to the behavior in which         5
       [278]                            Academia        wrong answers are just as convincingly justified as
                                                        correct ones, a phenomenon not uncommon in large
                                                        language models.
260    Ghorashi et al.         2023     Health/         "AI hallucination" occurs when chatbots have the           0
       [279]                            Academia        potential to falsify and create references.
261    McBee     et     al.    2023     Sport           "AI hallucination" occurs as the generation of unsup-      0
       [280]                                            ported or false information, which is a prevalent issue
                                                        with LLM-based chatbots.
262    Reis [281]              2023     Health          "AI hallucination" may generate plausible sounding         1
                                                        but incorrect or nonsensical answers that does not
                                                        seem to be justified by its training data, such as claim
                                                        to be human.
263    Joachimiak et al.       2023     Genetic         "AI hallucination" ocuurse when LLM model fabri-           0
       [282]                                            cated a term for a gene set.
264    Chen [283]              2023     Academia        "AI hallucination" refers to false or nonsense informa-    0
                                                        tion presented as fact by an LLM.
265    Delsoz    et     al.    2023     Health/         "AI hallucination" occurs when ChatGPT generates           0
       [284]                            ChatGPT         responses that appear fluent and believable but may
                                                        contain factual inaccuracies.
266    Takagi    et     al.    2023     Health/         "AI hallucination" is defined as producing nonsensical     11
       [285]                            ChatGPT         or untruthful content concerning certain sources.
267    Kaneda     et    al.    2023     Health/         "AI hallucination" occurs when ChatGPT provides            3
       [286]                            ChatGPT         erroneous information in a naturalistic manner.
268    Ahn [287]               2023     Health          "AI hallucination" refers to the generation of false       12
                                                        or logically incorrect text that appears plausible and
                                                        grammatically correct.
269    Jin et al. [288]        2023     Health          "AI hallucination" refers to the occurrence where        0
                                                        LLMs usually produce plausible-sounding but incor-
                                                        rect outputs.
                                                                                                   Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                           Definition                           Citation

270    Ahn [289]               2023     Health/         "AI hallucination" refers to a phenomenon where            0
                                        ChatGPT         outputs may deviate from factual accuracy or provided
                                                        context.
271    Liu et al. [290]        2023     Health/         “AI hallucination” refers to the fact that the content     20
                                        ChatGPT         generated by the model is not based on reality, creating
                                                        a completely fabricated story or fact.
272    Doorn [291]             2023     Water           "AI hallucination" refers to LLMs’ tendency to gen-        0
                                        Domain          erate factually nonsensical or incorrect text.
273    Hiesinger et al.        2023     Health          "AI hallucination" refers to the propensity of LLMs        3
       [292]                                            to generate factually incorrect statements.
274    Palal et al. [293]      2023     Health/         "AI hallucination" occurs when ChatGPT generates           0
                                        ChatGPT         inaccurate or contradictory information.
275    Coskun     et     al.   2023     Question        "AI hallucination" refers to an instance where the         0
       [294]                            Answering/      model generates information that is not supported by
                                        ChatGPT         existing evidence or factual data.
276    Baldassarre et al.      2023     ChatGPT         "AI hallucination" refers to the phenomenon of inac-       0
       [295]                                            curate information.
277    Boujemaa et al.         2023     Retail          "AI hallucination" occures where the model produces        0
       [296]                                            untruthful or misleading information.
278    Li et al. [297]         2023     Dialogue        "AI hallucination" are NLP generated content that          0
                                                        appear to be relevant bbut are not actually faithful to
                                                        the underlting text.
279    Urban     et      al.   2023     Database        "AI hallucination" is a phenomenon where LLMs              1
       [298]                                            generate non-factual statements.
280    Mahon      et     al.   2023     Academia        "AI hallucination" is the generation of output that        0
       [299]                                            appears convincing but is factually untrue or unrelated
                                                        to the current context.
281    August     et     al.   2023     Health          "AI hallucination" refers to the limitation in current     20
       [300]                                            text generation capabilities, which carries the risk of
                                                        generating factually incorrect or inconsistent text.
282    Fischer [301]           2023     Legal           "AI hallucination" refers to a phenomenon where an         0
                                        Setting         LM, or Language Model, is a system for haphazardly
                                                        stitching together sequences of linguistic forms it
                                                        has observed in its vast training data, according to
                                                        probabilistic information about how they combine,
                                                        but without any reference to meaning. Also called
                                                        "Stochastic Parrots."
283    Lee et al. [302]        2023     Dialogue        "AI hallucination" are LLMs generated information          3
                                                        that is non-factual or nonsensical.
284    Scotti et al. [303]     2023     Chatbot         "AI hallucination" refers to the case where agents         0
                                                        generate responses without actually knowing the infor-
                                                        mation it is talking about or without referring to some
                                                        knowledge base, leading to possibly wrong/misleading
                                                        information.
285    Zhan et al. [304]       2023     ChatGPT         "AI hallucination" refers to the phenomenon, also          0
                                                        known as the "hallucination effect," where chatbots
                                                        like ChatGPT may generate misleading and deceptive
                                                        information that can have adverse impacts on users
                                                        who may struggle to distinguish fact from fiction.
286    Vargas-Murillo          2023     Academia        "AI hallucination" refers to the phenomenon, also          0
       et al. [305]                                     known as the "hallucination effect," which causes an
                                                        AI to invent familiar terms.
287    White et al. [306]      2023     Text Sum-       "AI hallucination" refers to the tendency to produce       0
                                        marization      content that is nonsensical or untruthful in relation to
                                                        certain sources.
288    Cusumano [307]          2023     Technology      "AI hallucination" occurs when LLMs, in the absence       0
                                                        of an answer to a query, use predictive analytics
                                                        to make up reasonable but sometimes incorrect re-
                                                        sponses.
                                                                                                    Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

289    Nashid       et   al.   2023     Large           "AI hallucination" refers to ChatGPT’s outputs that are     12
       [308]                            Language        incorrect or nonsensical.
                                        Model
290    Pataranutaporn et       2023     Technology      "AI hallucination" refers to the phenomenon where           2
       al. [309]                                        ChatGPT generates content that is nonsensical or
                                                        unfaithful to the provided source content.
291    Faggioli     et   al.   2023     Large           "AI hallucination" occurs when LLMs generate text           0
       [310]                            Language        that contains inaccurate or false information, often
                                        Model           in an affirmative manner that makes it difficult for
                                                        humans to even suspect errors.
292    Marczak-Czajka          2023     Human-          "AI hallucination" occurs when generative models pro-       0
       and    Cleland-                  Value Story     duce output that is nonsensical with incorrect, bizarre
       Huang [311]                                      logic.
293    Stefńnska et al.       2023     Quantum         "AI hallucination" occurs when LLMs generate data           0
       [312]                            Physics         not observed in the training dataset.
294    Ezenkwu [313]           2023     Customer        "AI hallucination" occurs when ChatGPT generates            0
                                        Service         responses that are irrelevant or incorrect.
295    Fayyad [314]            2023     ChatGPT         "AI hallucination" occurs when generative AI models         0
                                                        lose track of the source of information, lacking reason-
                                                        ing capability or semantic understanding, and instead
                                                        resort to autocompletion through pattern matching,
                                                        resulting in making things up.
296    Mrabet      and         2023     ChatGPT         "AI hallucination" refers to The AI’s inability to          2
       Studholme [315]                                  understand what it has written is clear.
297    Pitt [316]              2023     Academia        "AI hallucination" occurs when the AI/LLM produces          0
                                                        a plausible output that, however, does not seem to be
                                                        warranted by the training data.
298    Crosthwaite and         2023     ChatGPT         "AI hallucination" occurs when ChatGPT invents              1
       Baisa [317]                                      terms that lie outside of its training data.
299    Vinny [318]             2023     Health          "AI hallucination" occurs when LLMs generate erro-          0
                                                        neous medical information to support their opinions
300    Solyman et al.          2023     Grammar         "AI hallucination" occurs when the system produces          4
       [319]                                            translations that are completely inadequate due to an
                                                        overreliance on the target context in NMT.
301    Waqas        et   al.   2023     Health          "AI hallucination" refers to a known limitation of          0
       [320]                                            generative AI, encompassing mistakes in the generated
                                                        text or images that are semantically, syntactically, or
                                                        visually plausible but are, in fact, incorrect, nonsensi-
                                                        cal, and do not refer to any real-world concepts.
302    Stephens et al.         2023     Health          "AI hallucination" occurs when chatbots may propa-          0
       [321]                                            gate erroneous information or even make up informa-
                                                        tion.
303    Dien [322]              2023     Health/         "AI hallucination" refers to the highly susceptible         5
                                        Academia        nature of ChatGPT to produce erroneous outputs.
304    Abu-Farha et al.        2023     Health          "AI hallucination" refers to the generation of scien-       0
       [323]                                            tifically false content that might seem convincing to
                                                        nonexperts.
305    Tippareddy et al.       2023     Health/         "AI hallucination" refers to confident responses gen-       0
       [324]                            Academia        erated by ChatGPT without being justified by training
                                                        data.
306    Oviedo-                 2023     ChatGPT         "AI hallucination" refers to the occurrence where           18
       Trespalacios                                     ChatGPT can produce answers that appear credible
       et al. [325]                                     but may be incorrect or nonsensical.
307    Sarraju      et   al.   2023     Health/         "AI hallucination" refers to Inaccurate information         0
       [326]                            ChatGPT         may be presented in a confident manner, including
                                                        nonexistent references to scientific literature. Also
                                                        called "confabulation."
                                                                                                      Continued on next page
                                      TABLE V – continued from previous page
Num.      Author(s)            Year     Application                            Definition                           Citation

308    Coskun    et      al.   2023     Health/         "AI hallucination" refers to situations where the AI        0
       [327]                            ChatGPT         generates responses that are not derived from its
                                                        training data, potentially leading to inaccuracies or
                                                        misunderstandings.
309    Lam [328]               2023     Health          "AI hallucination" refers to occurrences where AI           0
                                                        models output plausible but factually incorrect infor-
                                                        mation.
310    Pantanowitz             2023     Health/         "AI hallucination" occurs when the AI imagines facts        0
       and Pantanowitz                  ChatGPT         that are not real as being real or makes reasoning errors
       [329]                                            that it should not be making.
311    Boussen et al.          2023     Health/         "AI hallucination" refers to the occurrence where a         0
       [330]                            Academia        model might ’invent’ information that seems plausible
                                                        based on the patterns and structures it has learned.
312    Schlam    et      al.   2023     Health/         "AI hallucination" refers to the phenomenon where           0
       [331]                            Academia        NLPs often make subtle mistakes.
313    Lyu and        Wu       2023     Academia        "AI hallucination" occurs when LLM responses can            0
       [332]                                            sometimes be repetitive or contain non-factual infor-
                                                        mation.
314    Sparkes [333]           2023     Chatbot         "AI hallucination" refers to the phenomenon where an        0
                                                        AI, in response to prompts, will produce convincing
                                                        statements that are actually inaccurate or totally false
315    Bhatia       and        2023     Academia        "AI hallucination" occurs when the AI sometimes             0
       Kulkarni [334]                                   writes plausible-sounding but incorrect, nonsensical
                                                        answers.
316    Chatelan et al.         2023     Health          "AI hallucination" occurs when ChatGPT makes up             0
       [335]                                            or distorts facts, including the creation of made-up
                                                        references.
317    Lareyre   et      al.   2023     Health          "AI hallucination" refers to the phenomenon where the       0
       [336]                                            model can generate content output that is incorrect or
                                                        nonsensical, despite appearing reliable.
318    Wilkins [337]           2023     Health          "AI hallucination" occurs when the system erro-             0
                                                        neously generates “fantastical, unfaithful, or nonsen-
                                                        sical outputs.”
319    Piazza    et      al.   2023     Academia        "AI hallucination" refers to the phenomenon where           0
       [338]                                            ChatGPT may generate output that is grammatically
                                                        correct and coherent but may not be appropriate for
                                                        the intended audience or purpose.
320    Scanlon   et      al.   2023     Academia        "AI hallucination" refers to the phenomenon where           2
       [339]                                            ChatGPT prioritize generating humanlike text in re-
                                                        sponse to a prompt, often leading them to focus
                                                        on providing an answer rather than the correct one,
                                                        resulting in inaccurate or incorrect responses presented
                                                        to users with an unfounded confidence, and even gen-
                                                        erating fake bibliographic information when asked for
                                                        references, which could be misleading if not critically
                                                        examined.
321    Kaneda [340]            2023     Health          "AI hallucination" refers to a phenomenon where             1
                                                        ChatGPT generates plausible but untrue responses.
322    Tan et al. [341]        2023     Health          "AI hallucination" refers to the occurrence where in-       0
                                                        vented, inaccurate statements are presented as lucidly
                                                        as accurate information. Also called "fact fabrication."
323    Ai et al. [342]         2023     Information     "AI hallucination" occurs when LLMs may occasion-           3
                                        Retrieval       ally generate erroneous or nonsensical responses.
324    Ruma et al. [343]       2023     Natural         "AI hallucination" occurs when an NLG system gen-           0
                                        Language        erates unfaithful or nonfactual content.
                                        Generation
325    Mao et al. [344]        2023     Large           "AI hallucination" refers to a scenario where the       0
                                        Language        generated content by LLMs appears plausible but is,
                                        Model           in fact, entirely fictional.
                                                                                                  Continued on next page
                                 TABLE V – continued from previous page
Num.      Author(s)       Year     Application                           Definition                           Citation

326    Chaiken [345]      2023     Health          "Hallucination in AI" refers to the generation of out-     0
                                                   puts that may sound plausible but are either factually
                                                   incorrect or unrelated to the given context. These out-
                                                   puts often emerge from the AI model’s inherent biases,
                                                   lack of real-world understanding, or training data lim-
                                                   itations. In other words, the AI system "hallucinates"
                                                   information that it has not been explicitly trained on,
                                                   leading to unreliable or misleading responses.
327    Otaki [346]        2023     Academia        "AI hallucination" refers to the production of inaccu-     0
                                                   rate or logically incorrect text that appears believable
                                                   and grammatically sound.
328    Triguero et al.    2023     Academia        "AI hallucination" occurs when in LLMs, the system         0
       [347]                                       may output untrue statements with high confidence.
329    Huang [348]        2023     Health/         "AI hallucination" occurs when ChatGPT produces            0
                                   Academia        content that may cause users to depend on measures
                                                   making it challenging to determine the accuracy of
                                                   specific information.
330    Muranga et al.     2023     Academia        "AI hallucination" refers to the phenomenon where AI       0
       [349]                                       often produces completely false information conveyed
                                                   in a convincing manner, including the invention of
                                                   items such as references and citations.
331    Polverini    and   2023     Physics         "AI hallucination" refers to the occurrence where          0
       Gregorcic [350]                             LLM-generated output contains factually incorrect
                                                   statements.
332    Rajendran et al.   2023     Hardware        "AI hallucination" refers to a phenomenon where AI         0
       [351]                       Tech            models create outputs that appear seemingly correct
                                                   and confident but are, in fact, factually wrong.
333    Lobentanzer and    2023     Biomedicine     "AI hallucination" refers to the phenomenon where          0
       Saez-Rodriguez                              LLMs make up facts as they go along, and, to make
       [352]                                       matters worse, are convinced - and convincing - re-
                                                   garding the truth of their hallucinations.
                             R EFERENCES                                         [25] A. Bryant, “Ai chatbots: Threat or opportunity?” Informatics, vol. 10,
                                                                                      no. 2, 2023.
 [1] S. Baker and T. Kanade, “Hallucinating faces,” in Proceedings Fourth        [26] M. Miao, F. Meng, Y. Liu, X.-H. Zhou, and J. Zhou, “Prevent the lan-
     IEEE International Conference on Automatic Face and Gesture Recog-               guage model from being overconfident in neural machine translation,”
     nition (Cat. No. PR00580), 2000, pp. 83–88.                                      arXiv preprint arXiv:2105.11098, 2021.
 [2] H. Xiang, Q. Zou, M. A. Nawaz, X. Huang, F. Zhang, and H. Yu,               [27] K. Lee, O. Firat, A. Agarwal, C. Fannjiang, and D. Sussillo,
     “Deep learning for image inpainting: A survey,” Pattern Recognition,             “Hallucinations in neural machine translation,” 2019. [Online].
     vol. 134, p. 109046, 2023.                                                       Available: https://openreview.net/forum?id=SkxJ-309FQ
 [3] A. Pumarola, A. Agudo, A. Sanfeliu, and F. Moreno-Noguer, “Unsu-            [28] Z. Zhao, S. B. Cohen, and B. Webber, “Reducing quantity hallucina-
     pervised person image synthesis in arbitrary poses,” in Proceedings of           tions in abstractive summarization,” arXiv preprint arXiv:2009.13312,
     the IEEE conference on computer vision and pattern recognition, 2018,            2020.
     pp. 8620–8628.                                                              [29] Y. Dong, S. Wang, Z. Gan, Y. Cheng, J. C. K. Cheung, and
 [4] A. F. Biten, L. Gómez, and D. Karatzas, “Let there be a clock on                 J. Liu, “Multi-fact correction in abstractive text summarization,” arXiv
     the beach: Reducing object hallucination in image captioning,” in                preprint arXiv:2010.02443, 2020.
     Proceedings of the IEEE/CVF Winter Conference on Applications of            [30] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald, “On faith-
     Computer Vision, 2022, pp. 1381–1390.                                            fulness and factuality in abstractive summarization,” arXiv preprint
 [5] A. Braunegg, A. Chakraborty, M. Krumdick, N. Lape, S. Leary,                     arXiv:2005.00661, 2020.
     K. Manville, E. Merkhofer, L. Strickhart, and M. Walmer, “Apricot: A        [31] Y. Lyu, C. Zhu, T. Xu, Z. Yin, and E. Chen, “Faithful abstractive
     dataset of physical adversarial attacks on object detection,” in Computer        summarization via fact-aware consistency-constrained transformer,” in
     Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August                  Proceedings of the 31st ACM International Conference on Information
     23–28, 2020, Proceedings, Part XXI 16. Springer, 2020, pp. 35–50.                & Knowledge Management, 2022, pp. 1410–1419.
 [6] P. Koehn and R. Knowles, “Six challenges for neural machine transla-        [32] OpenAI. (2022) Chatgpt 3.5. https://www.openai.com/.
     tion,” 2017.                                                                [33] C. DeVon. (2023) Ai chatbots can ‘hallucinate’ and
 [7] S. Wiseman, S. M. Shieber, and A. M. Rush, “Challenges in data-to-               make things up—why it happens and how to spot
     document generation,” 2017.                                                      it.    Accessed      on     Jan    03,   2024.     [Online].    Available:
 [8] K. Filippova, “Controlled hallucinations: Learning to generate faith-            https://www.cnbc.com/2023/12/22/why-ai-chatbots-hallucinate.html
     fully from noisy data,” arXiv preprint arXiv:2010.05873, 2020.              [34] K. Weise and C. Metz. (2023) When a.i. chatbots
 [9] T. R. Insel, “Rethinking schizophrenia,” Nature, vol. 468, no. 7321,             hallucinate. Accessed on Jan 03, 2024. [Online]. Available:
     pp. 187–193, 2010.                                                               https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html
[10] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,       [35] S. Diamond. (2023) A.i. chatbots, hens and humans can
     A. Madotto, and P. Fung, “Survey of hallucination in natural language            all ‘hallucinate’. Accessed on Jan 03, 2024. [Online]. Available:
     generation,” ACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023.             https://www.nytimes.com/2023/12/17/insider/ai-chatbots-humans-hallucinate.html
[11] S. D. Østergaard and K. L. Nielbo, “False responses from artificial         [36] C. Thorbecke. (2023) Ai tools make things up a lot, and that’s
     intelligence models are not hallucinations,” Schizophrenia Bulletin, p.          a huge problem. Accessed on Jan 03, 2024. [Online]. Available:
     sbad068, 2023.                                                                   https://www.cnn.com/2023/08/29/tech/ai-chatbot-hallucinations/index.html
[12] R. Emsley, “Chatgpt: these are not hallucinations–they’re fabrications      [37] G. Press. (2023) Celebrating 80 years of hallucinating about
     and falsifications,” Schizophrenia, vol. 9, no. 1, p. 52, 2023.                  artificial intelligence. Accessed on Jan 03, 2024. [Online]. Available:
[13] H. Ye, T. Liu, A. Zhang, W. Hua, and W. Jia, “Cognitive mirage: A                https://www.forbes.com/sites/gilpress/2023/12/30/celebrating-80-years-of-hallucinating
     review of hallucinations in large language models,” 2023.                   [38] C. Dictionary. (2023) The cambridge dictionary word of the
[14] C. Karakas, D. Brock, and A. Lakhotia, “Leveraging chatgpt in the                year 2023 is. . . . Accessed on Jan 03, 2024. [Online]. Available:
     pediatric neurology clinic: Practical considerations for use to improve          https://dictionary.cambridge.org/editorial/woty
     efficiency and outcomes,” Available at SSRN 4475000, 2023.                  [39] Dictionary.com.         (2023)     Word      of      the     year.    Ac-
[15] D. L. Rodgers, M. Needler, A. Robinson, R. Barnes, T. Brosche,                   cessed        on      Jan      03,     2024.     [Online].      Available:
     J. Hernandez, J. Poore, P. VandeKoppel, and R. Ahmed, “Artificial                https://content.dictionary.com/word-of-the-year-2023/
     intelligence and the simulationists,” Simulation in Healthcare: Journal     [40] K. Lee, O. Firat, A. Agarwal, C. Fannjiang, and D. Sussillo, “Hallu-
     of the Society for Simulation in Healthcare, 2023.                               cinations in neural machine translation,” 2018.
[16] M. G. Madden, B. A. McNicholas, and J. G. Laffey, “Assessing                [41] F. Nie, J.-G. Yao, J. Wang, R. Pan, and C.-Y. Lin, “A simple
     the usefulness of a large language model to query and summarize                  recipe towards reducing hallucination in neural surface realisation,”
     unstructured medical notes in intensive care,” Intensive Care Medicine,          in Proceedings of the 57th Annual Meeting of the Association for
     pp. 1–3, 2023.                                                                   Computational Linguistics, 2019, pp. 2673–2679.
[17] Z. Li, “The dark side of chatgpt: Legal and ethical challenges from         [42] R. Tian, S. Narayan, T. Sellam, and A. P. Parikh, “Sticking to the facts:
     stochastic parrots and hallucination,” arXiv preprint arXiv:2304.14347,          Confident decoding for faithful data-to-text generation,” arXiv preprint
     2023.                                                                            arXiv:1910.08684, 2019.
[18] J. Ge and J. Lai, “Artificial intelligence-based text generators in hep-    [43] O. Dušek, D. M. Howcroft, and V. Rieser, “Semantic noise matters for
     atology: Chatgpt is just the beginning,” Hepatology Communications,              neural natural language generation,” arXiv preprint arXiv:1911.03905,
     vol. 7, no. 4, 2023.                                                             2019.
[19] N. Curtis et al., “To chatgpt or not to chatgpt? the impact of artificial   [44] T. C. Ferreira, C. van der Lee, E. Van Miltenburg, and E. Krahmer,
     intelligence on academic publishing,” The Pediatric Infectious Disease           “Neural data-to-text generation: A comparison between pipeline and
     Journal, vol. 42, no. 4, p. 275, 2023.                                           end-to-end architectures,” arXiv preprint arXiv:1908.09022, 2019.
[20] A. Borji, “A categorical archive of chatgpt failures,” arXiv preprint       [45] M. Martindale, M. Carpuat, K. Duh, and P. McNamee, “Identifying
     arXiv:2302.03494, 2023.                                                          fluently inadequate output in neural and statistical machine translation,”
[21] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.          in Proceedings of Machine Translation Summit XVII: Research Track,
     Tan, and D. S. W. Ting, “Large language models in medicine,” Nature              2019, pp. 233–243.
     medicine, pp. 1–11, 2023.                                                   [46] O. Dušek and Z. Kasner, “Evaluating semantic accuracy of data-
[22] D. S. J. Ting, T. F. Tan, and D. S. W. Ting, “Chatgpt in ophthalmology:          to-text generation with natural language inference,” arXiv preprint
     the dawn of a new era?” Eye, pp. 1–4, 2023.                                      arXiv:2011.10819, 2020.
[23] A. Sriwastwa, P. Ravi, A. Emmert, S. Chokshi, S. Kondor, K. Dhal,           [47] D. Kang and T. Hashimoto, “Improved natural language generation via
     P. Patel, L. L. Chepelev, F. J. Rybicki, and R. Gupta, “Generative ai            loss truncation,” arXiv preprint arXiv:2004.14589, 2020.
     for medical 3d printing: a comparison of chatgpt outputs to reference       [48] A. P. Parikh, X. Wang, S. Gehrmann, M. Faruqui, B. Dhingra, D. Yang,
     standard education,” 3D Printing in Medicine, vol. 9, no. 1, p. 21, 2023.        and D. Das, “Totto: A controlled table-to-text generation dataset,” arXiv
[24] J. Gravel, M. D’Amours-Gravel, and E. Osmanlliu, “Learning to fake               preprint arXiv:2004.14373, 2020.
     it: limited responses and fabricated references provided by chatgpt for     [49] E. Durmus, H. He, and M. Diab, “Feqa: A question answering
     medical questions,” Mayo Clinic Proceedings: Digital Health, vol. 1,             evaluation framework for faithfulness assessment in abstractive sum-
     no. 3, pp. 226–234, 2023.                                                        marization,” arXiv preprint arXiv:2005.03754, 2020.
[50] C. Zhou, G. Neubig, J. Gu, M. Diab, P. Guzman, L. Zettlemoyer,                 [71] N. Lee, W. Ping, P. Xu, M. Patwary, P. N. Fung, M. Shoeybi, and
     and M. Ghazvininejad, “Detecting hallucinated content in conditional                B. Catanzaro, “Factuality enhanced language models for open-ended
     neural sequence generation,” arXiv preprint arXiv:2011.02593, 2020.                 text generation,” Advances in Neural Information Processing Systems,
[51] H. Elsahar, M. Coavoux, M. Gallé, and J. Rozen, “Self-supervised                    vol. 35, pp. 34 586–34 599, 2022.
     and controlled multi-document opinion summarization,” arXiv preprint           [72] M. A. S. Cabezudo and T. A. S. Pardo, “Exploring a pos-based two-
     arXiv:2004.14754, 2020.                                                             stage approach for improving low-resource amr-to-text generation,” in
[52] X. Tang, A. Nair, B. Wang, B. Wang, J. Desai, A. Wade, H. Li,                       Proceedings, 2022.
     A. Celikyilmaz, Y. Mehdad, and D. Radev, “Confit: Toward faithful              [73] L. van der Poel, R. Cotterell, and C. Meister, “Mutual information
     dialogue summarization with linguistically-informed contrastive fine-               alleviates hallucinations in abstractive summarization,” arXiv preprint
     tuning,” arXiv preprint arXiv:2112.08713, 2021.                                     arXiv:2210.13210, 2022.
[53] M. Cao, Y. Dong, and J. C. K. Cheung, “Hallucinated but factual! in-           [74] N. Dziri, S. Milton, M. Yu, O. Zaiane, and S. Reddy, “On the origin
     specting the factuality of hallucinations in abstractive summarization,”            of hallucinations in conversational models: Is it the datasets or the
     arXiv preprint arXiv:2109.09784, 2021.                                              models?” arXiv preprint arXiv:2204.07931, 2022.
[54] S. Chen, F. Zhang, K. Sone, and D. Roth, “Improving faithfulness               [75] N. Dziri, E. Kamalloo, S. Milton, O. Zaiane, M. Yu, E. M. Ponti,
     in abstractive summarization with contrast candidate generation and                 and S. Reddy, “Faithdial: A faithful benchmark for information-
     selection,” arXiv preprint arXiv:2104.09061, 2021.                                  seeking dialogue,” Transactions of the Association for Computational
[55] H. Wang, Y. Gao, Y. Bai, M. Lapata, and H. Huang, “Exploring explain-               Linguistics, vol. 10, pp. 1473–1490, 2022.
     able selection to control abstractive summarization,” in Proceedings of        [76] F. Koto, T. Baldwin, and J. H. Lau, “Ffci: A framework for inter-
     the AAAI Conference on Artificial Intelligence, vol. 35, no. 15, 2021,              pretable automatic evaluation of summarization,” Journal of Artificial
     pp. 13 933–13 941.                                                                  Intelligence Research, vol. 73, pp. 1553–1607, 2022.
[56] Y. Xiao and W. Y. Wang, “On hallucination and predictive uncertainty           [77] S. M. Goodman, E. Buehler, P. Clary, A. Coenen, A. Donsbach, T. N.
     in conditional language generation,” arXiv preprint arXiv:2103.15025,               Horne, M. Lahav, R. MacDonald, R. B. Michaels, A. Narayanan et al.,
     2021.                                                                               “Lampost: Design and evaluation of an ai-assisted email writing proto-
[57] N. Dziri, A. Madotto, O. Zaiane, and A. J. Bose, “Neural path hunter:               type for adults with dyslexia,” in Proceedings of the 24th International
     Reducing hallucination in dialogue systems via path grounding,” arXiv               ACM SIGACCESS Conference on Computers and Accessibility, 2022,
     preprint arXiv:2104.08455, 2021.                                                    pp. 1–18.
[58] T. Liu, Y. Zhang, C. Brockett, Y. Mao, Z. Sui, W. Chen, and B. Dolan,          [78] S. Gehrmann, E. Clark, and T. Sellam, “Repairing the cracked founda-
     “A token-level reference-free hallucination detection benchmark for                 tion: A survey of obstacles in evaluation practices for generated text,”
     free-form text generation,” arXiv preprint arXiv:2104.08704, 2021.                  Journal of Artificial Intelligence Research, vol. 77, pp. 103–166, 2023.
                                                                                    [79] E. Erdem, M. Kuyu, S. Yagcioglu, A. Frank, L. Parcalabescu, B. Plank,
[59] Y. Huang, X. Feng, X. Feng, and B. Qin, “The factual inconsistency
                                                                                         A. Babii, O. Turuta, A. Erdem, I. Calixto et al., “Neural natural
     problem in abstractive text summarization: A survey,” arXiv preprint
                                                                                         language generation: A survey on multilinguality, multimodality, con-
     arXiv:2104.14839, 2021.
                                                                                         trollability and learning,” Journal of Artificial Intelligence Research,
[60] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models
                                                                                         vol. 73, pp. 1131–1207, 2022.
     mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.
                                                                                    [80] Z. Y. Tun, A. Speggiorin, J. Dalton, and M. Stamper, “Comex: A
[61] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, “Retrieval                   multi-task benchmark for knowledge-grounded conversational media
     augmentation reduces hallucination in conversation,” arXiv preprint                 exploration,” in Proceedings of the 4th Conference on Conversational
     arXiv:2104.07567, 2021.                                                             User Interfaces, 2022, pp. 1–11.
[62] I. Sekulić, M. Aliannejadi, and F. Crestani, “Towards facet-driven            [81] S. Gurrapu, L. Huang, and F. A. Batarseh, “Exclaim: Explainable neural
     generation of clarifying questions for conversational search,” in Pro-              claim verification using rationalization,” in 2022 IEEE 29th Annual
     ceedings of the 2021 ACM SIGIR international conference on theory                   Software Technology Conference (STC). IEEE, 2022, pp. 19–26.
     of information retrieval, 2021, pp. 167–175.                                   [82] J. Yang, D. Vega-Oliveros, T. Seibt, and A. Rocha, “Explainable fact-
[63] S. Ghosh, Z. Qi, S. Chaturvedi, and S. Srivastava, “How helpful                     checking through question answering,” in ICASSP 2022-2022 IEEE
     is inverse reinforcement learning for table-to-text generation?” in                 International Conference on Acoustics, Speech and Signal Processing
     Proceedings of the 59th Annual Meeting of the Association for Com-                  (ICASSP). IEEE, 2022, pp. 8952–8956.
     putational Linguistics and the 11th International Joint Conference on          [83] P. Narayanan Venkit, S. Gautam, R. Panchanadikar, T.-H. Huang, and
     Natural Language Processing (Volume 2: Short Papers), 2021, pp. 71–                 S. Wilson, “Unmasking nationality bias: A study of human perception
     79.                                                                                 of nationalities in ai-generated articles,” in Proceedings of the 2023
[64] L. Perez-Beltrachini and M. Lapata, “Multi-document summarization                   AAAI/ACM Conference on AI, Ethics, and Society, 2023, pp. 554–565.
     with determinantal point process attention,” Journal of Artificial Intel-      [84] F. Leiser, S. Eckhardt, M. Knaeble, A. Maedche, G. Schwabe, and
     ligence Research, vol. 71, pp. 371–399, 2021.                                       A. Sunyaev, “From chatgpt to factgpt: A participatory design study to
[65] R. Ali, O. Y. Tang, I. D. Connolly, J. S. Fridley, J. H. Shin, P. L. Z. Sul-        mitigate the effects of large language model hallucinations on users,”
     livan, D. Cielo, A. A. Oyelese, C. E. Doberstein, A. E. Telfeian et al.,            in Mensch und Computer 2023, 2023, pp. 81–90.
     “Performance of chatgpt, gpt-4, and google bard on a neurosurgery oral         [85] S. Wang, N. Cooper, M. Eby, and E. S. Jo, “From human-centered
     boards preparation question bank,” Neurosurgery, pp. 10–1227, 2022.                 to social-centered artificial intelligence: Assessing chatgpt’s impact
[66] K. Ando, T. Okumura, M. Komachi, H. Horiguchi, and Y. Matsumoto,                    through disruptive events,” arXiv preprint arXiv:2306.00227, 2023.
     “Is artificial intelligence capable of generating hospital discharge sum-      [86] W. Ma, S. Liu, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, and
     maries from inpatient records?” PLOS Digital Health, vol. 1, no. 12,                Y. Liu, “The scope of chatgpt in software engineering: A thorough
     p. e0000158, 2022.                                                                  investigation,” arXiv preprint arXiv:2305.12138, 2023.
[67] ——, “Exploring optimal granularity for extractive summarization of             [87] S. A. Vaghefi, Q. Wang, V. Muccione, J. Ni, M. Kraus, J. Bingler,
     unstructured health records: Analysis of the largest multi-institutional            T. Schimanski, C. Colesanti-Senni, N. Webersinke, C. Huggel, and
     archive of health records in japan,” PLOS Digital Health, vol. 1, no. 9,            M. Leippold, “chatclimate: Grounding conversational ai in climate
     p. e0000099, 2022.                                                                  science,” 2023.
[68] C. Rebuffel, M. Roberti, L. Soulier, G. Scoutheeten, R. Cancelliere,           [88] X. Daull, P. Bellot, E. Bruno, V. Martin, and E. Murisasco, “Complex
     and P. Gallinari, “Controlling hallucinations at word level in data-to-             qa and language models hybrid architectures, survey,” arXiv preprint
     text generation,” Data Mining and Knowledge Discovery, pp. 1–37,                    arXiv:2302.09051, 2023.
     2022.                                                                          [89] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,
[69] D. Wan and M. Bansal, “Factpegasus: Factuality-aware pre-training                   Y. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-
     and fine-tuning for abstractive summarization,” arXiv preprint                      lucination in large language models,” arXiv preprint arXiv:2309.01219,
     arXiv:2205.07830, 2022.                                                             2023.
[70] J. G. Corbelle, A. B. Diz, J. Alonso-Moral, and J. Taboada, “Dealing           [90] O. Romanko, A. Narayan, and R. H. Kwon, “Chatgpt-based investment
     with hallucination and omission in neural natural language generation:              portfolio selection,” arXiv preprint arXiv:2308.06260, 2023.
     A use case on meteorology.” in Proceedings of the 15th International           [91] P. Henderson, T. Hashimoto, and M. Lemley, “Where’s the liability in
     Conference on Natural Language Generation, 2022, pp. 121–130.                       harmful ai speech?” arXiv preprint arXiv:2308.04635, 2023.
 [92] J. Ni, J. Bingler, C. Colesanti-Senni, M. Kraus, G. Gostlow, T. Schi-      [118]   H.-U. Hua, A.-H. Kaakour, A. Rachitskaya, S. Srivastava, S. Sharma,
      manski, D. Stammbach, S. A. Vaghefi, Q. Wang, N. Webersinke et al.,                and D. A. Mammo, “Evaluation and comparison of ophthalmic scien-
      “Chatreport: Democratizing sustainability disclosure analysis through              tific abstracts and references by current artificial intelligence chatbots,”
      llm-based tools,” arXiv preprint arXiv:2307.15770, 2023.                           JAMA ophthalmology, 2023.
 [93] R. Mahmood, G. Wang, M. Kalra, and P. Yan, “Fact-checking of ai-           [119]   D. T. Brameier, A. A. Alnasser, J. M. Carnino, A. R. Bhashyam, A. G.
      generated reports,” arXiv preprint arXiv:2307.14634, 2023.                         von Keudell, and M. J. Weaver, “Artificial intelligence in orthopaedic
 [94] A. Goyal, M. Siddique, N. Parekh, Z. Schwitzky, C. Broekaert,                      surgery: Can a large language model “write” a believable orthopaedic
      C. Michelotti, A. Wong, L. Y. Cheung, R. O. Hanlon, M. De Choudhury                journal article?” JBJS, vol. 105, no. 17, pp. 1388–1392, 2023.
      et al., “Chatgpt and bard responses to polarizing questions,” arXiv [120]          P. Lee, S. Bubeck, and J. Petro, “Benefits, limits, and risks of gpt-4 as
      preprint arXiv:2307.12402, 2023.                                                   an ai chatbot for medicine,” New England Journal of Medicine, vol.
 [95] S. Zhang, “Bridging intelligence and instinct: A new control paradigm              388, no. 13, pp. 1233–1239, 2023.
      for autonomous robots,” arXiv preprint arXiv:2307.10690, 2023.             [121]   C. Long, K. Lowe, A. dos Santos, J. Zhang, A. Alanazi,
 [96] S. Li, S. Park, I. Lee, and O. Bastani, “Trac: Trustworthy retrieval               D. O’Brien, E. Wright, and D. Cote, “Evaluating chatgpt-4
      augmented chatbot,” arXiv preprint arXiv:2307.04642, 2023.                         in otolaryngology–head and neck surgery board examination
 [97] Z. Li, S. Zhang, H. Zhao, Y. Yang, and D. Yang, “Batgpt: A bidi-                   using the cvsa model,” medRxiv, 2023. [Online]. Available:
      rectional autoregessive talker from generative pre-trained transformer,”           https://www.medrxiv.org/content/early/2023/06/01/2023.05.30.23290758
      arXiv preprint arXiv:2307.00360, 2023.                                     [122]   D. Zhang, “Should chatgpt and bard share revenue with their data
 [98] S. Curran, S. Lansley, and O. Bethell, “Hallucination is the last thing            providers? a new business model for the ai era,” arXiv preprint
      you need,” arXiv preprint arXiv:2306.11520, 2023.                                  arXiv:2305.02555, 2023.
 [99] P. Feldman, J. R. Foulds, and S. Pan, “Trapping llm hallucinations using
                                                                                 [123]   P. Puchert, P. Poonam, C. van Onzenoodt, and T. Ropinski, “Llmmaps–
      tagged context prompts,” arXiv preprint arXiv:2306.06085, 2023.
                                                                                         a visual metaphor for stratified evaluation of large language models,”
[100] A. Mukherjee and H. Chang, “The creative frontier of genera-
                                                                                         arXiv preprint arXiv:2304.00457, 2023.
      tive ai: Managing the novelty-usefulness tradeoff,” arXiv preprint
      arXiv:2306.03601, 2023.                                                    [124]   Y. Wang, S. Visweswaran, S. Kappor, S. Kooragayalu, and X. Wu,
                                                                                         “Chatgpt, enhanced with clinical practice guidelines, is a superior
[101] M. Salvagno, F. S. Taccone, and A. G. Gerli, “Artificial intelligence
                                                                                         decision support tool,” medRxiv, pp. 2023–08, 2023.
      hallucinations,” Critical Care, vol. 27, no. 1, pp. 1–2, 2023.
[102] G. Beutel, E. Geerits, and J. T. Kielstein, “Artificial hallucination: Gpt [125]   R. K. Garg, V. L. Urs, A. A. Agrawal, S. K. Chaudhary, V. Paliwal,
      on lsd?” Critical Care, vol. 27, no. 1, p. 148, 2023.                              and S. K. Kar, “Exploring the role of chat gpt in patient care (diagnosis
[103] R. Azamfirei, S. R. Kudchadkar, and J. Fackler, “Large language                    and treatment) and medical research: A systematic review,” medRxiv,
      models and the perils of their hallucinations,” Critical Care, vol. 27,            pp. 2023–06, 2023.
      no. 1, pp. 1–2, 2023.                                                      [126]   C. Han, D. W. Kim, S. Kim, S. C. You, S. Bae, and D. Yoon,
[104] J. G. Meyer, R. J. Urbanowicz, P. C. Martin, K. O’Connor, R. Li, P.-C.             “Large-language-model-based 10-year risk prediction of cardiovascular
      Peng, T. J. Bright, N. Tatonetti, K. J. Won, G. Gonzalez-Hernandez                 disease: insight from the uk biobank data,” medRxiv, pp. 2023–05,
      et al., “Chatgpt and large language models in academia: opportunities              2023.
      and challenges,” Biodata Mining, vol. 16, no. 1, p. 20, 2023.              [127]   N. T. Dolan and J. Freer, “5 things to know about generative text ai
[105] P. Hernigou and M. M. Scarlat, “Two minutes of orthopaedics with                   tools... that might be outdated or upgraded by the time of publication,”
      chatgpt: it is just the beginning; it’s going to be hot, hot, hot!”                2023.
      International Orthopaedics, pp. 1–7, 2023.                                 [128]   J. Scott-Branch, R. Laws, and P. Terzi, “The intersection of ai, infor-
[106] H.      R.    Patil.   (2023)    Don’t     be     a     victim     of   ai         mation and digital literacy: Harnessing chatgpt and other generative
      hallucinations. Accessed on Sep 10, 2023. [Online]. Available:                     tools to enhance teaching and learning,” 2023.
      https://www.accountingtoday.com/opinion/dont-be-a-victim-of-ai-hallucinations
                                                                                 [129]   I. Larsen-Ledet, “Embracing data noise,” 2023.
[107] K. Alexander, C. Savvidou, and C. Alexander, “Who wrote this essay?        [130]   J. K. Kim, M. Chua, M. Rickard, and A. Lorenzo, “Chatgpt and large
      detecting ai-generated writing in second language education in higher              language model (llm) chatbots: the current state of acceptability and a
      education.” Teaching English with Technology, vol. 23, no. 2, 2023.                proposal for guidelines on utilization in academic medicine,” Journal
[108] I. Lyell, “What history teachers need to know about chatgpt,” agora,               of Pediatric Urology, 2023.
      vol. 58, no. 2, pp. 3–7, 2023.                                             [131]   J. R. H. Tay, N. Ethan, D. Y. Chow, and C. P. C. Sim, “The use
[109] S. Grassini, “Shaping the future of education: exploring the potential             of artificial intelligence to aid in oral hygiene education: A scoping
      and consequences of ai and chatgpt in educational settings,” Education             review,” Journal of Dentistry, p. 104564, 2023.
      Sciences, vol. 13, no. 7, p. 692, 2023.                                    [132]   L. Z. Cai, A. Shaheen, A. Jin, R. Fukui, S. Y. Jonathan, N. Yannuzzi,
[110] G. L. Brodeur, G. Hall, and E. Tynch, “Chatgpt for legal and tax                   and C. Alabiad, “Performance of generative large language models on
      professionals,” The CPA Journal, vol. 93, no. 7/8, pp. 68–71, 2023.                ophthalmology board style questions,” American Journal of Ophthal-
[111] S.-W. Lee and W.-J. Choi, “Utilizing chatgpt in clinical research                  mology, 2023.
      related to anesthesiology: a comprehensive review of opportunities and     [133]   I. Jahic, M. Ebner, and S. Schön, “Harnessing the power of artificial
      limitations,” Anesthesia and Pain Medicine, vol. 18, no. 3, pp. 244–251,           intelligence and chatgpt in education–a first rapid literature review,”
      2023.                                                                              Proceedings of EdMedia+ Innovate Learning, vol. 2023, pp. 1462–
[112] T. Chatfield, “Ai hallucination,” New Philosopher, p. 76, June 2023,               1470, 2023.
      accessed 10 Sept. 2023.
                                                                                 [134]   B. Randell and B. Coghlan, “Chatgpt’s astonishing fabrications about
[113] A. McGowan, Y. Gui, M. Dobbs, S. Shuster, M. Cotter, A. Selloni,
                                                                                         percy ludgate,” IEEE Annals of the History of Computing, vol. 45,
      M. Goodman, A. Srivastava, G. A. Cecchi, and C. M. Corcoran,
                                                                                         no. 2, pp. 71–72, 2023.
      “Chatgpt and bard exhibit spontaneous citation fabrication during
      psychiatry literature search,” Psychiatry Research, vol. 326, p. 115334,   [135]   T. D. Brender, “Chatbot confabulations are not hallucinations—reply,”
      2023.                                                                              JAMA Internal Medicine, 2023.
[114] R. T. Wu and R. R. Dang, “Chatgpt in head and neck scientific writ-        [136]   I. Pedersen, “Adapting to ai writing,” 2023.
      ing: A precautionary anecdote,” American Journal of Otolaryngology,        [137]   A. Munoz, A. Wilson, B. Pereira Nunes, C. Del Medico, C. Slade,
      vol. 44, no. 6, p. 103980, 2023.                                                   D. Bennett, D. Tyler, E. Seymour, G. Hepplewhite, H. Randell-Moon
[115] H. Wang, W. Wu, Z. Dou, L. He, and L. Yang, “Performance and                       et al., “Aain generative artificial intelligence guidelines,” 2023.
      exploration of chatgpt in medical examination, records and education in    [138]   R. Hatem, B. Simmons, and J. E. Thornton, “A call to address ai
      chinese: Pave the way for medical ai,” International Journal of Medical            “hallucinations” and how healthcare professionals can mitigate their
      Informatics, vol. 177, p. 105173, 2023.                                            risks,” Cureus, vol. 15, no. 9, 2023.
[116] W. Hou and Z. Ji, “Geneturing tests gpt models in genomics,” bioRxiv,      [139]   S. A. Athaluri, S. V. Manthena, V. K. M. Kesapragada, V. Yarlagadda,
      pp. 2023–03, 2023.                                                                 T. Dave, and R. T. S. Duddumpudi, “Exploring the boundaries of real-
[117] J. Au Yeung, Z. Kraljevic, A. Luintel, A. Balston, E. Idowu, R. J.                 ity: investigating the phenomenon of artificial intelligence hallucination
      Dobson, and J. T. Teo, “Ai chatbots not yet ready for clinical use,”               in scientific writing through chatgpt references,” Cureus, vol. 15, no. 4,
      Frontiers in Digital Health, vol. 5, p. 60, 2023.                                  2023.
[140] T. Gorichanaz, “Accused: How students respond to allegations of using                portunities, and threats,” in 2023 Systems and Information Engineering
      chatgpt on assessments,” Learning: Research and Practice, pp. 1–14,                  Design Symposium (SIEDS). IEEE, 2023, pp. 274–279.
      2023.                                                                        [164]   O. H. Hamid, “Chatgpt and the chinese room argument: An eloquent
[141] S. R. Cox, A. Abdul, and W. T. Ooi, “Prompting a large language                      ai conversationalist lacking true understanding and consciousness,” in
      model to generate diverse motivational messages: A comparison with                   2023 9th International Conference on Information Technology Trends
      human-written messages,” arXiv preprint arXiv:2308.13479, 2023.                      (ITT). IEEE, 2023, pp. 238–241.
[142] O. P. Yadava, “Chatgpt—a foe or an ally?” Indian Journal of Thoracic         [165]   D. De Silva, N. Mills, M. El-Ayoubi, M. Manic, and D. Alahakoon,
      and Cardiovascular Surgery, vol. 39, no. 3, pp. 217–221, 2023.                       “Chatgpt and generative ai guidelines for addressing academic integrity
[143] Y. Dai, A. Liu, and C. P. Lim, “Reconceptualizing chatgpt and                        and augmenting pre-existing chatbots,” in 2023 IEEE International
      generative ai as a student-driven innovation in higher education,” Apr               Conference on Industrial Technology (ICIT). IEEE, 2023, pp. 1–6.
      2023. [Online]. Available: edarxiv.org/nwqju                                 [166]   S. Atallah, N. Banda, A. Banda, and N. Roeck, “How large language
[144] L. Lingard, “Writing with chatgpt: An illustration of its capacity,                  models including generative pre-trained transformer (gpt) 3 and 4 will
      limitations & implications for academic writers,” Perspectives on                    impact medicine and surgery,” Techniques in Coloproctology, pp. 1–6,
      Medical Education, vol. 12, no. 1, p. 261, 2023.                                     2023.
[145] T. Woodland, “Chatgpt for improving medical education: Proceed with          [167]   M. Byrne, “The disruptive impacts of next generation generative
      caution,” Mayo Clinic Proceedings: Digital Health, vol. 1, no. 3, pp.                artificial intelligence,” CIN: Computers, Informatics, Nursing, vol. 41,
      294–295, 2023.                                                                       no. 7, pp. 479–481, 2023.
[146] C. Kashangura, “The human-machine evo-ecosystem; the emergence               [168]   K. Beam, P. Sharma, P. Levy, and A. L. Beam, “Artificial intelligence
      of a non-life species that adapts through the algorithm and not dna or               in the neonatal intensive care unit: the time is now,” Journal of
      rna: Is our human dna ready?” 2023.                                                  Perinatology, pp. 1–5, 2023.
[147] H. L. Walker, S. Ghani, C. Kuemmerli, C. A. Nebiker, B. P. Müller,           [169]   M. Komorowski, M. del Pilar Arias López, and A. C. Chang, “How
      D. A. Raptis, and S. M. Staubli, “Reliability of medical information                 could chatgpt impact my practice as an intensivist? an overview of
      provided by chatgpt: Assessment against clinical guidelines and patient              potential applications, risks and limitations,” Intensive Care Medicine,
      information quality instrument,” Journal of Medical Internet Research,               pp. 1–4, 2023.
      vol. 25, p. e47479, 2023.                                                    [170]   M. Sallam, “Chatgpt utility in healthcare education, research, and
[148] H. Tenge Hansen and T. Røsand Valø, “Cybersecurity mindfulness in                    practice: systematic review on the promising perspectives and valid
      the age of mindless ais: Investigating ai assistants impact in high-                 concerns,” in Healthcare, vol. 11, no. 6. MDPI, 2023, p. 887.
      reliability organizations,” Master’s thesis, University of Agder, 2023.      [171]   A. J. Rothschild, “Artificial intelligence and the journal of clinical
[149] M. Tupper, I. W. Hendy, and J. R. Shipway, “Field courses for                        psychopharmacology,” pp. 397–398, 2023.
      dummies: can chatgpt design a higher education field course?” Aug            [172]   E.-O. Editor(s): Im, “Ai and nursing knowledge generation,” Advances
      2023.                                                                                in Nursing Science, August 2023.
[150] A. A. Borkowski, C. E. Jakey, S. M. Mastorides, A. L. Kraus, G. Vid-         [173]   A. Hulman, O. L. Dollerup, J. F. Mortensen, M. E. Fenech, K. Norman,
      yarthi, N. Viswanadhan, and J. L. Lezama, “Applications of chatgpt                   H. Støvring, and T. K. Hansen, “Chatgpt-versus human-generated
      and large language models in medicine and health care: Benefits and                  answers to frequently asked questions about diabetes: A turing test-
      pitfalls.” Federal Practitioner, vol. 40, no. 6, 2023.                               inspired survey among employees of a danish diabetes center,” Plos
                                                                                           one, vol. 18, no. 8, p. e0290773, 2023.
[151] R. Subramanya and P. Furlong, “Revolutionising rulemaking: How
                                                                                   [174]   B. H. H. Cheung, G. K. K. Lau, G. T. C. Wong, E. Y. P. Lee, D. Kulka-
      digitised rules can accelerate digital transformation,” 2023.
                                                                                           rni, C. S. Seow, R. Wong, and M. T.-H. Co, “Chatgpt versus human
[152] L. Y. G. Tenzer, “Defamation in the age of artificial intelligence,”
                                                                                           in generating medical graduate exam multiple choice questions—a
      Available at SSRN 4545070, 2023.
                                                                                           multinational prospective study (hong kong sar, singapore, ireland, and
[153] B. Alarie and R. McCreight, “The ethics of generative ai in tax                      the united kingdom),” PloS one, vol. 18, no. 8, p. e0290691, 2023.
      practice,” Tax Notes Federal, pp. 785–793, 2023.                             [175]   L. S. Moskatel and N. Zhang, “The utility of chatgpt in the assessment
[154] J. Han, C. Gong, W. Qiu, and E. Lichtfouse, “What does ai think of                   of literature on the prevention of migraine: an observational, qualitative
      my paper?” Available at SSRN 4525950, 2023.                                          study,” Frontiers in Neurology, vol. 14, 2023.
[155] P. G. Picht, “Chatgpt, microsoft and competition law–nemesis or fresh        [176]   M. Javid, M. Reddiboina, and M. Bhandari, “Emergence of artificial
      chance for digital markets enforcement?” Microsoft and Competition                   generative intelligence and its potential impact on urology,” The Cana-
      Law–Nemesis or Fresh Chance for Digital Markets Enforcement, 2023.                   dian Journal of Urology, vol. 30, no. 4, pp. 11 588–11 598, 2023.
[156] P. Treleaven, J. Barnett, D. Brown, A. Bud, E. Fenoglio, C. Kerrigan,        [177]   M. Buholayka, R. Zouabi, and A. Tadinada, “The readiness of chatgpt
      A. Koshiyama, S. Sfeir-Tait, and M. Schoernig. (2023, July) The future               to write scientific case reports independently: A comparative evaluation
      of cybercrime: Ai and emerging technologies are creating a cybercrime                between human and artificial intelligence,” Cureus, vol. 15, no. 5, 2023.
      tsunami. [Online]. Available: https://ssrn.com/abstract=4507244              [178]   H. Alkaissi and S. I. McFarlane, “Artificial hallucinations in chatgpt:
[157] H. Ariyaratne. (2023, May) Chatgpt and intermediary liability: Why                   implications in scientific writing,” Cureus, vol. 15, no. 2, 2023.
      section 230 does not and should not protect generative algorithms.           [179]   G. Eysenbach et al., “The role of chatgpt, generative language models,
      [Online]. Available: https://ssrn.com/abstract=4422583                               and artificial intelligence in medical education: a conversation with
[158] T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, and Y. Tang, “A                     chatgpt and a call for papers,” JMIR Medical Education, vol. 9, no. 1,
      brief overview of chatgpt: The history, status quo and potential future              p. e46885, 2023.
      development,” IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 5,         [180]   S. Thorne, “Experimenting with chatgpt for spreadsheet formula gen-
      pp. 1122–1136, 2023.                                                                 eration: Evidence of risk in ai generated spreadsheets,” arXiv preprint
[159] Y. Wang, Y. Pan, M. Yan, Z. Su, and T. H. Luan, “A survey on chat-                   arXiv:2309.00095, 2023.
      gpt: Ai-generated contents, challenges, and solutions,” arXiv preprint       [181]   Y. Huang, A. Gomaa, T. Weissmann, J. Grigo, H. B. Tkhayat, B. Frey,
      arXiv:2305.18339, 2023.                                                              U. S. Gaipl, L. V. Distel, A. Maier, R. Fietkau et al., “Benchmarking
[160] I. Amaro, P. Barra, A. Della Greca, R. Francese, and C. Tucci, “Believe              chatgpt-4 on acr radiation oncology in-training exam (txit): Potentials
      in artificial intelligence? a user study on the chatgpt’s fake information           and challenges for ai-assisted medical education and decision making
      impact,” IEEE Transactions on Computational Social Systems, 2023.                    in radiation oncology,” arXiv preprint arXiv:2304.11957, 2023.
[161] H. K. Skrodelis, A. Romanovs, N. Zenina, and H. Gorskis, “The                [182]   K. W˛ecel, M. Sawiński, M. Stróżyna, W. Lewoniewski, P. Stolarski,
      latest in natural language generation: Trends, tools and applications                E. Ksi˛eżniak, and W. Abramowicz, “Artificial intelligence - friend or
      in industry,” in 2023 IEEE 10th Jubilee Workshop on Advances in                      foe in fake news campaigns,” Economics and Business Review, vol. 9,
      Information, Electronic and Electrical Engineering (AIEEE). IEEE,                    no. 2, pp. 41–70, 2023.
      2023, pp. 1–5.                                                               [183]   A. M. Bhatti, “Ai in medical research – chatgpt and beyond,” Pakistan
[162] M. Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj, “From chatgpt              Armed Forces Medical Journal, vol. 73, no. 4, pp. 954–954, 2023.
      to threatgpt: Impact of generative ai in cybersecurity and privacy,” IEEE    [184]   M. Mahyoob, J. Al-Garaady, and A. Alblwi, “A proposed framework
      Access, 2023.                                                                        for human-like language processing of chatgpt in academic writing,”
[163] A. Bahrini, M. Khamoshifar, H. Abbasimehr, R. J. Riggs, M. Esmaeili,                 International Journal of Emerging Technologies in Learning (iJET),
      R. M. Majdabadkohne, and M. Pasehvar, “Chatgpt: Applications, op-                    vol. 18, no. 14, 2023.
[185] M. Lee, “A mathematical investigation of hallucination and creativity      [208] R. J. Lyons, S. R. Arepalli, O. Fromal, J. D. Choi, and N. Jain,
      in gpt models,” Mathematics, vol. 11, no. 10, p. 2320, 2023.                     “Artificial intelligence chatbot performance in triage of ophthalmic
[186] A. Piñeiro-Martín, C. García-Mateo, L. Docío-Fernández, and M. d. C.             conditions,” Canadian Journal of Ophthalmology, 2023.
      López-Pérez, “Ethical challenges in the development of virtual assis-      [209] A. L. Opdahl, B. Tessem, D.-T. Dang-Nguyen, E. Motta, V. Setty,
      tants powered by large language models,” Electronics, vol. 12, no. 14,           E. Throndsen, A. Tverberg, and C. Trattner, “Trustworthy journalism
      p. 3170, 2023.                                                                   through ai,” Data & Knowledge Engineering, vol. 146, p. 102182,
[187] H. M. Alhaidry, B. Fatani, J. O. Alrayes, A. M. Almana, N. K. Alfhaed,           2023.
      H. Alhaidry, J. Alrayes, A. Almana, and N. K. Alfhaed Sr, “Chatgpt         [210] F. Blanchard, M. Assefi, N. Gatulle, and J.-M. Constantin, “Chatgpt
      in dentistry: A comprehensive review,” Cureus, vol. 15, no. 4, 2023.             in the world of medical research: From how it works to how to use
[188] S. Khurana and A. Vaddi, “Chatgpt from the perspective of an academic            it.” Anaesthesia, Critical Care & Pain Medicine, pp. 101 231–101 231,
      oral and maxillofacial radiologist,” Cureus, vol. 15, no. 6, 2023.               2023.
[189] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, “Chatdoctor:       [211] Z. W. Lim, K. Pushpanathan, S. M. E. Yew, Y. Lai, C.-H. Sun, J. S. H.
      A medical chat model fine-tuned on a large language model meta-ai                Lam, D. Z. Chen, J. H. L. Goh, M. C. J. Tan, B. Sheng et al.,
      (llama) using medical domain knowledge,” Cureus, vol. 15, no. 6, 2023.           “Benchmarking large language models’ performances for myopia care:
[190] K. Walczak and W. Cellary, “Challenges for higher education in the               a comparative analysis of chatgpt-3.5, chatgpt-4.0, and google bard,”
      era of widespread access to generative ai,” Economics and Business               Ebiomedicine, vol. 95, 2023.
      Review, vol. 9, no. 2, pp. 71–100, 2023.                                   [212] T.-H. Kim, J. W. Kang, and M. S. Lee, “Ai chat bot-chatgpt-4: A new
[191] R. Marinaccio, A. M. Clark, and J. O’Connor, “Harnessing the                     opportunity and challenges in complementary and alternative medicine
      power of ai: Legal considerations for employers,” Rochester Business             (cam),” Integrative Medicine Research, vol. 12, no. 3, p. 100977, 2023.
      Journal, vol. 39, no. 10, p. 12, Aug 04 2023. [Online]. Available:         [213] T. Alqahtani, H. A. Badreldin, M. Alrashed, A. I. Alshaya, S. S. Al-
      https://www.proquest.com/trade-journals/harnessing-power-ai-legal-considerations/docview/2856491812/se-2
                                                                                       ghamdi, K. bin Saleh, S. A. Alowais, O. A. Alshaya, I. Rahman, M. S.
[192] G. Gebrael, K. K. Sahu, B. Chigarira, N. Tripathi, V. Mathew Thomas,             Al Yami et al., “The emergent role of artificial intelligence, natural
      N. Sayegh, B. L. Maughan, N. Agarwal, U. Swami, and H. Li, “En-                  learning processing, and large language models in higher education
      hancing triage efficiency and accuracy in emergency rooms for patients           and research,” Research in Social and Administrative Pharmacy, 2023.
      with metastatic prostate cancer: a retrospective analysis of artificial
                                                                                 [214] A. A. Jairoun, S. S. Al-Hemyari, M. Shahwan, G. R. H. Alnuaimi,
      intelligence-assisted triage using chatgpt 4.0,” Cancers, vol. 15, no. 14,
                                                                                       H. Z. Sa’ed, and M. Jairoun, “Chatgpt: Threat or boon to the future of
      p. 3717, 2023.
                                                                                       pharmacy practice?” pp. S1551–7411, 2023.
[193] A. Tiwari, A. Kumar, S. Jain, K. S. Dhull, A. Sajjanar, R. Puthenkan-
      dathil, K. Paiwal, R. Singh, and A. Sajjanar, “Implications of chatgpt     [215] J. Šlapeta, “Are chatgpt and other pretrained language models good
      in public health dentistry: A systematic review,” Cureus, vol. 15, no. 6,        parasitologists?” Trends in Parasitology, 2023.
      2023.                                                                      [216] D. Dillion, N. Tandon, Y. Gu, and K. Gray, “Can ai language models
[194] M. Bhattacharyya, V. M. Miller, D. Bhattacharyya, L. E. Miller,                  replace human participants?” Trends in Cognitive Sciences, 2023.
      and V. Miller, “High rates of fabricated and inaccurate references in      [217] C. Thomson, E. Reiter, and B. Sundararajan, “Evaluating factual
      chatgpt-generated medical content,” Cureus, vol. 15, no. 5, 2023.                accuracy in complex data-to-text,” Computer Speech & Language,
[195] J. Dossantos, J. An, and R. Javan, “Eyes on ai: Chatgpt’s transformative         vol. 80, p. 101482, 2023.
      potential impact on ophthalmology,” Cureus, vol. 15, no. 6, 2023.          [218] M. Balas and E. B. Ing, “Conversational ai models for ophthalmic diag-
[196] E. Loos, J. Gröpler, and M.-L. S. Goudeau, “Using chatgpt in educa-              nosis: Comparison of chatgpt and the isabel pro differential diagnosis
      tion: Human reflection on chatgpt’s self-reflection,” Societies, vol. 13,        generator,” JFO Open Ophthalmology, vol. 1, p. 100005, 2023.
      no. 8, p. 196, 2023.                                                       [219] M. Salah, H. Al Halbusi, and F. Abdelfattah, “May the force of text
[197] S. Koga, “The integration of large language models such as chatgpt in            data analysis be with you: Unleashing the power of generative ai for
      scientific writing: Harnessing potential and addressing pitfalls,” Korean        social psychology research,” Computers in Human Behavior: Artificial
      Journal of Radiology, vol. 24, no. 9, p. 924, 2023.                              Humans, p. 100006, 2023.
[198] M. Birenbaum, “The chatbots’ challenge to education: Disruption or         [220] J. Ilicki, “A framework for critically assessing chatgpt and other large
      destruction?” Education Sciences, vol. 13, no. 7, p. 711, 2023.                  language artificial intelligence model applications in health care,” Mayo
[199] M. Cascella, J. Montomoli, V. Bellini, and E. Bignami, “Evaluating the           Clinic Proceedings: Digital Health, vol. 1, no. 2, pp. 185–188, 2023.
      feasibility of chatgpt in healthcare: an analysis of multiple clinical and [221] B. J. Jansen, S.-g. Jung, and J. Salminen, “Employing large language
      research scenarios,” Journal of Medical Systems, vol. 47, no. 1, p. 33,          models in survey research,” Natural Language Processing Journal,
      2023.                                                                            vol. 4, p. 100020, 2023.
[200] J. K. Aronson, “When i use a word... chatgpt: a differential diagnosis,”   [222] J. E. Casal and M. Kessler, “Can linguists distinguish between chat-
      2023.                                                                            gpt/ai and human writing?: A study of research ethics and academic
[201] M. Kumar, U. A. Mani, P. Tripathi, M. Saalim, S. Roy, and S. Roy Sr,             publishing,” Research Methods in Applied Linguistics, vol. 2, no. 3, p.
      “Artificial hallucinations by google bard: Think before you leap,”               100068, 2023.
      Cureus, vol. 15, no. 8, 2023.                                              [223] X. Qi, Z. Zhu, and B. Wu, “The promise and peril of chatgpt in geriatric
[202] J. M. L. Ferres, W. B. Weeks, L. C. Chu, S. P. Rowe, and E. K. Fishman,          nursing education: What we know and do not know,” p. 100136, 2023.
      “Beyond chatting: The opportunities and challenges of chatgpt in           [224] Z. Lin, “Why and how to embrace ai such as chatgpt in your
      medicine and radiology,” Diagnostic and Interventional Imaging, pp.              academic life,” Feb 2023. [Online]. Available: psyarxiv.com/sdx3j
      S2211–5684, 2023.
                                                                                 [225] B. V. Janssen, G. Kazemier, and M. G. Besselink, “The use of chatgpt
[203] J. Varghese and J. Chapiro, “Chatgpt: The transformative influence of
                                                                                       and other large language models in surgical science,” p. zrad032, 2023.
      generative ai on science and healthcare,” Journal of Hepatology, 2023.
[204] P. Dunn and D. Cianflone, “Artificial intelligence in cardiology: Ex-      [226] Y. Shen, L. Heacock, J. Elias, K. D. Hentel, B. Reig, G. Shih, and
      citing but handle with caution,” International Journal of Cardiology,            L. Moy, “Chatgpt and other large language models are double-edged
      2023.                                                                            swords,” p. e230163, 2023.
[205] A. A. Theodosiou and R. C. Read, “Artificial intelligence, machine         [227] A. J. Thirunavukarasu, R. Hassan, S. Mahmood, R. Sanghera,
      learning and deep learning: Potential resources for the infection clini-         K. Barzangi, M. El Mukashfi, and S. Shah, “Trialling a large language
      cian,” Journal of Infection, 2023.                                               model (chatgpt) in general practice with the applied knowledge test:
[206] D.      R.      Fesenmaier       and    K.      Wöber,      “Ai,     chat-       observational study demonstrating opportunities and limitations in
      gpt      and     the     university,”   Annals     of     Tourism      Re-       primary care,” JMIR Medical Education, vol. 9, no. 1, p. e46599, 2023.
      search, vol. 101, p. 103578, 2023. [Online]. Available:                    [228] J. Qadir, “Engineering education in the era of chatgpt: Promise and pit-
      https://www.sciencedirect.com/science/article/pii/S0160738323000518              falls of generative ai for education,” in 2023 IEEE Global Engineering
[207] S. S. Sohail, F. Farhat, Y. Himeur, M. Nadeem, D. Ø. Madsen, Y. Singh,           Education Conference (EDUCON). IEEE, 2023, pp. 1–9.
      S. Atalla, and W. Mansoor, “Decoding chatgpt: A taxonomy of existing       [229] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz,
      research, current challenges, and possible future directions,” Journal of        P. C. Petersen, A. Chevalier, and J. Berner, “Mathematical capabilities
      King Saud University-Computer and Information Sciences, p. 101675,               of chatgpt,” arXiv preprint arXiv:2301.13867, 2023.
      2023.                                                                      [230] OpenAI, “Gpt-4 technical report,” 2023.
[231] P. Manakul, A. Liusie, and M. J. Gales, “Selfcheckgpt: Zero-resource          [254] E. Murgia, Z. Abbasiantaeb, M. Aliannejadi, T. Huibers, M. Landoni,
      black-box hallucination detection for generative large language mod-                and M. S. Pera, “Chatgpt in the classroom: A preliminary exploration
      els,” arXiv preprint arXiv:2303.08896, 2023.                                        on the feasibility of adapting chatgpt to support children’s information
[232] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,                      discovery,” in Adjunct Proceedings of the 31st ACM Conference on
      “Capabilities of gpt-4 on medical challenge problems,” arXiv preprint               User Modeling, Adaptation and Personalization, 2023, pp. 22–27.
      arXiv:2303.13375, 2023.                                                       [255] K. N. Kunze, S. J. Jang, M. A. Fullerton, J. M. Vigdorchik, and F. S.
[233] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:                   Haddad, “What’s all the chatter about?: current applications and ethical
      A large-scale hallucination evaluation benchmark for large language                 considerations of artificial intelligence language models,” The Bone &
      models,” arXiv e-prints, pp. arXiv–2305, 2023.                                      Joint Journal, vol. 105, no. 6, pp. 587–589, 2023.
[234] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,                 [256] M. Cascella, J. Montomoli, V. Bellini, A. Ottaiano, M. Santorsola,
      B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language                     F. Perri, F. Sabbatino, A. Vittori, and E. G. Bignami, “Writing the paper
      models,” arXiv preprint arXiv:2303.18223, 2023.                                     “unveiling artificial intelligence: an insight into ethics and applications
[235] V. Adlakha, P. BehnamGhader, X. H. Lu, N. Meade, and S. Reddy,                      in anesthesia” implementing the large language model chatgpt: a
      “Evaluating correctness and faithfulness of instruction-following mod-              qualitative study,” Journal of Medical Artificial Intelligence, vol. 6,
      els for question answering,” arXiv preprint arXiv:2307.16877, 2023.                 2023.
[236] A. Athavale, J. Baier, E. Ross, and E. Fukaya, “The potential of              [257] A. Watanabe, “Exploring totalitarian elements of artificial intelligence
      chatbots in chronic venous disease patient management,” JVS-Vascular                in higher education with hannah arendt,” International Journal of
      Insights, p. 100019, 2023.                                                          Technoethics (IJT), vol. 14, no. 1, pp. 1–15, 2023.
[237] T. C. Chen, E. Kaminski, L. Koduri, A. Singer, J. Singer, M. Couldwell,       [258] T. Nakaura, N. Yoshida, N. Kobayashi, K. Shiraishi, Y. Nagayama,
      J. Delashaw, A. Dumont, and A. Wang, “Chat gpt as a neuro-score                     H. Uetani, M. Kidoh, M. Hokamura, Y. Funama, and T. Hirai, “Pre-
      calculator: Analysis of a large language model’s performance on                     liminary assessment of automated radiology report generation with
      various neurological exam grading scales,” World Neurosurgery, 2023.                generative pre-trained transformers: comparing results to radiologist-
[238] B. C. Stahl and D. Eke, “The ethics of chatgpt–exploring the ethical                generated reports,” Japanese Journal of Radiology, pp. 1–11, 2023.
      issues of an emerging technology,” International Journal of Information       [259] S. Feuerriegel, J. Hartmann, C. Janiesch, and et al., “Generative ai,”
      Management, vol. 74, p. 102700, 2024.                                               Business & Information Systems Engineering, 2023.
[239] W. H. Walters and E. I. Wilder, “Fabrication and errors in the bibli-         [260] B. N. Hryciw, Z. Fortin, J. Ghossein, and K. Kyeremanteng, “Doctor-
      ographic citations generated by chatgpt,” Scientific Reports, vol. 13,              patient interactions in the age of ai: navigating innovation and exper-
      no. 1, p. 14045, 2023.                                                              tise,” Frontiers in Medicine, vol. 10, 2023.
[240] C. L. Munro and A. A. Hope, “Artificial intelligence in critical care         [261] E. Bran, C. Rughiniş, G. Nadoleanu, and M. G. Flaherty, “The
      practice and research,” American Journal of Critical Care, vol. 32,                 emerging social status of generative ai: Vocabularies of ai competence
      no. 5, pp. 321–323, 2023.                                                           in public discourse,” in 2023 24th International Conference on Control
[241] D. A. Hashimoto and K. B. Johnson, “The use of artificial intelligence              Systems and Computer Science (CSCS). IEEE, 2023, pp. 391–398.
      tools to prepare medical school applications,” Academic Medicine,             [262] F. Sovrano, K. Ashley, and A. Bacchelli, “Toward eliminating hal-
      vol. 98, no. 9, pp. 978–982, 2023.                                                  lucinations: Gpt-based explanatory ai for intelligent textbooks and
[242] A. Jain, “Chatgpt for scientific community: Boon or bane?” Medical                  documentation,” 2023.
      journal, Armed Forces India, vol. 79, no. 5, pp. 498–499, 2023.               [263] P. Mishra, M. Warr, and R. Islam, “Tpack in the age of chatgpt and
[243] W. L. J. Ho, B. Koussayer, and J. Sujka, “Chatgpt: Friend or foe in                 generative ai,” Journal of Digital Learning in Teacher Education, pp.
      medical writing? an example of how chatgpt can be utilized in writing               1–17, 2023.
      case reports,” Surgery in Practice and Science, p. 100185, 2023.              [264] K.-Y. Lam, V. C. Cheng, and Z. K. Yeong, “Applying large language
[244] G. M. Currie, “Academic integrity and artificial intelligence: is chatgpt           models for enhancing contract drafting,” 2023.
      hype, hero or heresy?” in Seminars in Nuclear Medicine. Elsevier,             [265] D. Wan, M. Liu, K. McKeown, M. Dreyer, and M. Bansal,
      2023.                                                                               “Faithfulness-aware decoding strategies for abstractive summarization,”
[245] X. Liu, H. Lai, H. Yu, Y. Xu, A. Zeng, Z. Du, P. Zhang, Y. Dong, and                arXiv preprint arXiv:2303.03278, 2023.
      J. Tang, “Webglm: Towards an efficient web-enhanced question answer-          [266] A. B. Houston and E. M. Corrado, “Embracing chatgpt: Implications
      ing system with human preferences,” arXiv preprint arXiv:2306.07906,                of emergent language models for academia and libraries,” Technical
      2023.                                                                               Services Quarterly, vol. 40, no. 2, pp. 76–91, 2023.
[246] M. Campbell and M. Jovanović, “Detecting artificial intelligence: A          [267] C. González-Mora, C. Barros, I. Garrigós, J. Zubcoff, E. Lloret, and
      new cyberarms race begins,” Computer, vol. 56, no. 8, pp. 100–105,                  J.-N. Mazón, “Improving open data web api documentation through
      2023.                                                                               interactivity and natural language generation,” Computer Standards &
[247] N. Kshetri, “The economics of generative artificial intelligence in the             Interfaces, vol. 83, p. 103657, 2023.
      academic industry,” Computer, vol. 56, no. 8, pp. 77–83, 2023.                [268] J. Lim, M. Kang, Y. Hur, S. Jung, J. Kim, Y. Jang, D. Lee, H. Ji,
[248] M. R. Ali, C. A. Lawson, A. M. Wood, and K. Khunti, “Addressing                     D. Shin, S. Kim et al., “You truly understand what i need: Intellectual
      ethnic and global health inequalities in the era of artificial intelligence         and friendly dialogue agents grounding knowledge and persona,” arXiv
      healthcare models: a call for responsible implementation,” Journal of               preprint arXiv:2301.02401, 2023.
      the Royal Society of Medicine, vol. 116, no. 8, pp. 260–262, 2023.            [269] S. A. Alowais, S. S. Alghamdi, N. Alsuhebany, T. Alqahtani, A. I.
[249] J. Q. Tay, “Chatgpt and the future of plastic surgery research: evolu-              Alshaya, S. N. Almohareb, A. Aldairem, M. Alrashed, K. Bin Saleh,
      tionary tool or revolutionary force in academic publishing?” European               H. A. Badreldin et al., “Revolutionizing healthcare: the role of artificial
      Journal of Plastic Surgery, pp. 1–2, 2023.                                          intelligence in clinical practice,” BMC Medical Education, vol. 23,
[250] Z. Xu and D. Cohen, “A lightweight constrained generation alternative               no. 1, pp. 1–15, 2023.
      for query-focused summarization,” arXiv preprint arXiv:2304.11721,            [270] S. Liu, A. P. Wright, B. L. Patterson, J. P. Wanderer, R. W. Turer, S. D.
      2023.                                                                               Nelson, A. B. McCoy, D. F. Sittig, and A. Wright, “Using ai-generated
[251] S. Kernan Freire, M. Foosherian, C. Wang, and E. Niforatos, “Har-                   suggestions from chatgpt to optimize clinical decision support,” Journal
      nessing large language models for cognitive assistants in factories,”               of the American Medical Informatics Association, vol. 30, no. 7, pp.
      in Proceedings of the 5th International Conference on Conversational                1237–1245, 2023.
      User Interfaces, 2023, pp. 1–6.                                               [271] Q. Xie, E. J. Schenck, H. S. Yang, Y. Chen, Y. Peng, and F. Wang,
[252] M.-L. Tsai, C. W. Ong, and C.-L. Chen, “Exploring the use of large                  “Faithful ai in medicine: A systematic review with large language
      language models (llms) in chemical engineering education: Building                  models and beyond,” Medrxiv: the Preprint Server for Health Sciences,
      core course problem models with chat-gpt,” Education for Chemical                   2023.
      Engineers, vol. 44, pp. 71–95, 2023.                                          [272] I. A. Bernstein, Y. V. Zhang, D. Govil, I. Majid, R. T. Chang,
[253] C. Xin, H. Lin, S. Wu, X. Han, B. Chen, W. Dai, S. Chen, B. Wang,                   Y. Sun, A. Shue, J. C. Chou, E. Schehlein, K. L. Christopher et al.,
      and L. Sun, “Dialogue rewriting via skeleton-guided generation,” in                 “Comparison of ophthalmologist and large language model chatbot
      Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37,             responses to online patient eye care questions,” JAMA Network Open,
      no. 11, 2023, pp. 13 825–13 833.                                                    vol. 6, no. 8, pp. e2 330 320–e2 330 320, 2023.
[273] T. G. Heck, “What artificial intelligence knows about 70 kda heat             [296] N. Boujemaa, A. Hassan, G. Kokaia, and P. K. Sinha, “How responsible
      shock proteins, and how we will face this chatgpt era,” Cell Stress                 llms are beneficial to search and exploration in retail industry,” in
      and Chaperones, vol. 28, no. 3, pp. 225–229, 2023.                                  Proceedings of the 2023 ACM International Conference on Multimedia
[274] P. P. Ray and P. Majumder, “The potential of chatgpt to transform                   Retrieval, 2023, pp. 669–669.
      healthcare and address ethical challenges in artificial intelligence-driven   [297] D. Li, T. Chen, A. Zadikian, A. Tung, and L. B. Chilton, “Improving
      medicine,” Journal of Clinical Neurology (Seoul, Korea), vol. 19, no. 5,            automatic summarization for browsing longform spoken dialog,” in
      p. 509, 2023.                                                                       Proceedings of the 2023 CHI Conference on Human Factors in
[275] T. L. Ang, M. Choolani, K. C. See, and K. K. Poh, “The rise of artificial           Computing Systems, 2023, pp. 1–20.
      intelligence: addressing the impact of large language models such as          [298] M. Urban, D. D. Nguyen, and C. Binnig, “Omniscientdb: A large
      chatgpt on scientific publications,” Singapore Medical Journal, vol. 64,            language model-augmented dbms that knows what other dbmss do
      no. 4, p. 219, 2023.                                                                not know,” in Proceedings of the Sixth International Workshop on
[276] A. Talyshinskii, N. Naik, B. Z. Hameed, U. Zhanbyrbekuly, G. Khairli,               Exploiting Artificial Intelligence Techniques for Data Management,
      B. Guliev, P. Juilebø-Jones, L. Tzelves, and B. K. Somani, “Expanding               2023, pp. 1–7.
      horizons and navigating challenges for enhanced clinical workflows:           [299] J. Mahon, B. Mac Namee, and B. A. Becker, “No more pencils
      Chatgpt in urology,” Frontiers in Surgery, vol. 10, 2023.                           no more books: Capabilities of generative ai on irish and uk
[277] J. E. Kung, C. Marshall, C. Gauthier, T. A. Gonzalez, and J. B.                     computer science school leaving examinations,” in Proceedings of
      Jackson III, “Evaluating chatgpt performance on the orthopaedic in-                 the 2023 Conference on United Kingdom & Ireland Computing
      training examination,” JBJS Open Access, vol. 8, no. 3, p. e23, 2023.               Education Research, ser. UKICER ’23. New York, NY, USA:
[278] H. Friederichs, W. J. Friederichs, and M. März, “Chatgpt in medical                 Association for Computing Machinery, 2023. [Online]. Available:
      school: how successful is ai in progress testing?” Medical Education                https://doi.org/10.1145/3610969.3610982
      Online, vol. 28, no. 1, p. 2220920, 2023.                                     [300] T. August, L. L. Wang, J. Bragg, M. A. Hearst, A. Head, and K. Lo,
[279] N. Ghorashi, A. Ismail, P. Ghosh, A. Sidawy, R. Javan, and N. S.                    “Paper plain: Making medical research papers approachable to health-
      Ghorashi, “Ai-powered chatbots in medical education: Potential appli-               care consumers with natural language processing,” ACM Transactions
      cations and implications,” Cureus, vol. 15, no. 8, 2023.                            on Computer-Human Interaction, 2022.
[280] J. C. McBee, D. Y. Han, L. Liu, L. Ma, D. A. Adjeroh, D. Xu, and              [301] J. E. Fischer, “Generative ai considered harmful,” 2023.
      G. Hu, “Interdisciplinary inquiry via panelgpt: Application to explore        [302] Y. Lee, T. S. Kim, S. Kim, Y. Yun, and J. Kim, “Dapie: Interactive
      chatbot application in sports rehabilitation,” medRxiv, pp. 2023–07,                step-by-step explanatory dialogues to answer children’s why and how
      2023.                                                                               questions,” in Proceedings of the 2023 CHI Conference on Human
[281] L. O. Reis, “Chatgpt for medical applications and urological science,”              Factors in Computing Systems, 2023, pp. 1–22.
      International braz j urol, vol. 49, pp. 652–656, 2023.                        [303] V. Scotti, L. Sbattella, and R. Tedesco, “A primer on seq2seq models
[282] M. P. Joachimiak, J. H. Caufield, N. Harris, and C. J. Mungall,                     for generative chatbots,” ACM Computing Surveys, 2023.
      “Gene set summarization using large language models,” arXiv preprint
                                                                                    [304] X. Zhan, Y. Xu, and S. Sarkadi, “Deceptive ai ecosystems: The case
      arXiv:2305.13338, 2023.
                                                                                          of chatgpt,” arXiv preprint arXiv:2306.13671, 2023.
[283] A. R. Chen, “Research training in an ai world,” 2023.
                                                                                    [305] A. R. Vargas-Murillo, I. N. M. d. l. A. Pari-Bedoya, and F. d. J.
[284] M. Delsoz, Y. Madadi, W. M. Munir, B. Tamm, S. Mehravaran,
                                                                                          Guevara-Soto, “The ethics of ai assisted learning: A systematic lit-
      M. Soleimani, A. Djalilian, and S. Yousefi, “Performance of chatgpt
                                                                                          erature review on the impacts of chatgpt usage in education,” in
      in diagnosis of corneal eye diseases,” medRxiv, 2023.
                                                                                          Proceedings of the 2023 8th International Conference on Distance
[285] S. Takagi, T. Watari, A. Erabi, K. Sakaguchi et al., “Performance                   Education and Learning, 2023, pp. 8–13.
      of gpt-3.5 and gpt-4 on the japanese medical licensing examination:
      comparison study,” JMIR Medical Education, vol. 9, no. 1, p. e48002,          [306] R. White, T. Peng, P. Sripitak, A. Rosenberg Johansen, and M. Snyder,
      2023.                                                                               “Clinidigest: A case study in large language model based large-scale
                                                                                          summarization of clinical trial descriptions,” in Proceedings of the 2023
[286] Y. Kaneda, R. Takahashi, U. Kaneda, S. Akashima, H. Okita, S. Misaki,
                                                                                          ACM Conference on Information Technology for Social Good, 2023,
      A. Yamashiro, A. Ozaki, and T. Tanimoto, “Assessing the performance
                                                                                          pp. 396–402.
      of gpt-3.5 and gpt-4 on the 2023 japanese nursing examination,”
      Cureus, vol. 15, no. 8, 2023.                                                 [307] M. A. Cusumano, “Generative ai as a new innovation platform,”
[287] S. Ahn, “The impending impacts of large language models on medical                  Communications of the ACM, vol. 66, no. 10, pp. 18–21, 2023.
      education,” Korean Journal of Medical Education, vol. 35, no. 1, p.           [308] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt
      103, 2023.                                                                          selection for code-related few-shot learning,” in Proceedings of the 45th
[288] Q. Jin, Z. Wang, C. S. Floudas, J. Sun, and Z. Lu, “Matching                        International Conference on Software Engineering (ICSE’23), 2023.
      patients to clinical trials with large language models,” arXiv preprint       [309] P. Pataranutaporn, V. Danry, L. Blanchard, L. Thakral, N. Ohsugi,
      arXiv:2307.15051, 2023.                                                             P. Maes, and M. Sra, “Living memories: Ai-generated characters as
[289] S. Ahn, “A use case of chatgpt in a flipped medical terminology                     digital mementos,” in Proceedings of the 28th International Conference
      course,” Korean Journal of Medical Education, vol. 35, no. 3, p. 303,               on Intelligent User Interfaces, 2023, pp. 889–901.
      2023.                                                                         [310] G. Faggioli, L. Dietz, C. L. Clarke, G. Demartini, M. Hagen, C. Hauff,
[290] J. Liu, C. Wang, and S. Liu, “Utility of chatgpt in clinical practice,”             N. Kando, E. Kanoulas, M. Potthast, B. Stein et al., “Perspectives on
      Journal of Medical Internet Research, vol. 25, p. e48568, 2023.                     large language models for relevance judgment,” in Proceedings of the
[291] N. Doorn, “On the use of large language models in the water domain:                 2023 ACM SIGIR International Conference on Theory of Information
      Navigating the scylla of naïve techno-optimism and the charybdis of                 Retrieval, 2023, pp. 39–50.
      technology denial,” The Science of the Total Environment, 2023.               [311] A. Marczak-Czajka and J. Cleland-Huang, “Using chatgpt to generate
[292] W. Hiesinger, C. Zakka, A. Chaurasia, R. Shad, A. Dalal, J. Kim,                    human-value user stories as inspirational triggers,” in 2023 IEEE
      M. Moor, K. Alexander, E. Ashley, J. Boyd et al., “Almanac: Retrieval-              31st International Requirements Engineering Conference Workshops
      augmented language models for clinical medicine,” 2023.                             (REW), 2023, pp. 52–61.
[293] D. Palal, S. Ghonge, V. Jadav, and H. Rathod, “Chatgpt:                       [312] A. Stefńnska, T. P. Stefański, and M. Czubenko, “Evaluation of chatgpt
      A double-edged sword?” Health Services Insights, vol. 16, p.                        applicability to learning quantum physics,” in 2023 16th International
      11786329231174338, 2023.                                                            Conference on Signal Processing and Communication System (IC-
[294] B. N. Coskun, B. Yagiz, G. Ocakoglu, E. Dalkilic, and Y. Pehlivan,                  SPCS), 2023, pp. 1–10.
      “Assessing the accuracy and completeness of artificial intelligence           [313] C. P. Ezenkwu, “Towards expert systems for improved customer
      language models in providing information on methotrexate use,”                      services using chatgpt as an inference engine,” in 2023 International
      Rheumatology International, pp. 1–7, 2023.                                          Conference on Digital Applications, Transformation & Economy (IC-
[295] M. T. Baldassarre, D. Caivano, B. Fernandez Nieto, D. Gigante, and                  DATE), 2023, pp. 1–5.
      A. Ragone, “The social impact of generative ai: An analysis on                [314] U. M. Fayyad, “From stochastic parrots to intelligent assistants—the
      chatgpt,” in Proceedings of the 2023 ACM Conference on Information                  secrets of data and human interventions,” IEEE Intelligent Systems,
      Technology for Social Good, 2023, pp. 363–373.                                      vol. 38, no. 3, pp. 63–67, 2023.
[315] J. Mrabet and R. Studholme, “Chatgpt: A friend or a foe?” in 2023 In-       [338] P. Piazza, E. Checcucci, S. Puliatti, I. R. Belenchòn, A. Veccia, J. G.
      ternational Conference on Computational Intelligence and Knowledge                Rivas, M. Taratkin, K.-F. Kowalewski, S. Rodler, and G. E. Cacciamani,
      Economy (ICCIKE). IEEE, 2023, pp. 269–274.                                        “The long but necessary journey towards optimization of the cause-
[316] J. Pitt, “Chatsh* t and other conversations (that we should be having,            effect relationship between input and output for accountable use of
      but mostly are not),” IEEE Technology and Society Magazine, vol. 42,              chatgpt for academic purposes,” European Urology Focus, 2023.
      no. 3, pp. 7–13, 2023.                                                      [339] M. Scanlon, B. Nikkel, and Z. Geradts, “Digital forensic investigation
[317] P. Crosthwaite and V. Baisa, “Generative ai and the end of corpus-                in the age of chatgpt,” Forensic Science International: Digital Investi-
      assisted data-driven learning? not so fast!” Applied Corpus Linguistics,          gation, vol. 44, 2023.
      vol. 3, no. 3, p. 100066, 2023.                                             [340] Y. Kaneda, “Chatgpt in infectious diseases: A practical evaluation and
[318] P. W. Vinny, “Invoking ai for diagnosis: Art at the cutting edge of               future considerations,” New Microbes and New Infections, vol. 54,
      science,” Journal of the Neurological Sciences, 2023.                             2023.
[319] A. Solyman, M. Zappatore, W. Zhenyu, Z. Mahmoud, A. Alfatemi,               [341] T. F. Tan, A. J. Thirunavukarasu, J. P. Campbell, P. A. Keane,
      A. O. Ibrahim, and L. A. Gabralla, “Optimizing the impact of data                 L. R. Pasquale, M. D. Abramoff, J. Kalpathy-Cramer, F. Lum, J. E.
      augmentation for low-resource grammatical error correction,” Journal              Kim, S. L. Baxter et al., “Generative artificial intelligence through
      of King Saud University-Computer and Information Sciences, vol. 35,               chatgpt and other large language models in ophthalmology: Clinical
      no. 6, p. 101572, 2023.                                                           applications and challenges,” Ophthalmology Science, p. 100394, 2023.
[320] A. Waqas, M. M. Bui, E. F. Glassy, I. El Naqa, P. Borkowski, A. A.          [342] Q. Ai, T. Bai, Z. Cao, Y. Chang, J. Chen, Z. Chen, Z. Cheng, S. Dong,
      Borkowski, and G. Rasool, “Revolutionizing digital pathology with                 Z. Dou, F. Feng et al., “Information retrieval meets large language
      the power of generative artificial intelligence and foundation models,”           models: A strategic report from chinese ir community,” AI Open, vol. 4,
      Laboratory Investigation, p. 100255, 2023.                                        pp. 80–90, 2023.
[321] L. D. Stephens, J. W. Jacobs, B. D. Adkins, and G. S. Booth, “Battle        [343] J. F. Ruma, T. T. Mayeesha, and R. M. Rahman, “Transformer based
      of the (chat) bots: comparing large language models to practice guide-            answer-aware bengali question generation,” International Journal of
      lines for transfusion-associated graft-versus-host disease prevention,”           Cognitive Computing in Engineering, 2023.
      Transfusion Medicine Reviews, p. 150753, 2023.                              [344] R. Mao, K. He, X. Zhang, G. Chen, J. Ni, Z. Yang, and E. Cambria,
[322] J. Dien, “Generative artificial intelligence as a plagiarism problem,” p.         “A survey on semantic processing techniques,” Information Fusion, p.
      108621, 2023.                                                                     101988, 2023.
[323] R. Abu-Farha, L. Fino, F. Y. Al-Ashwal, M. Zawiah, L. Gharaibeh,            [345] B. P. Chaiken, “Unleashing precision medicine to deliver personalized
      M. H. Mea’ad, and F. D. Elhajji, “Evaluation of community phar-                   care,” 2023.
      macists’ perceptions and willingness to integrate chatgpt into their        [346] B. Otaki, “Feedback in the era of generative ai,” 2023.
      pharmacy practice: A study from jordan,” Journal of the American            [347] I. Triguero, D. Molina, J. Poyatos, J. Del Ser, and F. Herrera,
      Pharmacists Association, 2023.                                                    “General purpose artificial intelligence systems (gpais): Properties,
[324] C. Tippareddy, S. Jiang, K. Bera, and N. Ramaiya, “Radiology reading              definition, taxonomy, open challenges and implications,” arXiv preprint
      room for the future: Harnessing the power of large language models                arXiv:2307.14283, 2023.
      like chatgpt,” Current Problems in Diagnostic Radiology, 2023.              [348] H. Huang, “Performance of chatgpt on registered nurse license exam
[325] O. Oviedo-Trespalacios, A. E. Peden, T. Cole-Hunter, A. Costantini,               in taiwan: A descriptive study,” 2023.
      M. Haghani, J. Rod, S. Kelly, H. Torkamaan, A. Tariq, J. D. A. Newton       [349] K. Muranga, I. S. Muse, E. N. Köroğlu, and Y. Yildirim, “Artificial
      et al., “The risks of using chatgpt to obtain common safety-related               intelligence and underfunded education,” London Journal of Social
      information and advice,” Safety Science, vol. 167, p. 106244, 2023.               Sciences, no. 6, pp. 56–68, 2023.
[326] A. Sarraju, D. Ouyang, and D. Itchhaporia, “The opportunities and           [350] G. Polverini and B. Gregorcic, “How understanding large language
      challenges of large language models in cardiology,” JACC: Advances,               models can inform their use in physics education,” arXiv preprint
      vol. 2, no. 7, p. 100438, 2023.                                                   arXiv:2309.12074, 2023.
[327] B. Coskun, G. Ocakoglu, M. Yetemen, and O. Kay-                             [351] B. Rajendran, O. Simeone, and B. M. Al-Hashimi, “Towards efficient
      gisiz, “Author reply,” Urology, 2023. [Online]. Available:                        and trustworthy ai through hardware-algorithm-communication co-
      https://www.sciencedirect.com/science/article/pii/S0090429523005721               design,” 2023.
[328] K. Lam, “Chatgpt for low-and middle-income countries: a greek gift?”        [352] S. Lobentanzer and J. Saez-Rodriguez, “A platform for the biomedical
      The Lancet Regional Health–Western Pacific, vol. 41, 2023.                        application of large language models,” 2023.
[329] J. Pantanowitz and L. Pantanowitz, “Implications of chatgpt for cy-
      topathology and recommendations for updating jasc guidelines on the
      responsible use of artificial intelligence.” Journal of the American
      Society of Cytopathology, pp. S2213–2945, 2023.
[330] S. Boussen, J.-B. Denis, P. Simeone, D. Lagier, N. Bruder, and L. Velly,
      “Chatgpt and the stochastic parrot: artificial intelligence in medical
      research,” British Journal of Anaesthesia, vol. 131, no. 4, pp. e120–
      e121, 2023.
[331] I. Schlam, M. S. Menezes, C. Corti, A. Tan, I. Abuali, and S. Tolaney,
      “Artificial intelligence as an adjunct tool for breast oncologists–are we
      there yet?” ESMO open, vol. 8, no. 5, 2023.
[332] Y.-G. Lyu and F. Wu, “Toward a more general empowering artificial
      intelligence,” Engineering, vol. 25, pp. 1–2, 2023. [Online]. Available:
      https://www.sciencedirect.com/science/article/pii/S2095809923002096
[333] M. Sparkes, “Are chatbots really able to think like people?” 2023.
[334] G. Bhatia and A. Kulkarni, “Chatgpt as co-author: Are
      researchers impressed or distressed?” Asian Journal of
      Psychiatry, vol. 84, p. 103564, 2023. [Online]. Available:
      https://www.sciencedirect.com/science/article/pii/S1876201823001193
[335] A. Chatelan, A. Clerc, and P.-A. Fonta, “Chatgpt and future artificial
      intelligence chatbots: What may be the influence on credentialed nutri-
      tion and dietetics practitioners?” Journal of the Academy of Nutrition
      and Dietetics, 2023.
[336] F. Lareyre, B. Nasr, A. Chaudhuri, G. Di Lorenzo, M. Carlier, and
      J. Raffort, “Comprehensive review of natural language processing (nlp)
      in vascular surgery,” in EJVES Vascular Forum. Elsevier, 2023.
[337] A. Wilkins, “The robot doctor will see you soon,” New Scientist, vol.
      257, no. 3423, p. 28, 2023.
